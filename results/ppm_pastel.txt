Files already downloaded and verified
Files already downloaded and verified
  0%|          | 0/30 [00:00<?, ?it/s]Selected users for the training: [7 0 5 6 9 2 3 1 8 4]
------------------------------------------
------User: 7, Epoch: 0--------
------------------------------------------
| Global Round : 0 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.011143
| Global Round : 0 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.005544
| Global Round : 0 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 1.252827
| Global Round : 0 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 1.284943
| Global Round : 0 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000108
| Global Round : 0 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.794577
| Global Round : 0 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.027961
| Global Round : 0 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.373600
| Global Round : 0 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.543838
| Global Round : 0 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.004554
| Global Round : 0 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.198744
| Global Round : 0 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.401948
| Global Round : 0 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 1.309438
| Global Round : 0 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.015491
| Global Round : 0 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.503414
| Global Round : 0 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.031741
| Global Round : 0 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.271206
| Global Round : 0 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.832976
| Global Round : 0 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.004380
| Global Round : 0 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.929548
| Global Round : 0 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.198526
| Global Round : 0 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.410467
| Global Round : 0 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 1.044103
| Global Round : 0 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.025473
| Global Round : 0 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000222
| Global Round : 0 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.475995
| Global Round : 0 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000937
| Global Round : 0 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.399202
| Global Round : 0 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.067427
| Global Round : 0 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.117845
| Global Round : 0 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.726506
| Global Round : 0 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.274972
| Global Round : 0 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.008018
| Global Round : 0 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.727064
| Global Round : 0 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.016476
| Global Round : 0 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.372588
| Global Round : 0 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.154513
| Global Round : 0 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.224427
| Global Round : 0 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.415991
| Global Round : 0 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000522
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0]})
------------------------------------------
------User: 0, Epoch: 0--------
------------------------------------------
| Global Round : 0 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 2.532974
| Global Round : 0 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000014
| Global Round : 0 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.549563
| Global Round : 0 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.011604
| Global Round : 0 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.240425
| Global Round : 0 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.009581
| Global Round : 0 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.003025
| Global Round : 0 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.842284
| Global Round : 0 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.006031
| Global Round : 0 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.024663
| Global Round : 0 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.400448
| Global Round : 0 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.002521
| Global Round : 0 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.016255
| Global Round : 0 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.717177
| Global Round : 0 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.338633
| Global Round : 0 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.274895
| Global Round : 0 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 1.449683
| Global Round : 0 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.018467
| Global Round : 0 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.007674
| Global Round : 0 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 1.961774
| Global Round : 0 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.090593
| Global Round : 0 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 1.274808
| Global Round : 0 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 1.017697
| Global Round : 0 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.548956
| Global Round : 0 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.282458
| Global Round : 0 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.293592
| Global Round : 0 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.320226
| Global Round : 0 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.104514
| Global Round : 0 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 2.191424
| Global Round : 0 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.483304
| Global Round : 0 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.295985
| Global Round : 0 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.971179
| Global Round : 0 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.484034
| Global Round : 0 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.022980
| Global Round : 0 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 1.307891
| Global Round : 0 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 1.882778
| Global Round : 0 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 1.585177
| Global Round : 0 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.010970
| Global Round : 0 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.034464
| Global Round : 0 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000808
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0], 'client_accuracy_0': [94.0]})
Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
/home2/felhattab/.local/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 49.80% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 48.50 %
----Attack Model Testing----
Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

------------------------------------------
------User: 5, Epoch: 0--------
------------------------------------------
| Global Round : 0 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000003
| Global Round : 0 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000006
| Global Round : 0 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000071
| Global Round : 0 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.035264
| Global Round : 0 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.007269
| Global Round : 0 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.135620
| Global Round : 0 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 1.297117
| Global Round : 0 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.519471
| Global Round : 0 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.092733
| Global Round : 0 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.018485
| Global Round : 0 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.022214
| Global Round : 0 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.206923
| Global Round : 0 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.299374
| Global Round : 0 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 1.068860
| Global Round : 0 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.119571
| Global Round : 0 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.597249
| Global Round : 0 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.551421
| Global Round : 0 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.072162
| Global Round : 0 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000392
| Global Round : 0 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.047256
| Global Round : 0 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.427286
| Global Round : 0 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.537024
| Global Round : 0 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.678663
| Global Round : 0 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.137036
| Global Round : 0 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.374577
| Global Round : 0 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000294
| Global Round : 0 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000727
| Global Round : 0 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 3.240736
| Global Round : 0 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 1.816672
| Global Round : 0 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.341325
| Global Round : 0 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.184615
| Global Round : 0 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 1.330594
| Global Round : 0 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.639715
| Global Round : 0 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.386860
| Global Round : 0 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.026133
| Global Round : 0 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.284856
| Global Round : 0 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.002634
| Global Round : 0 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.983342
| Global Round : 0 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.002426
| Global Round : 0 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.003532
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0], 'client_accuracy_0': [94.0], 'client_accuracy_5': [98.0]})
------------------------------------------
------User: 6, Epoch: 0--------
------------------------------------------
| Global Round : 0 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.680490
| Global Round : 0 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.275294
| Global Round : 0 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.762033
| Global Round : 0 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.187877
| Global Round : 0 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.136114
| Global Round : 0 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.003177
| Global Round : 0 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.582819
| Global Round : 0 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.673931
| Global Round : 0 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.515371
| Global Round : 0 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 2.467485
| Global Round : 0 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.322533
| Global Round : 0 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.115152
| Global Round : 0 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000510
| Global Round : 0 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.817423
| Global Round : 0 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.127853
| Global Round : 0 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000438
| Global Round : 0 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.010594
| Global Round : 0 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.066913
| Global Round : 0 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 1.217289
| Global Round : 0 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.055002
| Global Round : 0 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.002496
| Global Round : 0 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.009631
| Global Round : 0 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.293166
| Global Round : 0 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.436350
| Global Round : 0 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.553847
| Global Round : 0 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.183326
| Global Round : 0 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.044891
| Global Round : 0 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.689914
| Global Round : 0 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.305363
| Global Round : 0 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.311688
| Global Round : 0 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.255004
| Global Round : 0 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.059789
| Global Round : 0 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 1.020512
| Global Round : 0 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.035980
| Global Round : 0 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.165489
| Global Round : 0 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.002301
| Global Round : 0 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.315350
| Global Round : 0 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 1.255508
| Global Round : 0 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.125536
| Global Round : 0 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.009093
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0], 'client_accuracy_0': [94.0], 'client_accuracy_5': [98.0], 'client_accuracy_6': [96.2]})
------------------------------------------
------User: 9, Epoch: 0--------
------------------------------------------
| Global Round : 0 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000002
| Global Round : 0 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000042
| Global Round : 0 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.001011
| Global Round : 0 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000012
| Global Round : 0 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 1.094391
| Global Round : 0 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 1.007880
| Global Round : 0 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.055648
| Global Round : 0 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.195731
| Global Round : 0 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000369
| Global Round : 0 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.840543
| Global Round : 0 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.007957
| Global Round : 0 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.798990
| Global Round : 0 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.045129
| Global Round : 0 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 1.908619
| Global Round : 0 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.005311
| Global Round : 0 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.010295
| Global Round : 0 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 2.061965
| Global Round : 0 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.006506
| Global Round : 0 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.295439
| Global Round : 0 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 1.236158
| Global Round : 0 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 1.661526
| Global Round : 0 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.316499
| Global Round : 0 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000087
| Global Round : 0 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.320706
| Global Round : 0 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.024101
| Global Round : 0 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.251322
| Global Round : 0 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.045293
| Global Round : 0 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 1.095332
| Global Round : 0 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 1.913160
| Global Round : 0 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 2.000300
| Global Round : 0 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.325026
| Global Round : 0 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 1.017378
| Global Round : 0 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.019432
| Global Round : 0 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.030822
| Global Round : 0 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.770668
| Global Round : 0 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.979887
| Global Round : 0 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.132551
| Global Round : 0 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.175215
| Global Round : 0 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.011713
| Global Round : 0 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 1.678324
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0], 'client_accuracy_0': [94.0], 'client_accuracy_5': [98.0], 'client_accuracy_6': [96.2], 'client_accuracy_9': [97.2]})
------------------------------------------
------User: 2, Epoch: 0--------
------------------------------------------
| Global Round : 0 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.217566
| Global Round : 0 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.808288
| Global Round : 0 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000001
| Global Round : 0 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.029724
| Global Round : 0 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.001658
| Global Round : 0 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.193568
| Global Round : 0 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.003434
| Global Round : 0 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000515
| Global Round : 0 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.418828
| Global Round : 0 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.080824
| Global Round : 0 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 2.847016
| Global Round : 0 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.430201
| Global Round : 0 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.186725
| Global Round : 0 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.006587
| Global Round : 0 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.001872
| Global Round : 0 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 1.564595
| Global Round : 0 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.003541
| Global Round : 0 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.380481
| Global Round : 0 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.020460
| Global Round : 0 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.022583
| Global Round : 0 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.320306
| Global Round : 0 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 1.351978
| Global Round : 0 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.003437
| Global Round : 0 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.124283
| Global Round : 0 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.743684
| Global Round : 0 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.003513
| Global Round : 0 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.443734
| Global Round : 0 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.110658
| Global Round : 0 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.342396
| Global Round : 0 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 1.082384
| Global Round : 0 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.026268
| Global Round : 0 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.015907
| Global Round : 0 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.371351
| Global Round : 0 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.319072
| Global Round : 0 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.239188
| Global Round : 0 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.007585
| Global Round : 0 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.093262
| Global Round : 0 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.038953
| Global Round : 0 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.365631
| Global Round : 0 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.395333
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0], 'client_accuracy_0': [94.0], 'client_accuracy_5': [98.0], 'client_accuracy_6': [96.2], 'client_accuracy_9': [97.2], 'client_accuracy_2': [96.0]})
------------------------------------------
------User: 3, Epoch: 0--------
------------------------------------------
| Global Round : 0 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.001323
| Global Round : 0 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 1.713730
| Global Round : 0 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 1.580964
| Global Round : 0 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.003461
| Global Round : 0 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.086748
| Global Round : 0 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000139
| Global Round : 0 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.982492
| Global Round : 0 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.005760
| Global Round : 0 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.954030
| Global Round : 0 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.001099
| Global Round : 0 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.060425
| Global Round : 0 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.756752
| Global Round : 0 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.009769
| Global Round : 0 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.008094
| Global Round : 0 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.468594
| Global Round : 0 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.002650
| Global Round : 0 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.048645
| Global Round : 0 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.184709
| Global Round : 0 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.567074
| Global Round : 0 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000783
| Global Round : 0 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 1.405637
| Global Round : 0 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.435714
| Global Round : 0 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.003615
| Global Round : 0 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.144902
| Global Round : 0 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.426825
| Global Round : 0 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.185518
| Global Round : 0 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 2.377557
| Global Round : 0 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.002332
| Global Round : 0 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 1.572961
| Global Round : 0 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.003176
| Global Round : 0 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.224510
| Global Round : 0 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.069417
| Global Round : 0 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 1.008568
| Global Round : 0 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.244149
| Global Round : 0 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.138590
| Global Round : 0 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.443045
| Global Round : 0 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.047322
| Global Round : 0 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.029132
| Global Round : 0 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.775668
| Global Round : 0 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 1.587055
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0], 'client_accuracy_0': [94.0], 'client_accuracy_5': [98.0], 'client_accuracy_6': [96.2], 'client_accuracy_9': [97.2], 'client_accuracy_2': [96.0], 'client_accuracy_3': [96.6]})
------------------------------------------
------User: 1, Epoch: 0--------
------------------------------------------
| Global Round : 0 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.051122
| Global Round : 0 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 1.428238
| Global Round : 0 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 1.195455
| Global Round : 0 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 1.569878
| Global Round : 0 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000005
| Global Round : 0 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.154649
| Global Round : 0 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000370
| Global Round : 0 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.004299
| Global Round : 0 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.030257
| Global Round : 0 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.844755
| Global Round : 0 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000436
| Global Round : 0 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000145
| Global Round : 0 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.106967
| Global Round : 0 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.003238
| Global Round : 0 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.495016
| Global Round : 0 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.073249
| Global Round : 0 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 1.162288
| Global Round : 0 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.005594
| Global Round : 0 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.175054
| Global Round : 0 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.328878
| Global Round : 0 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.001829
| Global Round : 0 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.325992
| Global Round : 0 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 1.195248
| Global Round : 0 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.401874
| Global Round : 0 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.890931
| Global Round : 0 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.213775
| Global Round : 0 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.007990
| Global Round : 0 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.374117
| Global Round : 0 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.499411
| Global Round : 0 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.010385
| Global Round : 0 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.004509
| Global Round : 0 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.027041
| Global Round : 0 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.267509
| Global Round : 0 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.008601
| Global Round : 0 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.223314
| Global Round : 0 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.170400
| Global Round : 0 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.042066
| Global Round : 0 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.571648
| Global Round : 0 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.581445
| Global Round : 0 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.099562
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0], 'client_accuracy_0': [94.0], 'client_accuracy_5': [98.0], 'client_accuracy_6': [96.2], 'client_accuracy_9': [97.2], 'client_accuracy_2': [96.0], 'client_accuracy_3': [96.6], 'client_accuracy_1': [97.2]})
------------------------------------------
------User: 8, Epoch: 0--------
------------------------------------------
| Global Round : 0 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000005
| Global Round : 0 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000065
| Global Round : 0 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.007718
| Global Round : 0 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 1.213868
| Global Round : 0 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000818
| Global Round : 0 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.007769
| Global Round : 0 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.003550
| Global Round : 0 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.039587
| Global Round : 0 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.622237
| Global Round : 0 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.025474
| Global Round : 0 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.999702
| Global Round : 0 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.327527
| Global Round : 0 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.148950
| Global Round : 0 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.132496
| Global Round : 0 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.005772
| Global Round : 0 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.517205
| Global Round : 0 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.395011
| Global Round : 0 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.451644
| Global Round : 0 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 1.682222
| Global Round : 0 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.704064
| Global Round : 0 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.437530
| Global Round : 0 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.304141
| Global Round : 0 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.405529
| Global Round : 0 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.052952
| Global Round : 0 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.226489
| Global Round : 0 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.048778
| Global Round : 0 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.005859
| Global Round : 0 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.789605
| Global Round : 0 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.102861
| Global Round : 0 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000311
| Global Round : 0 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.978581
| Global Round : 0 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.008703
| Global Round : 0 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.199549
| Global Round : 0 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.222665
| Global Round : 0 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 1.496140
| Global Round : 0 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.011501
| Global Round : 0 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.009353
| Global Round : 0 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.339448
| Global Round : 0 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.047702
| Global Round : 0 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.034344
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0], 'client_accuracy_0': [94.0], 'client_accuracy_5': [98.0], 'client_accuracy_6': [96.2], 'client_accuracy_9': [97.2], 'client_accuracy_2': [96.0], 'client_accuracy_3': [96.6], 'client_accuracy_1': [97.2], 'client_accuracy_8': [96.8]})
------------------------------------------
------User: 4, Epoch: 0--------
------------------------------------------
| Global Round : 0 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.211594
| Global Round : 0 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.002158
| Global Round : 0 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000075
| Global Round : 0 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000040
| Global Round : 0 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 1.385256
| Global Round : 0 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.515382
| Global Round : 0 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.002742
| Global Round : 0 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.021633
| Global Round : 0 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.677027
| Global Round : 0 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.003669
| Global Round : 0 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.902604
| Global Round : 0 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000848
| Global Round : 0 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 1.434794
| Global Round : 0 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.117729
| Global Round : 0 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.280580
| Global Round : 0 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.247536
| Global Round : 0 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.300135
| Global Round : 0 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 2.543247
| Global Round : 0 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000417
| Global Round : 0 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.009871
| Global Round : 0 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 1.651923
| Global Round : 0 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.020127
| Global Round : 0 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.150889
| Global Round : 0 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.600527
| Global Round : 0 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.048259
| Global Round : 0 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.702703
| Global Round : 0 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.253411
| Global Round : 0 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.024181
| Global Round : 0 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.317104
| Global Round : 0 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.646965
| Global Round : 0 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.554005
| Global Round : 0 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.043576
| Global Round : 0 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.240503
| Global Round : 0 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.001392
| Global Round : 0 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.775245
| Global Round : 0 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.664369
| Global Round : 0 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.113661
| Global Round : 0 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.353969
| Global Round : 0 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.022019
| Global Round : 0 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000377
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0], 'client_accuracy_0': [94.0], 'client_accuracy_5': [98.0], 'client_accuracy_6': [96.2], 'client_accuracy_9': [97.2], 'client_accuracy_2': [96.0], 'client_accuracy_3': [96.6], 'client_accuracy_1': [97.2], 'client_accuracy_8': [96.8], 'client_accuracy_4': [96.8]})
 
Avg Training Stats after 1 global rounds:
Training Loss : nan
Train Accuracy: 9628.00% 

Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.80%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.80%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.80%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.80%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.80%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.80%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.80%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.80%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.80%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.80%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.80%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.80%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.80%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.80%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.80%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.80%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.80%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.80%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.80%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.80%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.80%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.80%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.80%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.80%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.80%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.80%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.80%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.80%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.80%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.80%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.80%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.80%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.80%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.80%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.80%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.80%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.80%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.80%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.80%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.80%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.80%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.80%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.80%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.80%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.80%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.80%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.80%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.80%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.80%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.80%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 48.80 %
----Attack Model Testing----
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  3%|         | 1/30 [03:45<1:49:03, 225.63s/it]Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

Selected users for the training: [3 9 5 0 6 2 8 4 1 7]
------------------------------------------
------User: 3, Epoch: 1--------
------------------------------------------
| Global Round : 1 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.006270
| Global Round : 1 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.350845
| Global Round : 1 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000097
| Global Round : 1 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.872441
| Global Round : 1 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000333
| Global Round : 1 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.239956
| Global Round : 1 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 1.296988
| Global Round : 1 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.037301
| Global Round : 1 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.629938
| Global Round : 1 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.024847
| Global Round : 1 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.015492
| Global Round : 1 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.012180
| Global Round : 1 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.448895
| Global Round : 1 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.439206
| Global Round : 1 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.003647
| Global Round : 1 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.012562
| Global Round : 1 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.074616
| Global Round : 1 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000037
| Global Round : 1 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.190866
| Global Round : 1 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.019585
| Global Round : 1 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.025066
| Global Round : 1 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.004068
| Global Round : 1 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.745238
| Global Round : 1 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.032830
| Global Round : 1 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.050461
| Global Round : 1 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.002002
| Global Round : 1 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.228961
| Global Round : 1 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.026674
| Global Round : 1 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.231387
| Global Round : 1 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.005779
| Global Round : 1 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.537375
| Global Round : 1 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.169162
| Global Round : 1 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.249302
| Global Round : 1 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000437
| Global Round : 1 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.156882
| Global Round : 1 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000954
| Global Round : 1 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.314772
| Global Round : 1 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.181886
| Global Round : 1 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.469853
| Global Round : 1 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.035650
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0], 'client_accuracy_0': [94.0], 'client_accuracy_5': [98.0], 'client_accuracy_6': [96.2], 'client_accuracy_9': [97.2], 'client_accuracy_2': [96.0], 'client_accuracy_3': [96.6, 99.6], 'client_accuracy_1': [97.2], 'client_accuracy_8': [96.8], 'client_accuracy_4': [96.8]})
------------------------------------------
------User: 9, Epoch: 1--------
------------------------------------------
| Global Round : 1 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.027937
| Global Round : 1 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.302041
| Global Round : 1 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000788
| Global Round : 1 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.001247
| Global Round : 1 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.235323
| Global Round : 1 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.015769
| Global Round : 1 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.074951
| Global Round : 1 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.222552
| Global Round : 1 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.597937
| Global Round : 1 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.326393
| Global Round : 1 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.052460
| Global Round : 1 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000711
| Global Round : 1 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.002095
| Global Round : 1 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.388821
| Global Round : 1 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.100567
| Global Round : 1 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.285385
| Global Round : 1 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.066652
| Global Round : 1 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.004560
| Global Round : 1 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 1.385391
| Global Round : 1 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.008841
| Global Round : 1 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.378237
| Global Round : 1 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.004788
| Global Round : 1 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.249071
| Global Round : 1 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.423574
| Global Round : 1 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.182392
| Global Round : 1 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.009814
| Global Round : 1 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.474576
| Global Round : 1 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.058201
| Global Round : 1 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.067337
| Global Round : 1 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.071372
| Global Round : 1 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.539690
| Global Round : 1 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.003657
| Global Round : 1 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.079164
| Global Round : 1 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.014507
| Global Round : 1 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.096325
| Global Round : 1 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.004217
| Global Round : 1 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.933154
| Global Round : 1 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000828
| Global Round : 1 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.004059
| Global Round : 1 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.035994
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0], 'client_accuracy_0': [94.0], 'client_accuracy_5': [98.0], 'client_accuracy_6': [96.2], 'client_accuracy_9': [97.2, 99.4], 'client_accuracy_2': [96.0], 'client_accuracy_3': [96.6, 99.6], 'client_accuracy_1': [97.2], 'client_accuracy_8': [96.8], 'client_accuracy_4': [96.8]})
------------------------------------------
------User: 5, Epoch: 1--------
------------------------------------------
| Global Round : 1 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.114010
| Global Round : 1 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.364222
| Global Round : 1 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000815
| Global Round : 1 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000791
| Global Round : 1 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.069264
| Global Round : 1 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.002650
| Global Round : 1 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.009396
| Global Round : 1 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.001380
| Global Round : 1 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.179868
| Global Round : 1 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.002211
| Global Round : 1 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.441306
| Global Round : 1 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000965
| Global Round : 1 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.044154
| Global Round : 1 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.012529
| Global Round : 1 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.019361
| Global Round : 1 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.031206
| Global Round : 1 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000208
| Global Round : 1 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.001821
| Global Round : 1 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000028
| Global Round : 1 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.020068
| Global Round : 1 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.160511
| Global Round : 1 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.271745
| Global Round : 1 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.007554
| Global Round : 1 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.106805
| Global Round : 1 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.592060
| Global Round : 1 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.003221
| Global Round : 1 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.008697
| Global Round : 1 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.060998
| Global Round : 1 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.027115
| Global Round : 1 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.596313
| Global Round : 1 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.032236
| Global Round : 1 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.015988
| Global Round : 1 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.004267
| Global Round : 1 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.073269
| Global Round : 1 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.010155
| Global Round : 1 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.444496
| Global Round : 1 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.010434
| Global Round : 1 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.211121
| Global Round : 1 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000357
| Global Round : 1 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.033441
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0], 'client_accuracy_0': [94.0], 'client_accuracy_5': [98.0, 99.2], 'client_accuracy_6': [96.2], 'client_accuracy_9': [97.2, 99.4], 'client_accuracy_2': [96.0], 'client_accuracy_3': [96.6, 99.6], 'client_accuracy_1': [97.2], 'client_accuracy_8': [96.8], 'client_accuracy_4': [96.8]})
------------------------------------------
------User: 0, Epoch: 1--------
------------------------------------------
| Global Round : 1 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.002077
| Global Round : 1 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.086467
| Global Round : 1 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.004090
| Global Round : 1 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.003532
| Global Round : 1 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.100198
| Global Round : 1 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.145072
| Global Round : 1 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000190
| Global Round : 1 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.470548
| Global Round : 1 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.305565
| Global Round : 1 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.010148
| Global Round : 1 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.750225
| Global Round : 1 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.003139
| Global Round : 1 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.001161
| Global Round : 1 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.100150
| Global Round : 1 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.420943
| Global Round : 1 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.798284
| Global Round : 1 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.001438
| Global Round : 1 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.885127
| Global Round : 1 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.031998
| Global Round : 1 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.133367
| Global Round : 1 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.078219
| Global Round : 1 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.002502
| Global Round : 1 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.035377
| Global Round : 1 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.108659
| Global Round : 1 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.081400
| Global Round : 1 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.004378
| Global Round : 1 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.009577
| Global Round : 1 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.737618
| Global Round : 1 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.007635
| Global Round : 1 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.009049
| Global Round : 1 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.036293
| Global Round : 1 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.006532
| Global Round : 1 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.383534
| Global Round : 1 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.015546
| Global Round : 1 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.015503
| Global Round : 1 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.001072
| Global Round : 1 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.002407
| Global Round : 1 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.003640
| Global Round : 1 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.339483
| Global Round : 1 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.007536
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0], 'client_accuracy_0': [94.0, 99.6], 'client_accuracy_5': [98.0, 99.2], 'client_accuracy_6': [96.2], 'client_accuracy_9': [97.2, 99.4], 'client_accuracy_2': [96.0], 'client_accuracy_3': [96.6, 99.6], 'client_accuracy_1': [97.2], 'client_accuracy_8': [96.8], 'client_accuracy_4': [96.8]})
Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 48.50 %
----Attack Model Testing----
Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

------------------------------------------
------User: 6, Epoch: 1--------
------------------------------------------
| Global Round : 1 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.003178
| Global Round : 1 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000032
| Global Round : 1 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.016002
| Global Round : 1 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.001128
| Global Round : 1 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.001317
| Global Round : 1 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000028
| Global Round : 1 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.010985
| Global Round : 1 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.455893
| Global Round : 1 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.163778
| Global Round : 1 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.671275
| Global Round : 1 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.254495
| Global Round : 1 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.002469
| Global Round : 1 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.240133
| Global Round : 1 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.008241
| Global Round : 1 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000409
| Global Round : 1 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.002757
| Global Round : 1 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.398342
| Global Round : 1 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.068370
| Global Round : 1 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.003012
| Global Round : 1 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.062885
| Global Round : 1 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.138034
| Global Round : 1 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.005101
| Global Round : 1 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.068196
| Global Round : 1 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000728
| Global Round : 1 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.001370
| Global Round : 1 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.014644
| Global Round : 1 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.406566
| Global Round : 1 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.004460
| Global Round : 1 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.129650
| Global Round : 1 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.037912
| Global Round : 1 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.008340
| Global Round : 1 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000077
| Global Round : 1 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.081323
| Global Round : 1 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.026495
| Global Round : 1 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.014388
| Global Round : 1 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.290384
| Global Round : 1 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.003416
| Global Round : 1 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.366393
| Global Round : 1 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.205406
| Global Round : 1 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.166425
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0], 'client_accuracy_0': [94.0, 99.6], 'client_accuracy_5': [98.0, 99.2], 'client_accuracy_6': [96.2, 99.6], 'client_accuracy_9': [97.2, 99.4], 'client_accuracy_2': [96.0], 'client_accuracy_3': [96.6, 99.6], 'client_accuracy_1': [97.2], 'client_accuracy_8': [96.8], 'client_accuracy_4': [96.8]})
------------------------------------------
------User: 2, Epoch: 1--------
------------------------------------------
| Global Round : 1 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.018519
| Global Round : 1 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.531100
| Global Round : 1 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000038
| Global Round : 1 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.092058
| Global Round : 1 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.016741
| Global Round : 1 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.224840
| Global Round : 1 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.042595
| Global Round : 1 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.536822
| Global Round : 1 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.303411
| Global Round : 1 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.024534
| Global Round : 1 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.348604
| Global Round : 1 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.002077
| Global Round : 1 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.017831
| Global Round : 1 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.002338
| Global Round : 1 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.003282
| Global Round : 1 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.127720
| Global Round : 1 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.506489
| Global Round : 1 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.012401
| Global Round : 1 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.086123
| Global Round : 1 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.004027
| Global Round : 1 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.021910
| Global Round : 1 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000313
| Global Round : 1 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.085812
| Global Round : 1 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.497596
| Global Round : 1 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.340082
| Global Round : 1 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.458007
| Global Round : 1 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.009566
| Global Round : 1 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000119
| Global Round : 1 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.065299
| Global Round : 1 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.008971
| Global Round : 1 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.047693
| Global Round : 1 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.110013
| Global Round : 1 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.006184
| Global Round : 1 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.450775
| Global Round : 1 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.213475
| Global Round : 1 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.537588
| Global Round : 1 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.033802
| Global Round : 1 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.017911
| Global Round : 1 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.303090
| Global Round : 1 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.047051
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0], 'client_accuracy_0': [94.0, 99.6], 'client_accuracy_5': [98.0, 99.2], 'client_accuracy_6': [96.2, 99.6], 'client_accuracy_9': [97.2, 99.4], 'client_accuracy_2': [96.0, 98.4], 'client_accuracy_3': [96.6, 99.6], 'client_accuracy_1': [97.2], 'client_accuracy_8': [96.8], 'client_accuracy_4': [96.8]})
------------------------------------------
------User: 8, Epoch: 1--------
------------------------------------------
| Global Round : 1 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.038918
| Global Round : 1 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.756655
| Global Round : 1 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.558580
| Global Round : 1 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.010959
| Global Round : 1 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.001926
| Global Round : 1 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000694
| Global Round : 1 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.018522
| Global Round : 1 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.937148
| Global Round : 1 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.010801
| Global Round : 1 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.032303
| Global Round : 1 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.001428
| Global Round : 1 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.053950
| Global Round : 1 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.004127
| Global Round : 1 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.012236
| Global Round : 1 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.161921
| Global Round : 1 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.374375
| Global Round : 1 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.001316
| Global Round : 1 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000030
| Global Round : 1 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.433366
| Global Round : 1 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000402
| Global Round : 1 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.470273
| Global Round : 1 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.004754
| Global Round : 1 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.022848
| Global Round : 1 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.025844
| Global Round : 1 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000729
| Global Round : 1 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000217
| Global Round : 1 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.003134
| Global Round : 1 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.187895
| Global Round : 1 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.005103
| Global Round : 1 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.005183
| Global Round : 1 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.242592
| Global Round : 1 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.031375
| Global Round : 1 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.026184
| Global Round : 1 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.038240
| Global Round : 1 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.076536
| Global Round : 1 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.036662
| Global Round : 1 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.050291
| Global Round : 1 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000772
| Global Round : 1 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.004314
| Global Round : 1 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.116244
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0], 'client_accuracy_0': [94.0, 99.6], 'client_accuracy_5': [98.0, 99.2], 'client_accuracy_6': [96.2, 99.6], 'client_accuracy_9': [97.2, 99.4], 'client_accuracy_2': [96.0, 98.4], 'client_accuracy_3': [96.6, 99.6], 'client_accuracy_1': [97.2], 'client_accuracy_8': [96.8, 98.6], 'client_accuracy_4': [96.8]})
------------------------------------------
------User: 4, Epoch: 1--------
------------------------------------------
| Global Round : 1 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 1.410546
| Global Round : 1 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.520038
| Global Round : 1 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.041118
| Global Round : 1 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.001892
| Global Round : 1 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.003040
| Global Round : 1 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000141
| Global Round : 1 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.016846
| Global Round : 1 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.101540
| Global Round : 1 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.483847
| Global Round : 1 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.255646
| Global Round : 1 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.001052
| Global Round : 1 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.003312
| Global Round : 1 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.016929
| Global Round : 1 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.532416
| Global Round : 1 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.114556
| Global Round : 1 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.017282
| Global Round : 1 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.008864
| Global Round : 1 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.008320
| Global Round : 1 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.001645
| Global Round : 1 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.279091
| Global Round : 1 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.005976
| Global Round : 1 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.642612
| Global Round : 1 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.033479
| Global Round : 1 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.005328
| Global Round : 1 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.284924
| Global Round : 1 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.525865
| Global Round : 1 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.007417
| Global Round : 1 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.416486
| Global Round : 1 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.024001
| Global Round : 1 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.607675
| Global Round : 1 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.008999
| Global Round : 1 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.327999
| Global Round : 1 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.005966
| Global Round : 1 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.137428
| Global Round : 1 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.342372
| Global Round : 1 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.039640
| Global Round : 1 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.624517
| Global Round : 1 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.587209
| Global Round : 1 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.018329
| Global Round : 1 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.248083
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0], 'client_accuracy_0': [94.0, 99.6], 'client_accuracy_5': [98.0, 99.2], 'client_accuracy_6': [96.2, 99.6], 'client_accuracy_9': [97.2, 99.4], 'client_accuracy_2': [96.0, 98.4], 'client_accuracy_3': [96.6, 99.6], 'client_accuracy_1': [97.2], 'client_accuracy_8': [96.8, 98.6], 'client_accuracy_4': [96.8, 97.2]})
------------------------------------------
------User: 1, Epoch: 1--------
------------------------------------------
| Global Round : 1 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.327185
| Global Round : 1 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.032859
| Global Round : 1 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.003210
| Global Round : 1 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.040718
| Global Round : 1 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.013788
| Global Round : 1 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.068262
| Global Round : 1 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.019404
| Global Round : 1 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000984
| Global Round : 1 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.011120
| Global Round : 1 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.005035
| Global Round : 1 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.395432
| Global Round : 1 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.027836
| Global Round : 1 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.199959
| Global Round : 1 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.014686
| Global Round : 1 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000282
| Global Round : 1 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.015127
| Global Round : 1 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.147181
| Global Round : 1 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000987
| Global Round : 1 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.244263
| Global Round : 1 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.469193
| Global Round : 1 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.001024
| Global Round : 1 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.124714
| Global Round : 1 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.021268
| Global Round : 1 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.479772
| Global Round : 1 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.002832
| Global Round : 1 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.002289
| Global Round : 1 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.738717
| Global Round : 1 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.006168
| Global Round : 1 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.003197
| Global Round : 1 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.004131
| Global Round : 1 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.020440
| Global Round : 1 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.014482
| Global Round : 1 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.022538
| Global Round : 1 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.006979
| Global Round : 1 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.016093
| Global Round : 1 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.004415
| Global Round : 1 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.011655
| Global Round : 1 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.005200
| Global Round : 1 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.003754
| Global Round : 1 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.180336
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0], 'client_accuracy_0': [94.0, 99.6], 'client_accuracy_5': [98.0, 99.2], 'client_accuracy_6': [96.2, 99.6], 'client_accuracy_9': [97.2, 99.4], 'client_accuracy_2': [96.0, 98.4], 'client_accuracy_3': [96.6, 99.6], 'client_accuracy_1': [97.2, 99.8], 'client_accuracy_8': [96.8, 98.6], 'client_accuracy_4': [96.8, 97.2]})
------------------------------------------
------User: 7, Epoch: 1--------
------------------------------------------
| Global Round : 1 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.027159
| Global Round : 1 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.967996
| Global Round : 1 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.008131
| Global Round : 1 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.295006
| Global Round : 1 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000573
| Global Round : 1 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000114
| Global Round : 1 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.004819
| Global Round : 1 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.336949
| Global Round : 1 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.009417
| Global Round : 1 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000917
| Global Round : 1 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.002527
| Global Round : 1 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.002730
| Global Round : 1 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.050919
| Global Round : 1 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.088976
| Global Round : 1 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 2.537022
| Global Round : 1 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.007114
| Global Round : 1 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.006217
| Global Round : 1 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000322
| Global Round : 1 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000717
| Global Round : 1 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.012424
| Global Round : 1 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.002826
| Global Round : 1 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.313819
| Global Round : 1 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.055549
| Global Round : 1 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.935136
| Global Round : 1 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.012216
| Global Round : 1 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.989375
| Global Round : 1 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.039209
| Global Round : 1 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.003650
| Global Round : 1 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.027406
| Global Round : 1 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.001248
| Global Round : 1 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.005266
| Global Round : 1 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000356
| Global Round : 1 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.623318
| Global Round : 1 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000867
| Global Round : 1 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.688827
| Global Round : 1 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.131933
| Global Round : 1 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.003015
| Global Round : 1 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.720060
| Global Round : 1 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.021487
| Global Round : 1 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.309168
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4], 'client_accuracy_0': [94.0, 99.6], 'client_accuracy_5': [98.0, 99.2], 'client_accuracy_6': [96.2, 99.6], 'client_accuracy_9': [97.2, 99.4], 'client_accuracy_2': [96.0, 98.4], 'client_accuracy_3': [96.6, 99.6], 'client_accuracy_1': [97.2, 99.8], 'client_accuracy_8': [96.8, 98.6], 'client_accuracy_4': [96.8, 97.2]})
 
Avg Training Stats after 2 global rounds:
Training Loss : nan
Train Accuracy: 9768.00% 

Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 49.70% | Val Loss: nan | Val Acc: 51.00%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 51.00%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 51.00%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 51.00%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 51.00%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 51.00%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 51.00%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 51.00%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 51.00%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 51.00%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 51.00%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 51.00%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 51.00%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 51.00%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 51.00%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 51.00%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 51.00%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 51.00%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 51.00%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 51.00%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 51.00%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 51.00%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 51.00%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 51.00%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 51.00%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 51.00%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 51.00%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 51.00%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 51.00%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 51.00%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 51.00%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 51.00%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 51.00%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 51.00%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 51.00%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 51.00%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 51.00%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 51.00%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 51.00%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 51.00%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 51.00%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 51.00%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 51.00%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 51.00%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 51.00%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 51.00%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 51.00%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 51.00%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 51.00%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 51.00%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 51.00 %
----Attack Model Testing----
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  7%|         | 2/30 [07:22<1:42:57, 220.62s/it]Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

Selected users for the training: [7 8 6 4 9 5 0 1 3 2]
------------------------------------------
------User: 7, Epoch: 2--------
------------------------------------------
| Global Round : 2 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.557328
| Global Round : 2 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000493
| Global Round : 2 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.007914
| Global Round : 2 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.001608
| Global Round : 2 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.706543
| Global Round : 2 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.021466
| Global Round : 2 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.001750
| Global Round : 2 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.002559
| Global Round : 2 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.305576
| Global Round : 2 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.078332
| Global Round : 2 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.299661
| Global Round : 2 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.198045
| Global Round : 2 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.006751
| Global Round : 2 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.094461
| Global Round : 2 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.492144
| Global Round : 2 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.011778
| Global Round : 2 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.023913
| Global Round : 2 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.033073
| Global Round : 2 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.018616
| Global Round : 2 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.011073
| Global Round : 2 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.080278
| Global Round : 2 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.484168
| Global Round : 2 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.004828
| Global Round : 2 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.062582
| Global Round : 2 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.003196
| Global Round : 2 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.004966
| Global Round : 2 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.137861
| Global Round : 2 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000561
| Global Round : 2 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.029606
| Global Round : 2 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.004621
| Global Round : 2 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.009085
| Global Round : 2 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.039160
| Global Round : 2 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.017817
| Global Round : 2 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.001221
| Global Round : 2 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.002806
| Global Round : 2 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.018848
| Global Round : 2 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.300476
| Global Round : 2 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.554704
| Global Round : 2 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.001116
| Global Round : 2 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.060800
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0], 'client_accuracy_0': [94.0, 99.6], 'client_accuracy_5': [98.0, 99.2], 'client_accuracy_6': [96.2, 99.6], 'client_accuracy_9': [97.2, 99.4], 'client_accuracy_2': [96.0, 98.4], 'client_accuracy_3': [96.6, 99.6], 'client_accuracy_1': [97.2, 99.8], 'client_accuracy_8': [96.8, 98.6], 'client_accuracy_4': [96.8, 97.2]})
------------------------------------------
------User: 8, Epoch: 2--------
------------------------------------------
| Global Round : 2 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.001670
| Global Round : 2 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.029226
| Global Round : 2 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.123037
| Global Round : 2 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.021222
| Global Round : 2 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.030198
| Global Round : 2 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.011551
| Global Round : 2 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.552143
| Global Round : 2 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.001261
| Global Round : 2 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.077706
| Global Round : 2 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.008646
| Global Round : 2 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.020460
| Global Round : 2 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000718
| Global Round : 2 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.083014
| Global Round : 2 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.008300
| Global Round : 2 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.271202
| Global Round : 2 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.328748
| Global Round : 2 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.015559
| Global Round : 2 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.199152
| Global Round : 2 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.006004
| Global Round : 2 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.015835
| Global Round : 2 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.002130
| Global Round : 2 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.005223
| Global Round : 2 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.345097
| Global Round : 2 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.002713
| Global Round : 2 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.009686
| Global Round : 2 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.378813
| Global Round : 2 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.185697
| Global Round : 2 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 1.669848
| Global Round : 2 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.024062
| Global Round : 2 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.021952
| Global Round : 2 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.107371
| Global Round : 2 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.001814
| Global Round : 2 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.009566
| Global Round : 2 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.008231
| Global Round : 2 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.013064
| Global Round : 2 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.033313
| Global Round : 2 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.031791
| Global Round : 2 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.023799
| Global Round : 2 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.121489
| Global Round : 2 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.302462
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0], 'client_accuracy_0': [94.0, 99.6], 'client_accuracy_5': [98.0, 99.2], 'client_accuracy_6': [96.2, 99.6], 'client_accuracy_9': [97.2, 99.4], 'client_accuracy_2': [96.0, 98.4], 'client_accuracy_3': [96.6, 99.6], 'client_accuracy_1': [97.2, 99.8], 'client_accuracy_8': [96.8, 98.6, 99.6], 'client_accuracy_4': [96.8, 97.2]})
------------------------------------------
------User: 6, Epoch: 2--------
------------------------------------------
| Global Round : 2 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.059173
| Global Round : 2 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000331
| Global Round : 2 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.001163
| Global Round : 2 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.091059
| Global Round : 2 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.081006
| Global Round : 2 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.017408
| Global Round : 2 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.006556
| Global Round : 2 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.002997
| Global Round : 2 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.001985
| Global Round : 2 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.058129
| Global Round : 2 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.009826
| Global Round : 2 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000263
| Global Round : 2 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.288790
| Global Round : 2 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.171315
| Global Round : 2 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.020728
| Global Round : 2 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.007962
| Global Round : 2 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.030648
| Global Round : 2 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 1.158909
| Global Round : 2 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.005278
| Global Round : 2 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.097195
| Global Round : 2 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.078555
| Global Round : 2 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.001052
| Global Round : 2 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.001038
| Global Round : 2 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.002664
| Global Round : 2 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.075042
| Global Round : 2 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.916279
| Global Round : 2 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.029180
| Global Round : 2 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.010224
| Global Round : 2 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.046969
| Global Round : 2 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.194525
| Global Round : 2 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.011284
| Global Round : 2 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.020043
| Global Round : 2 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.001649
| Global Round : 2 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.025481
| Global Round : 2 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.022256
| Global Round : 2 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.001981
| Global Round : 2 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.272272
| Global Round : 2 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.004563
| Global Round : 2 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.102810
| Global Round : 2 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.121952
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0], 'client_accuracy_0': [94.0, 99.6], 'client_accuracy_5': [98.0, 99.2], 'client_accuracy_6': [96.2, 99.6, 99.8], 'client_accuracy_9': [97.2, 99.4], 'client_accuracy_2': [96.0, 98.4], 'client_accuracy_3': [96.6, 99.6], 'client_accuracy_1': [97.2, 99.8], 'client_accuracy_8': [96.8, 98.6, 99.6], 'client_accuracy_4': [96.8, 97.2]})
------------------------------------------
------User: 4, Epoch: 2--------
------------------------------------------
| Global Round : 2 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.002400
| Global Round : 2 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.002342
| Global Round : 2 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.001739
| Global Round : 2 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.009401
| Global Round : 2 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.050623
| Global Round : 2 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.008149
| Global Round : 2 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.072630
| Global Round : 2 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.002349
| Global Round : 2 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.004676
| Global Round : 2 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000657
| Global Round : 2 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.017617
| Global Round : 2 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.002232
| Global Round : 2 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.060611
| Global Round : 2 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.001912
| Global Round : 2 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.252454
| Global Round : 2 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.014652
| Global Round : 2 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.001893
| Global Round : 2 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.199644
| Global Round : 2 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.018948
| Global Round : 2 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.043451
| Global Round : 2 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.070249
| Global Round : 2 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.419483
| Global Round : 2 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.133363
| Global Round : 2 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 1.482894
| Global Round : 2 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.012229
| Global Round : 2 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.002187
| Global Round : 2 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.010570
| Global Round : 2 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.295520
| Global Round : 2 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.240531
| Global Round : 2 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.001631
| Global Round : 2 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.014855
| Global Round : 2 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.048087
| Global Round : 2 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.002809
| Global Round : 2 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.041736
| Global Round : 2 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.026561
| Global Round : 2 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.040757
| Global Round : 2 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.002477
| Global Round : 2 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.010706
| Global Round : 2 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.009976
| Global Round : 2 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.024408
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0], 'client_accuracy_0': [94.0, 99.6], 'client_accuracy_5': [98.0, 99.2], 'client_accuracy_6': [96.2, 99.6, 99.8], 'client_accuracy_9': [97.2, 99.4], 'client_accuracy_2': [96.0, 98.4], 'client_accuracy_3': [96.6, 99.6], 'client_accuracy_1': [97.2, 99.8], 'client_accuracy_8': [96.8, 98.6, 99.6], 'client_accuracy_4': [96.8, 97.2, 99.6]})
------------------------------------------
------User: 9, Epoch: 2--------
------------------------------------------
| Global Round : 2 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000999
| Global Round : 2 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.005879
| Global Round : 2 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.003001
| Global Round : 2 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.011328
| Global Round : 2 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000272
| Global Round : 2 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.046889
| Global Round : 2 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.005469
| Global Round : 2 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.003593
| Global Round : 2 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000284
| Global Round : 2 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.011546
| Global Round : 2 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.009622
| Global Round : 2 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.040436
| Global Round : 2 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.014799
| Global Round : 2 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.001621
| Global Round : 2 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.022834
| Global Round : 2 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.002156
| Global Round : 2 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.054430
| Global Round : 2 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.003045
| Global Round : 2 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.004319
| Global Round : 2 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.002645
| Global Round : 2 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.005106
| Global Round : 2 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.001429
| Global Round : 2 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000880
| Global Round : 2 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.003349
| Global Round : 2 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.031940
| Global Round : 2 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.002322
| Global Round : 2 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.039893
| Global Round : 2 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.061932
| Global Round : 2 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.002107
| Global Round : 2 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.459492
| Global Round : 2 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.116162
| Global Round : 2 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.350361
| Global Round : 2 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.177868
| Global Round : 2 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.122727
| Global Round : 2 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.012317
| Global Round : 2 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.014024
| Global Round : 2 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000441
| Global Round : 2 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.004837
| Global Round : 2 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.371300
| Global Round : 2 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.241211
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0], 'client_accuracy_0': [94.0, 99.6], 'client_accuracy_5': [98.0, 99.2], 'client_accuracy_6': [96.2, 99.6, 99.8], 'client_accuracy_9': [97.2, 99.4, 99.8], 'client_accuracy_2': [96.0, 98.4], 'client_accuracy_3': [96.6, 99.6], 'client_accuracy_1': [97.2, 99.8], 'client_accuracy_8': [96.8, 98.6, 99.6], 'client_accuracy_4': [96.8, 97.2, 99.6]})
------------------------------------------
------User: 5, Epoch: 2--------
------------------------------------------
| Global Round : 2 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000107
| Global Round : 2 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000167
| Global Round : 2 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.008204
| Global Round : 2 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.082839
| Global Round : 2 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.013005
| Global Round : 2 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.001977
| Global Round : 2 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.109400
| Global Round : 2 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.010523
| Global Round : 2 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000622
| Global Round : 2 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.240510
| Global Round : 2 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.077106
| Global Round : 2 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.001461
| Global Round : 2 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.016646
| Global Round : 2 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.041520
| Global Round : 2 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.222515
| Global Round : 2 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.037988
| Global Round : 2 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.014116
| Global Round : 2 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.241782
| Global Round : 2 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.004806
| Global Round : 2 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.026012
| Global Round : 2 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.082647
| Global Round : 2 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.033600
| Global Round : 2 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.008104
| Global Round : 2 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.021434
| Global Round : 2 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000459
| Global Round : 2 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.390273
| Global Round : 2 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 1.070513
| Global Round : 2 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.008847
| Global Round : 2 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.004355
| Global Round : 2 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.003289
| Global Round : 2 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.198678
| Global Round : 2 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.006936
| Global Round : 2 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.048697
| Global Round : 2 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.061031
| Global Round : 2 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.002573
| Global Round : 2 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.041160
| Global Round : 2 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.250477
| Global Round : 2 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.004875
| Global Round : 2 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.004498
| Global Round : 2 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.617448
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0], 'client_accuracy_0': [94.0, 99.6], 'client_accuracy_5': [98.0, 99.2, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8], 'client_accuracy_9': [97.2, 99.4, 99.8], 'client_accuracy_2': [96.0, 98.4], 'client_accuracy_3': [96.6, 99.6], 'client_accuracy_1': [97.2, 99.8], 'client_accuracy_8': [96.8, 98.6, 99.6], 'client_accuracy_4': [96.8, 97.2, 99.6]})
------------------------------------------
------User: 0, Epoch: 2--------
------------------------------------------
| Global Round : 2 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000808
| Global Round : 2 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.008420
| Global Round : 2 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.046753
| Global Round : 2 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.071260
| Global Round : 2 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.166022
| Global Round : 2 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.003425
| Global Round : 2 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000612
| Global Round : 2 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000567
| Global Round : 2 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000409
| Global Round : 2 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 1.489208
| Global Round : 2 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.724984
| Global Round : 2 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.002360
| Global Round : 2 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.204234
| Global Round : 2 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.211942
| Global Round : 2 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.075032
| Global Round : 2 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.001962
| Global Round : 2 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.031617
| Global Round : 2 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000364
| Global Round : 2 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.005826
| Global Round : 2 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.302491
| Global Round : 2 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000215
| Global Round : 2 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.010750
| Global Round : 2 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.001403
| Global Round : 2 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.196183
| Global Round : 2 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.005496
| Global Round : 2 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.190116
| Global Round : 2 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.006650
| Global Round : 2 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.169714
| Global Round : 2 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.007563
| Global Round : 2 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.005216
| Global Round : 2 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.049123
| Global Round : 2 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.008690
| Global Round : 2 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.054197
| Global Round : 2 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.052387
| Global Round : 2 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.025770
| Global Round : 2 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.196473
| Global Round : 2 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.337995
| Global Round : 2 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.041973
| Global Round : 2 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.007003
| Global Round : 2 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.003976
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0], 'client_accuracy_0': [94.0, 99.6, 98.6], 'client_accuracy_5': [98.0, 99.2, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8], 'client_accuracy_9': [97.2, 99.4, 99.8], 'client_accuracy_2': [96.0, 98.4], 'client_accuracy_3': [96.6, 99.6], 'client_accuracy_1': [97.2, 99.8], 'client_accuracy_8': [96.8, 98.6, 99.6], 'client_accuracy_4': [96.8, 97.2, 99.6]})
Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 49.84% | Val Loss: nan | Val Acc: 47.80%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 50.09% | Val Loss: nan | Val Acc: 47.80%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 50.09% | Val Loss: nan | Val Acc: 47.80%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 50.09% | Val Loss: nan | Val Acc: 47.80%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 50.09% | Val Loss: nan | Val Acc: 47.80%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 50.09% | Val Loss: nan | Val Acc: 47.80%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 50.09% | Val Loss: nan | Val Acc: 47.80%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 50.09% | Val Loss: nan | Val Acc: 47.80%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 50.09% | Val Loss: nan | Val Acc: 47.80%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 50.09% | Val Loss: nan | Val Acc: 47.80%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 50.09% | Val Loss: nan | Val Acc: 47.80%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 50.09% | Val Loss: nan | Val Acc: 47.80%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 50.09% | Val Loss: nan | Val Acc: 47.80%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 50.09% | Val Loss: nan | Val Acc: 47.80%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 50.09% | Val Loss: nan | Val Acc: 47.80%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 50.09% | Val Loss: nan | Val Acc: 47.80%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 50.09% | Val Loss: nan | Val Acc: 47.80%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 50.09% | Val Loss: nan | Val Acc: 47.80%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 50.09% | Val Loss: nan | Val Acc: 47.80%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 50.09% | Val Loss: nan | Val Acc: 47.80%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 50.09% | Val Loss: nan | Val Acc: 47.80%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 50.09% | Val Loss: nan | Val Acc: 47.80%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 50.09% | Val Loss: nan | Val Acc: 47.80%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 50.09% | Val Loss: nan | Val Acc: 47.80%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 50.09% | Val Loss: nan | Val Acc: 47.80%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 50.09% | Val Loss: nan | Val Acc: 47.80%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 50.09% | Val Loss: nan | Val Acc: 47.80%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 50.09% | Val Loss: nan | Val Acc: 47.80%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 50.09% | Val Loss: nan | Val Acc: 47.80%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 50.09% | Val Loss: nan | Val Acc: 47.80%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 50.09% | Val Loss: nan | Val Acc: 47.80%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 50.09% | Val Loss: nan | Val Acc: 47.80%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 50.09% | Val Loss: nan | Val Acc: 47.80%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 50.09% | Val Loss: nan | Val Acc: 47.80%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 50.09% | Val Loss: nan | Val Acc: 47.80%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 50.09% | Val Loss: nan | Val Acc: 47.80%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 50.09% | Val Loss: nan | Val Acc: 47.80%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 50.09% | Val Loss: nan | Val Acc: 47.80%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 50.09% | Val Loss: nan | Val Acc: 47.80%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 50.09% | Val Loss: nan | Val Acc: 47.80%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 50.09% | Val Loss: nan | Val Acc: 47.80%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 50.09% | Val Loss: nan | Val Acc: 47.80%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 50.09% | Val Loss: nan | Val Acc: 47.80%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 50.09% | Val Loss: nan | Val Acc: 47.80%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 50.09% | Val Loss: nan | Val Acc: 47.80%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 50.09% | Val Loss: nan | Val Acc: 47.80%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 50.09% | Val Loss: nan | Val Acc: 47.80%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 50.09% | Val Loss: nan | Val Acc: 47.80%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 50.09% | Val Loss: nan | Val Acc: 47.80%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 50.09% | Val Loss: nan | Val Acc: 47.80%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 47.80 %
----Attack Model Testing----
Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

------------------------------------------
------User: 1, Epoch: 2--------
------------------------------------------
| Global Round : 2 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.110866
| Global Round : 2 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.257038
| Global Round : 2 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.004381
| Global Round : 2 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.032486
| Global Round : 2 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.003278
| Global Round : 2 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.001847
| Global Round : 2 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.040024
| Global Round : 2 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.013600
| Global Round : 2 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000517
| Global Round : 2 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.013465
| Global Round : 2 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.012027
| Global Round : 2 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.001770
| Global Round : 2 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.498346
| Global Round : 2 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.046292
| Global Round : 2 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000432
| Global Round : 2 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000461
| Global Round : 2 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.012663
| Global Round : 2 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000545
| Global Round : 2 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000721
| Global Round : 2 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.003544
| Global Round : 2 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.006360
| Global Round : 2 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.080451
| Global Round : 2 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.329461
| Global Round : 2 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.431132
| Global Round : 2 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.005176
| Global Round : 2 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.214626
| Global Round : 2 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.020359
| Global Round : 2 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.015810
| Global Round : 2 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.002112
| Global Round : 2 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.003929
| Global Round : 2 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.095556
| Global Round : 2 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.001125
| Global Round : 2 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.015077
| Global Round : 2 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.005157
| Global Round : 2 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.154382
| Global Round : 2 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.012234
| Global Round : 2 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.063646
| Global Round : 2 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000904
| Global Round : 2 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.015126
| Global Round : 2 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.367495
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0], 'client_accuracy_0': [94.0, 99.6, 98.6], 'client_accuracy_5': [98.0, 99.2, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8], 'client_accuracy_9': [97.2, 99.4, 99.8], 'client_accuracy_2': [96.0, 98.4], 'client_accuracy_3': [96.6, 99.6], 'client_accuracy_1': [97.2, 99.8, 99.4], 'client_accuracy_8': [96.8, 98.6, 99.6], 'client_accuracy_4': [96.8, 97.2, 99.6]})
------------------------------------------
------User: 3, Epoch: 2--------
------------------------------------------
| Global Round : 2 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.005592
| Global Round : 2 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.001485
| Global Round : 2 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.014179
| Global Round : 2 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.009272
| Global Round : 2 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.008706
| Global Round : 2 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.003429
| Global Round : 2 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.006382
| Global Round : 2 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.002040
| Global Round : 2 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000338
| Global Round : 2 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.008462
| Global Round : 2 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000137
| Global Round : 2 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.139613
| Global Round : 2 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.071243
| Global Round : 2 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.006672
| Global Round : 2 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.037782
| Global Round : 2 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.005835
| Global Round : 2 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.003426
| Global Round : 2 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.079257
| Global Round : 2 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.017316
| Global Round : 2 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.013893
| Global Round : 2 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.031202
| Global Round : 2 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.005317
| Global Round : 2 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.037081
| Global Round : 2 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.002445
| Global Round : 2 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.001668
| Global Round : 2 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.005094
| Global Round : 2 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000807
| Global Round : 2 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.410479
| Global Round : 2 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.022167
| Global Round : 2 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.033406
| Global Round : 2 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.004488
| Global Round : 2 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.018795
| Global Round : 2 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.029986
| Global Round : 2 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.028928
| Global Round : 2 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.033742
| Global Round : 2 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.125058
| Global Round : 2 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.032478
| Global Round : 2 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.011121
| Global Round : 2 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.007216
| Global Round : 2 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.266278
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0], 'client_accuracy_0': [94.0, 99.6, 98.6], 'client_accuracy_5': [98.0, 99.2, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8], 'client_accuracy_9': [97.2, 99.4, 99.8], 'client_accuracy_2': [96.0, 98.4], 'client_accuracy_3': [96.6, 99.6, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4], 'client_accuracy_8': [96.8, 98.6, 99.6], 'client_accuracy_4': [96.8, 97.2, 99.6]})
------------------------------------------
------User: 2, Epoch: 2--------
------------------------------------------
| Global Round : 2 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000512
| Global Round : 2 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.068597
| Global Round : 2 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.001536
| Global Round : 2 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.082898
| Global Round : 2 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.008969
| Global Round : 2 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.019409
| Global Round : 2 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.002710
| Global Round : 2 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.085179
| Global Round : 2 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.214079
| Global Round : 2 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.124675
| Global Round : 2 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.004836
| Global Round : 2 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.009671
| Global Round : 2 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.002022
| Global Round : 2 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.014872
| Global Round : 2 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.004035
| Global Round : 2 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.075089
| Global Round : 2 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.003379
| Global Round : 2 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.021276
| Global Round : 2 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000838
| Global Round : 2 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.311256
| Global Round : 2 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.006990
| Global Round : 2 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.005407
| Global Round : 2 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.004250
| Global Round : 2 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.004280
| Global Round : 2 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.023797
| Global Round : 2 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.038870
| Global Round : 2 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.464771
| Global Round : 2 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.007310
| Global Round : 2 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.204061
| Global Round : 2 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.121055
| Global Round : 2 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.019909
| Global Round : 2 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000273
| Global Round : 2 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.015067
| Global Round : 2 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.567142
| Global Round : 2 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.307570
| Global Round : 2 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.005093
| Global Round : 2 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.054100
| Global Round : 2 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.002956
| Global Round : 2 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.075649
| Global Round : 2 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.443253
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0], 'client_accuracy_0': [94.0, 99.6, 98.6], 'client_accuracy_5': [98.0, 99.2, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8], 'client_accuracy_9': [97.2, 99.4, 99.8], 'client_accuracy_2': [96.0, 98.4, 99.4], 'client_accuracy_3': [96.6, 99.6, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4], 'client_accuracy_8': [96.8, 98.6, 99.6], 'client_accuracy_4': [96.8, 97.2, 99.6]})
 
Avg Training Stats after 3 global rounds:
Training Loss : nan
Train Accuracy: 9829.33% 

Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 51.10 %
----Attack Model Testing----
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
 10%|         | 3/30 [11:10<1:40:40, 223.72s/it]Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

Selected users for the training: [6 8 3 1 7 5 0 2 4 9]
------------------------------------------
------User: 6, Epoch: 3--------
------------------------------------------
| Global Round : 3 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000336
| Global Round : 3 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.003983
| Global Round : 3 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000451
| Global Round : 3 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.004458
| Global Round : 3 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.001519
| Global Round : 3 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.001148
| Global Round : 3 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.026211
| Global Round : 3 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.015949
| Global Round : 3 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.051330
| Global Round : 3 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000358
| Global Round : 3 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000507
| Global Round : 3 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000575
| Global Round : 3 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.048167
| Global Round : 3 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.002289
| Global Round : 3 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.017226
| Global Round : 3 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.055656
| Global Round : 3 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.073526
| Global Round : 3 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.007107
| Global Round : 3 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.001773
| Global Round : 3 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.002504
| Global Round : 3 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.065112
| Global Round : 3 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.001519
| Global Round : 3 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.148491
| Global Round : 3 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.038188
| Global Round : 3 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.011817
| Global Round : 3 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.019966
| Global Round : 3 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.001716
| Global Round : 3 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.003697
| Global Round : 3 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.010334
| Global Round : 3 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.002306
| Global Round : 3 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.005613
| Global Round : 3 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.020990
| Global Round : 3 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.084584
| Global Round : 3 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.001025
| Global Round : 3 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.074547
| Global Round : 3 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.005262
| Global Round : 3 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.050633
| Global Round : 3 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.011849
| Global Round : 3 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.328684
| Global Round : 3 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.004414
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0], 'client_accuracy_0': [94.0, 99.6, 98.6], 'client_accuracy_5': [98.0, 99.2, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8], 'client_accuracy_2': [96.0, 98.4, 99.4], 'client_accuracy_3': [96.6, 99.6, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4], 'client_accuracy_8': [96.8, 98.6, 99.6], 'client_accuracy_4': [96.8, 97.2, 99.6]})
------------------------------------------
------User: 8, Epoch: 3--------
------------------------------------------
| Global Round : 3 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000581
| Global Round : 3 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.004709
| Global Round : 3 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.001179
| Global Round : 3 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000725
| Global Round : 3 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.004066
| Global Round : 3 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.002779
| Global Round : 3 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000710
| Global Round : 3 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000425
| Global Round : 3 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.001658
| Global Round : 3 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.012979
| Global Round : 3 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.005269
| Global Round : 3 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.005876
| Global Round : 3 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.001460
| Global Round : 3 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000249
| Global Round : 3 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.279457
| Global Round : 3 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.001985
| Global Round : 3 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.104337
| Global Round : 3 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.035119
| Global Round : 3 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.002383
| Global Round : 3 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.018014
| Global Round : 3 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.680196
| Global Round : 3 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.021260
| Global Round : 3 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.008418
| Global Round : 3 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.123419
| Global Round : 3 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.096622
| Global Round : 3 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.042152
| Global Round : 3 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.010121
| Global Round : 3 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000383
| Global Round : 3 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.068616
| Global Round : 3 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.005710
| Global Round : 3 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.002015
| Global Round : 3 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.059165
| Global Round : 3 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.008516
| Global Round : 3 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.234410
| Global Round : 3 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.134180
| Global Round : 3 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.006512
| Global Round : 3 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.017252
| Global Round : 3 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.032439
| Global Round : 3 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.102680
| Global Round : 3 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.142444
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0], 'client_accuracy_0': [94.0, 99.6, 98.6], 'client_accuracy_5': [98.0, 99.2, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8], 'client_accuracy_2': [96.0, 98.4, 99.4], 'client_accuracy_3': [96.6, 99.6, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6]})
------------------------------------------
------User: 3, Epoch: 3--------
------------------------------------------
| Global Round : 3 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.006613
| Global Round : 3 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.002655
| Global Round : 3 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.013283
| Global Round : 3 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000370
| Global Round : 3 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000923
| Global Round : 3 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000194
| Global Round : 3 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.005145
| Global Round : 3 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000555
| Global Round : 3 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.004428
| Global Round : 3 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.029500
| Global Round : 3 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.086655
| Global Round : 3 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000212
| Global Round : 3 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.277915
| Global Round : 3 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.006425
| Global Round : 3 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.046050
| Global Round : 3 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.017611
| Global Round : 3 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.047258
| Global Round : 3 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.013724
| Global Round : 3 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.007148
| Global Round : 3 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.019327
| Global Round : 3 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.095125
| Global Round : 3 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.001892
| Global Round : 3 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.008702
| Global Round : 3 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.008402
| Global Round : 3 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.003208
| Global Round : 3 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.010444
| Global Round : 3 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.037302
| Global Round : 3 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.007292
| Global Round : 3 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.073583
| Global Round : 3 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.026777
| Global Round : 3 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.004303
| Global Round : 3 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.018405
| Global Round : 3 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000816
| Global Round : 3 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.073245
| Global Round : 3 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.003859
| Global Round : 3 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.001285
| Global Round : 3 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.009769
| Global Round : 3 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.185138
| Global Round : 3 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.276247
| Global Round : 3 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.001747
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0], 'client_accuracy_0': [94.0, 99.6, 98.6], 'client_accuracy_5': [98.0, 99.2, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8], 'client_accuracy_2': [96.0, 98.4, 99.4], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6]})
------------------------------------------
------User: 1, Epoch: 3--------
------------------------------------------
| Global Round : 3 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.025829
| Global Round : 3 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.469576
| Global Round : 3 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.073569
| Global Round : 3 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.004902
| Global Round : 3 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000669
| Global Round : 3 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000313
| Global Round : 3 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.182208
| Global Round : 3 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.009757
| Global Round : 3 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000518
| Global Round : 3 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.051808
| Global Round : 3 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.002262
| Global Round : 3 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.055889
| Global Round : 3 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000385
| Global Round : 3 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000381
| Global Round : 3 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000849
| Global Round : 3 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000142
| Global Round : 3 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.010000
| Global Round : 3 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.002119
| Global Round : 3 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.010524
| Global Round : 3 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.024495
| Global Round : 3 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.139772
| Global Round : 3 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000440
| Global Round : 3 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.130243
| Global Round : 3 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.054712
| Global Round : 3 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.001149
| Global Round : 3 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.004077
| Global Round : 3 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.146980
| Global Round : 3 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.006042
| Global Round : 3 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000632
| Global Round : 3 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.006432
| Global Round : 3 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.006494
| Global Round : 3 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.002448
| Global Round : 3 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.009521
| Global Round : 3 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.036706
| Global Round : 3 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.004232
| Global Round : 3 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.015038
| Global Round : 3 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.006374
| Global Round : 3 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.011624
| Global Round : 3 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.001848
| Global Round : 3 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.198247
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0], 'client_accuracy_0': [94.0, 99.6, 98.6], 'client_accuracy_5': [98.0, 99.2, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8], 'client_accuracy_2': [96.0, 98.4, 99.4], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6]})
------------------------------------------
------User: 7, Epoch: 3--------
------------------------------------------
| Global Round : 3 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.031497
| Global Round : 3 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.577999
| Global Round : 3 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.007563
| Global Round : 3 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.571877
| Global Round : 3 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.202772
| Global Round : 3 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.015388
| Global Round : 3 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.016753
| Global Round : 3 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.006025
| Global Round : 3 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.017677
| Global Round : 3 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.011456
| Global Round : 3 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.132611
| Global Round : 3 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.166209
| Global Round : 3 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.002375
| Global Round : 3 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.124437
| Global Round : 3 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.025644
| Global Round : 3 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.014285
| Global Round : 3 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000900
| Global Round : 3 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.005977
| Global Round : 3 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.002753
| Global Round : 3 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000747
| Global Round : 3 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.016476
| Global Round : 3 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.018995
| Global Round : 3 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.008463
| Global Round : 3 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.060311
| Global Round : 3 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.006459
| Global Round : 3 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.045398
| Global Round : 3 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.003818
| Global Round : 3 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.030276
| Global Round : 3 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.001924
| Global Round : 3 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.008279
| Global Round : 3 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.001870
| Global Round : 3 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.036607
| Global Round : 3 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.008704
| Global Round : 3 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.110530
| Global Round : 3 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.165265
| Global Round : 3 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.001031
| Global Round : 3 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.042021
| Global Round : 3 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.324072
| Global Round : 3 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000890
| Global Round : 3 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.001989
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6], 'client_accuracy_0': [94.0, 99.6, 98.6], 'client_accuracy_5': [98.0, 99.2, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8], 'client_accuracy_2': [96.0, 98.4, 99.4], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6]})
------------------------------------------
------User: 5, Epoch: 3--------
------------------------------------------
| Global Round : 3 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.020205
| Global Round : 3 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000033
| Global Round : 3 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000410
| Global Round : 3 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.012494
| Global Round : 3 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.020857
| Global Round : 3 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.049673
| Global Round : 3 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.003543
| Global Round : 3 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000486
| Global Round : 3 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.259646
| Global Round : 3 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.046704
| Global Round : 3 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.001258
| Global Round : 3 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.170951
| Global Round : 3 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.019317
| Global Round : 3 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.009697
| Global Round : 3 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.002048
| Global Round : 3 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.002314
| Global Round : 3 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.299818
| Global Round : 3 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.070599
| Global Round : 3 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.328324
| Global Round : 3 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.025177
| Global Round : 3 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.005866
| Global Round : 3 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.008261
| Global Round : 3 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.045473
| Global Round : 3 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000508
| Global Round : 3 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.044235
| Global Round : 3 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.633940
| Global Round : 3 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.007599
| Global Round : 3 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.237379
| Global Round : 3 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.038623
| Global Round : 3 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.002138
| Global Round : 3 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.022191
| Global Round : 3 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.043216
| Global Round : 3 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.001954
| Global Round : 3 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.129164
| Global Round : 3 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.002282
| Global Round : 3 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.573717
| Global Round : 3 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.021706
| Global Round : 3 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.048721
| Global Round : 3 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.132832
| Global Round : 3 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.105260
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6], 'client_accuracy_0': [94.0, 99.6, 98.6], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8], 'client_accuracy_2': [96.0, 98.4, 99.4], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6]})
------------------------------------------
------User: 0, Epoch: 3--------
------------------------------------------
| Global Round : 3 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.002107
| Global Round : 3 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.009248
| Global Round : 3 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.001739
| Global Round : 3 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.025338
| Global Round : 3 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.020699
| Global Round : 3 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.001097
| Global Round : 3 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000208
| Global Round : 3 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.081477
| Global Round : 3 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.096279
| Global Round : 3 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.522511
| Global Round : 3 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.015197
| Global Round : 3 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.001929
| Global Round : 3 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.470115
| Global Round : 3 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.093628
| Global Round : 3 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.059970
| Global Round : 3 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.001475
| Global Round : 3 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.001127
| Global Round : 3 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.001343
| Global Round : 3 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.004712
| Global Round : 3 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.001986
| Global Round : 3 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000315
| Global Round : 3 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.007410
| Global Round : 3 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000872
| Global Round : 3 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000472
| Global Round : 3 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.005404
| Global Round : 3 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.360823
| Global Round : 3 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.106955
| Global Round : 3 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.387842
| Global Round : 3 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000451
| Global Round : 3 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.004176
| Global Round : 3 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000654
| Global Round : 3 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.046496
| Global Round : 3 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.007094
| Global Round : 3 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.057593
| Global Round : 3 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.039524
| Global Round : 3 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.004209
| Global Round : 3 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.208896
| Global Round : 3 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.014047
| Global Round : 3 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.011013
| Global Round : 3 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.016112
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8], 'client_accuracy_2': [96.0, 98.4, 99.4], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6]})
Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 49.74% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 50.00 %
----Attack Model Testing----
Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

------------------------------------------
------User: 2, Epoch: 3--------
------------------------------------------
| Global Round : 3 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.002092
| Global Round : 3 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.015961
| Global Round : 3 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000981
| Global Round : 3 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.009311
| Global Round : 3 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.048149
| Global Round : 3 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.003094
| Global Round : 3 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.001943
| Global Round : 3 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.012079
| Global Round : 3 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.039621
| Global Round : 3 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.069602
| Global Round : 3 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.102262
| Global Round : 3 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.017341
| Global Round : 3 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.020254
| Global Round : 3 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.015970
| Global Round : 3 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.021011
| Global Round : 3 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.058540
| Global Round : 3 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000341
| Global Round : 3 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.093252
| Global Round : 3 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.002217
| Global Round : 3 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.009623
| Global Round : 3 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.052134
| Global Round : 3 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.207220
| Global Round : 3 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.004766
| Global Round : 3 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.001132
| Global Round : 3 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.001097
| Global Round : 3 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.032273
| Global Round : 3 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.006629
| Global Round : 3 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.018598
| Global Round : 3 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.077728
| Global Round : 3 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.084754
| Global Round : 3 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.003520
| Global Round : 3 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.036431
| Global Round : 3 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.001043
| Global Round : 3 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.013500
| Global Round : 3 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.297681
| Global Round : 3 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.002514
| Global Round : 3 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.006857
| Global Round : 3 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.008689
| Global Round : 3 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.002414
| Global Round : 3 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.057695
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6]})
------------------------------------------
------User: 4, Epoch: 3--------
------------------------------------------
| Global Round : 3 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.016900
| Global Round : 3 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.008889
| Global Round : 3 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.269762
| Global Round : 3 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.005358
| Global Round : 3 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000100
| Global Round : 3 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.003401
| Global Round : 3 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000132
| Global Round : 3 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000452
| Global Round : 3 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.066585
| Global Round : 3 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.215039
| Global Round : 3 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.013284
| Global Round : 3 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.002446
| Global Round : 3 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.003914
| Global Round : 3 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.113400
| Global Round : 3 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.155798
| Global Round : 3 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.008462
| Global Round : 3 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.829674
| Global Round : 3 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.035132
| Global Round : 3 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.003116
| Global Round : 3 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.249009
| Global Round : 3 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.013095
| Global Round : 3 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.001556
| Global Round : 3 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.455011
| Global Round : 3 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.038102
| Global Round : 3 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.002898
| Global Round : 3 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.071942
| Global Round : 3 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.084234
| Global Round : 3 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.053336
| Global Round : 3 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.109275
| Global Round : 3 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.301985
| Global Round : 3 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.141435
| Global Round : 3 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.184553
| Global Round : 3 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.005785
| Global Round : 3 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.028807
| Global Round : 3 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.018088
| Global Round : 3 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.036201
| Global Round : 3 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.134599
| Global Round : 3 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.263854
| Global Round : 3 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.011891
| Global Round : 3 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.002914
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8]})
------------------------------------------
------User: 9, Epoch: 3--------
------------------------------------------
| Global Round : 3 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.012037
| Global Round : 3 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.010592
| Global Round : 3 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.003198
| Global Round : 3 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.001186
| Global Round : 3 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.001071
| Global Round : 3 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.002693
| Global Round : 3 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.007601
| Global Round : 3 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.002019
| Global Round : 3 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.001617
| Global Round : 3 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.021557
| Global Round : 3 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.039422
| Global Round : 3 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.002397
| Global Round : 3 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.111544
| Global Round : 3 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.045593
| Global Round : 3 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.004592
| Global Round : 3 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.008811
| Global Round : 3 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.304941
| Global Round : 3 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.182109
| Global Round : 3 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.024117
| Global Round : 3 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000782
| Global Round : 3 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.005053
| Global Round : 3 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.005741
| Global Round : 3 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.366848
| Global Round : 3 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.042199
| Global Round : 3 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.015140
| Global Round : 3 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.085174
| Global Round : 3 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000456
| Global Round : 3 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.072591
| Global Round : 3 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.012657
| Global Round : 3 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.012179
| Global Round : 3 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000369
| Global Round : 3 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.006172
| Global Round : 3 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000884
| Global Round : 3 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.006033
| Global Round : 3 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.019992
| Global Round : 3 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.005565
| Global Round : 3 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.018069
| Global Round : 3 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000912
| Global Round : 3 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.002717
| Global Round : 3 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.001503
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8]})
 
Avg Training Stats after 4 global rounds:
Training Loss : nan
Train Accuracy: 9869.50% 

Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 48.60 %
----Attack Model Testing----
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
 13%|        | 4/30 [14:50<1:36:18, 222.25s/it]Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

Selected users for the training: [9 8 0 2 3 5 7 1 4 6]
------------------------------------------
------User: 9, Epoch: 4--------
------------------------------------------
| Global Round : 4 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.011949
| Global Round : 4 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.010930
| Global Round : 4 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.003418
| Global Round : 4 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.001893
| Global Round : 4 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.001396
| Global Round : 4 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.224759
| Global Round : 4 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.043977
| Global Round : 4 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000219
| Global Round : 4 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.002426
| Global Round : 4 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.039553
| Global Round : 4 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.005565
| Global Round : 4 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.188445
| Global Round : 4 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.005413
| Global Round : 4 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.003544
| Global Round : 4 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.119619
| Global Round : 4 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.015793
| Global Round : 4 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.003067
| Global Round : 4 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.405393
| Global Round : 4 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.001435
| Global Round : 4 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.136186
| Global Round : 4 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.001945
| Global Round : 4 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.005496
| Global Round : 4 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.002077
| Global Round : 4 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.003128
| Global Round : 4 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.001896
| Global Round : 4 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.019459
| Global Round : 4 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.450390
| Global Round : 4 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.144669
| Global Round : 4 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.023210
| Global Round : 4 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.004576
| Global Round : 4 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000614
| Global Round : 4 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000901
| Global Round : 4 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.040392
| Global Round : 4 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.001467
| Global Round : 4 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.016311
| Global Round : 4 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.001578
| Global Round : 4 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.119994
| Global Round : 4 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.001822
| Global Round : 4 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.003693
| Global Round : 4 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.011963
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8]})
------------------------------------------
------User: 8, Epoch: 4--------
------------------------------------------
| Global Round : 4 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.510542
| Global Round : 4 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.010065
| Global Round : 4 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.001516
| Global Round : 4 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.050008
| Global Round : 4 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.004437
| Global Round : 4 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.002852
| Global Round : 4 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.045076
| Global Round : 4 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000564
| Global Round : 4 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.001660
| Global Round : 4 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.044591
| Global Round : 4 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.100957
| Global Round : 4 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.096205
| Global Round : 4 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000801
| Global Round : 4 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.059977
| Global Round : 4 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.007387
| Global Round : 4 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.003756
| Global Round : 4 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.086609
| Global Round : 4 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.039038
| Global Round : 4 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000178
| Global Round : 4 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.013011
| Global Round : 4 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000716
| Global Round : 4 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.001273
| Global Round : 4 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.336195
| Global Round : 4 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000223
| Global Round : 4 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000505
| Global Round : 4 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.005370
| Global Round : 4 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.002838
| Global Round : 4 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.001977
| Global Round : 4 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.009933
| Global Round : 4 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.025947
| Global Round : 4 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.027828
| Global Round : 4 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.167664
| Global Round : 4 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000422
| Global Round : 4 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.001393
| Global Round : 4 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000052
| Global Round : 4 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000990
| Global Round : 4 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.001930
| Global Round : 4 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.001603
| Global Round : 4 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.013388
| Global Round : 4 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.006686
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8]})
------------------------------------------
------User: 0, Epoch: 4--------
------------------------------------------
| Global Round : 4 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.106622
| Global Round : 4 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.001945
| Global Round : 4 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.044743
| Global Round : 4 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.001267
| Global Round : 4 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.003235
| Global Round : 4 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.001548
| Global Round : 4 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000833
| Global Round : 4 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.139976
| Global Round : 4 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.001231
| Global Round : 4 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.050704
| Global Round : 4 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.038509
| Global Round : 4 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.015991
| Global Round : 4 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.124956
| Global Round : 4 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.006828
| Global Round : 4 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.080118
| Global Round : 4 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000094
| Global Round : 4 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.007199
| Global Round : 4 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.026931
| Global Round : 4 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.008401
| Global Round : 4 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.080156
| Global Round : 4 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.003494
| Global Round : 4 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.031789
| Global Round : 4 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.048186
| Global Round : 4 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000745
| Global Round : 4 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.180202
| Global Round : 4 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.005157
| Global Round : 4 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.015726
| Global Round : 4 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.065292
| Global Round : 4 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000149
| Global Round : 4 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.001543
| Global Round : 4 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.045238
| Global Round : 4 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000990
| Global Round : 4 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.001789
| Global Round : 4 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.014535
| Global Round : 4 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.002790
| Global Round : 4 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.004239
| Global Round : 4 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000894
| Global Round : 4 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.008150
| Global Round : 4 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000235
| Global Round : 4 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.002540
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8]})
Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 49.70 %
----Attack Model Testing----
Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

------------------------------------------
------User: 2, Epoch: 4--------
------------------------------------------
| Global Round : 4 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.001512
| Global Round : 4 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.226081
| Global Round : 4 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000218
| Global Round : 4 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.006141
| Global Round : 4 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000195
| Global Round : 4 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.003309
| Global Round : 4 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.010604
| Global Round : 4 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.001671
| Global Round : 4 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.001122
| Global Round : 4 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.024422
| Global Round : 4 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.110796
| Global Round : 4 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.399579
| Global Round : 4 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.002876
| Global Round : 4 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.001619
| Global Round : 4 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.012135
| Global Round : 4 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000201
| Global Round : 4 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.003575
| Global Round : 4 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.001232
| Global Round : 4 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.011260
| Global Round : 4 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.003974
| Global Round : 4 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.043282
| Global Round : 4 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.004329
| Global Round : 4 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000089
| Global Round : 4 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000235
| Global Round : 4 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.261281
| Global Round : 4 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.010964
| Global Round : 4 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.392403
| Global Round : 4 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000613
| Global Round : 4 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.004884
| Global Round : 4 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.001636
| Global Round : 4 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.004093
| Global Round : 4 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.047996
| Global Round : 4 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.004826
| Global Round : 4 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.001218
| Global Round : 4 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.036035
| Global Round : 4 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.045426
| Global Round : 4 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.002638
| Global Round : 4 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000361
| Global Round : 4 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.005473
| Global Round : 4 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.001538
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8]})
------------------------------------------
------User: 3, Epoch: 4--------
------------------------------------------
| Global Round : 4 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.008219
| Global Round : 4 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.455650
| Global Round : 4 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.003415
| Global Round : 4 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.002925
| Global Round : 4 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.059332
| Global Round : 4 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.024345
| Global Round : 4 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.025957
| Global Round : 4 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.001209
| Global Round : 4 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000287
| Global Round : 4 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000212
| Global Round : 4 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000476
| Global Round : 4 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.102817
| Global Round : 4 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.007009
| Global Round : 4 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000545
| Global Round : 4 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000328
| Global Round : 4 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.024060
| Global Round : 4 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.005983
| Global Round : 4 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.115826
| Global Round : 4 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.002817
| Global Round : 4 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.012146
| Global Round : 4 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.001024
| Global Round : 4 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.039619
| Global Round : 4 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.001590
| Global Round : 4 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.019198
| Global Round : 4 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.176756
| Global Round : 4 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.001035
| Global Round : 4 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.024809
| Global Round : 4 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.016374
| Global Round : 4 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.113747
| Global Round : 4 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000187
| Global Round : 4 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.290361
| Global Round : 4 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.001215
| Global Round : 4 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.027978
| Global Round : 4 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.002735
| Global Round : 4 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.001414
| Global Round : 4 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.162269
| Global Round : 4 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.014420
| Global Round : 4 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000529
| Global Round : 4 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.011841
| Global Round : 4 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.009760
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8]})
------------------------------------------
------User: 5, Epoch: 4--------
------------------------------------------
| Global Round : 4 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.004320
| Global Round : 4 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000459
| Global Round : 4 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.002433
| Global Round : 4 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.001908
| Global Round : 4 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.003684
| Global Round : 4 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.041328
| Global Round : 4 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.116224
| Global Round : 4 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.034439
| Global Round : 4 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.466436
| Global Round : 4 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.066545
| Global Round : 4 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.016561
| Global Round : 4 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.007253
| Global Round : 4 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.001679
| Global Round : 4 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.009745
| Global Round : 4 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.004892
| Global Round : 4 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000266
| Global Round : 4 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.001945
| Global Round : 4 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000976
| Global Round : 4 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.001211
| Global Round : 4 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.004768
| Global Round : 4 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.002110
| Global Round : 4 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000241
| Global Round : 4 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.001346
| Global Round : 4 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.120037
| Global Round : 4 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.021901
| Global Round : 4 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.002209
| Global Round : 4 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.062004
| Global Round : 4 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.030643
| Global Round : 4 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.014711
| Global Round : 4 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.275996
| Global Round : 4 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.005303
| Global Round : 4 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.443656
| Global Round : 4 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.001644
| Global Round : 4 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000675
| Global Round : 4 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.002790
| Global Round : 4 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000090
| Global Round : 4 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000588
| Global Round : 4 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.012700
| Global Round : 4 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.003755
| Global Round : 4 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000337
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8]})
------------------------------------------
------User: 7, Epoch: 4--------
------------------------------------------
| Global Round : 4 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000201
| Global Round : 4 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.119723
| Global Round : 4 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.164114
| Global Round : 4 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000723
| Global Round : 4 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.004626
| Global Round : 4 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.001309
| Global Round : 4 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.005700
| Global Round : 4 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.005563
| Global Round : 4 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.145230
| Global Round : 4 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.152229
| Global Round : 4 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.326262
| Global Round : 4 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000588
| Global Round : 4 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.014260
| Global Round : 4 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000997
| Global Round : 4 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.006128
| Global Round : 4 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.037675
| Global Round : 4 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.009817
| Global Round : 4 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000690
| Global Round : 4 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.034122
| Global Round : 4 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.002222
| Global Round : 4 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.135938
| Global Round : 4 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000226
| Global Round : 4 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.001354
| Global Round : 4 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.003643
| Global Round : 4 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.006128
| Global Round : 4 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.002649
| Global Round : 4 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.002874
| Global Round : 4 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.004193
| Global Round : 4 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.057567
| Global Round : 4 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.003802
| Global Round : 4 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000723
| Global Round : 4 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.027572
| Global Round : 4 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000086
| Global Round : 4 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.091032
| Global Round : 4 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.192585
| Global Round : 4 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.001398
| Global Round : 4 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.061001
| Global Round : 4 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000776
| Global Round : 4 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.026222
| Global Round : 4 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.437964
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8]})
------------------------------------------
------User: 1, Epoch: 4--------
------------------------------------------
| Global Round : 4 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.003717
| Global Round : 4 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.151493
| Global Round : 4 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.022988
| Global Round : 4 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000519
| Global Round : 4 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.005039
| Global Round : 4 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000888
| Global Round : 4 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.049455
| Global Round : 4 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.045678
| Global Round : 4 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000673
| Global Round : 4 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.057471
| Global Round : 4 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.019756
| Global Round : 4 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.008425
| Global Round : 4 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.016953
| Global Round : 4 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000162
| Global Round : 4 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.210930
| Global Round : 4 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.016774
| Global Round : 4 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.005743
| Global Round : 4 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.001635
| Global Round : 4 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.009997
| Global Round : 4 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.013155
| Global Round : 4 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.001592
| Global Round : 4 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.002518
| Global Round : 4 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.005964
| Global Round : 4 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.044635
| Global Round : 4 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.001346
| Global Round : 4 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.005007
| Global Round : 4 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.002400
| Global Round : 4 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000155
| Global Round : 4 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.004119
| Global Round : 4 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.004433
| Global Round : 4 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.002112
| Global Round : 4 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000177
| Global Round : 4 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000817
| Global Round : 4 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.002207
| Global Round : 4 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.003865
| Global Round : 4 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.507207
| Global Round : 4 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.033713
| Global Round : 4 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.223646
| Global Round : 4 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000604
| Global Round : 4 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.001714
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8]})
------------------------------------------
------User: 4, Epoch: 4--------
------------------------------------------
| Global Round : 4 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000246
| Global Round : 4 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000403
| Global Round : 4 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.003054
| Global Round : 4 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000058
| Global Round : 4 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.006319
| Global Round : 4 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.004579
| Global Round : 4 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.002810
| Global Round : 4 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.020408
| Global Round : 4 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000719
| Global Round : 4 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000714
| Global Round : 4 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.007843
| Global Round : 4 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000487
| Global Round : 4 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.010503
| Global Round : 4 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.007881
| Global Round : 4 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.002267
| Global Round : 4 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000938
| Global Round : 4 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.064927
| Global Round : 4 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.095538
| Global Round : 4 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000129
| Global Round : 4 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000325
| Global Round : 4 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.005139
| Global Round : 4 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000800
| Global Round : 4 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.002006
| Global Round : 4 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.347025
| Global Round : 4 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.108020
| Global Round : 4 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.008687
| Global Round : 4 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.009430
| Global Round : 4 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.015686
| Global Round : 4 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.088569
| Global Round : 4 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.001095
| Global Round : 4 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.117363
| Global Round : 4 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.003081
| Global Round : 4 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.003781
| Global Round : 4 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.037603
| Global Round : 4 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.035507
| Global Round : 4 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.106668
| Global Round : 4 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000935
| Global Round : 4 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.120538
| Global Round : 4 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.048516
| Global Round : 4 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.009492
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6]})
------------------------------------------
------User: 6, Epoch: 4--------
------------------------------------------
| Global Round : 4 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.023735
| Global Round : 4 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.046466
| Global Round : 4 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.005487
| Global Round : 4 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.010077
| Global Round : 4 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000927
| Global Round : 4 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.038863
| Global Round : 4 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.001279
| Global Round : 4 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.056611
| Global Round : 4 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.054047
| Global Round : 4 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000705
| Global Round : 4 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.001442
| Global Round : 4 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.005001
| Global Round : 4 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.001277
| Global Round : 4 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.002646
| Global Round : 4 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.016070
| Global Round : 4 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.001749
| Global Round : 4 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.133787
| Global Round : 4 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000143
| Global Round : 4 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.011020
| Global Round : 4 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000874
| Global Round : 4 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000400
| Global Round : 4 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000946
| Global Round : 4 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.004908
| Global Round : 4 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.002037
| Global Round : 4 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.049775
| Global Round : 4 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.004265
| Global Round : 4 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.001932
| Global Round : 4 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.002878
| Global Round : 4 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.005473
| Global Round : 4 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.060854
| Global Round : 4 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000560
| Global Round : 4 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.079142
| Global Round : 4 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.010462
| Global Round : 4 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.020022
| Global Round : 4 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.010585
| Global Round : 4 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.001553
| Global Round : 4 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.015757
| Global Round : 4 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.008388
| Global Round : 4 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.086138
| Global Round : 4 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.005037
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6]})
 
Avg Training Stats after 5 global rounds:
Training Loss : nan
Train Accuracy: 9893.60% 

Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 49.70% | Val Loss: nan | Val Acc: 50.20%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.20%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.20%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.20%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.20%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.20%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.20%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.20%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.20%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.20%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.20%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.20%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.20%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.20%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.20%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.20%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.20%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.20%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.20%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.20%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.20%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.20%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.20%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.20%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.20%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.20%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.20%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.20%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.20%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.20%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.20%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.20%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.20%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.20%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.20%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.20%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.20%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.20%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.20%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.20%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.20%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.20%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.20%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.20%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.20%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.20%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.20%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.20%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.20%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.20%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 50.20 %
----Attack Model Testing----
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
 17%|        | 5/30 [18:27<1:31:48, 220.33s/it]Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

Selected users for the training: [9 7 0 2 1 8 3 4 5 6]
------------------------------------------
------User: 9, Epoch: 5--------
------------------------------------------
| Global Round : 5 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000584
| Global Round : 5 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.011184
| Global Round : 5 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.001746
| Global Round : 5 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.002046
| Global Round : 5 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.120428
| Global Round : 5 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.001305
| Global Round : 5 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.049714
| Global Round : 5 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.083605
| Global Round : 5 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.009927
| Global Round : 5 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000397
| Global Round : 5 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.043281
| Global Round : 5 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000215
| Global Round : 5 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.334066
| Global Round : 5 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.098474
| Global Round : 5 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.001057
| Global Round : 5 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.002229
| Global Round : 5 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.046701
| Global Round : 5 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.016073
| Global Round : 5 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000582
| Global Round : 5 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.006786
| Global Round : 5 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.006546
| Global Round : 5 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.004047
| Global Round : 5 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.279598
| Global Round : 5 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.019457
| Global Round : 5 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.028347
| Global Round : 5 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.004585
| Global Round : 5 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.024920
| Global Round : 5 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.001022
| Global Round : 5 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.022208
| Global Round : 5 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.018791
| Global Round : 5 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.002160
| Global Round : 5 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.111591
| Global Round : 5 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.348132
| Global Round : 5 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.005092
| Global Round : 5 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.021867
| Global Round : 5 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.102324
| Global Round : 5 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000145
| Global Round : 5 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.022536
| Global Round : 5 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.016217
| Global Round : 5 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.001795
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6]})
------------------------------------------
------User: 7, Epoch: 5--------
------------------------------------------
| Global Round : 5 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.005269
| Global Round : 5 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000602
| Global Round : 5 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000438
| Global Round : 5 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.001130
| Global Round : 5 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000672
| Global Round : 5 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.001514
| Global Round : 5 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000473
| Global Round : 5 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.004152
| Global Round : 5 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.156793
| Global Round : 5 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.008392
| Global Round : 5 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000598
| Global Round : 5 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.042893
| Global Round : 5 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.028906
| Global Round : 5 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.002811
| Global Round : 5 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.034475
| Global Round : 5 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.111030
| Global Round : 5 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.055707
| Global Round : 5 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.007611
| Global Round : 5 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.007339
| Global Round : 5 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.003717
| Global Round : 5 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.004949
| Global Round : 5 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.012756
| Global Round : 5 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000822
| Global Round : 5 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000210
| Global Round : 5 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.041342
| Global Round : 5 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.001383
| Global Round : 5 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.016481
| Global Round : 5 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.001908
| Global Round : 5 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.010120
| Global Round : 5 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.021666
| Global Round : 5 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.002081
| Global Round : 5 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.004805
| Global Round : 5 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.005760
| Global Round : 5 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.012565
| Global Round : 5 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.086778
| Global Round : 5 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.002459
| Global Round : 5 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.003714
| Global Round : 5 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.003432
| Global Round : 5 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.001653
| Global Round : 5 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.004457
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6]})
------------------------------------------
------User: 0, Epoch: 5--------
------------------------------------------
| Global Round : 5 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000482
| Global Round : 5 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.003520
| Global Round : 5 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000214
| Global Round : 5 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.004305
| Global Round : 5 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.003260
| Global Round : 5 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.004621
| Global Round : 5 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000996
| Global Round : 5 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.004223
| Global Round : 5 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.001604
| Global Round : 5 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.113956
| Global Round : 5 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000121
| Global Round : 5 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.032100
| Global Round : 5 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000480
| Global Round : 5 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.185737
| Global Round : 5 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.097372
| Global Round : 5 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.020993
| Global Round : 5 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.001022
| Global Round : 5 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.004771
| Global Round : 5 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.083131
| Global Round : 5 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.002100
| Global Round : 5 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.001242
| Global Round : 5 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000417
| Global Round : 5 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000146
| Global Round : 5 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.166650
| Global Round : 5 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.001188
| Global Round : 5 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000583
| Global Round : 5 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.006478
| Global Round : 5 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.001108
| Global Round : 5 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.002676
| Global Round : 5 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000100
| Global Round : 5 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.003292
| Global Round : 5 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000123
| Global Round : 5 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.047618
| Global Round : 5 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.001700
| Global Round : 5 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.013886
| Global Round : 5 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.012355
| Global Round : 5 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.068992
| Global Round : 5 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000711
| Global Round : 5 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000715
| Global Round : 5 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000147
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6]})
Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 49.70 %
----Attack Model Testing----
Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

------------------------------------------
------User: 2, Epoch: 5--------
------------------------------------------
| Global Round : 5 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.001448
| Global Round : 5 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.010858
| Global Round : 5 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.001105
| Global Round : 5 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.003470
| Global Round : 5 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000305
| Global Round : 5 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.003569
| Global Round : 5 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.011858
| Global Round : 5 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.004506
| Global Round : 5 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.004438
| Global Round : 5 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.002664
| Global Round : 5 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000760
| Global Round : 5 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.053532
| Global Round : 5 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000310
| Global Round : 5 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.002241
| Global Round : 5 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.104241
| Global Round : 5 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.331706
| Global Round : 5 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.002011
| Global Round : 5 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.003378
| Global Round : 5 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.008853
| Global Round : 5 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.003023
| Global Round : 5 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000917
| Global Round : 5 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.083484
| Global Round : 5 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.001471
| Global Round : 5 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.132239
| Global Round : 5 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.006846
| Global Round : 5 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000511
| Global Round : 5 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.022605
| Global Round : 5 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.073005
| Global Round : 5 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.001039
| Global Round : 5 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.002942
| Global Round : 5 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.013149
| Global Round : 5 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.002074
| Global Round : 5 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.021252
| Global Round : 5 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.025763
| Global Round : 5 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.029364
| Global Round : 5 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.004030
| Global Round : 5 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.010788
| Global Round : 5 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000672
| Global Round : 5 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.001235
| Global Round : 5 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.081569
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6]})
------------------------------------------
------User: 1, Epoch: 5--------
------------------------------------------
| Global Round : 5 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.011168
| Global Round : 5 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.006141
| Global Round : 5 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.003271
| Global Round : 5 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.006096
| Global Round : 5 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.004329
| Global Round : 5 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.005926
| Global Round : 5 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000524
| Global Round : 5 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000065
| Global Round : 5 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000596
| Global Round : 5 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.009650
| Global Round : 5 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000169
| Global Round : 5 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.008127
| Global Round : 5 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000931
| Global Round : 5 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.002191
| Global Round : 5 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.001309
| Global Round : 5 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.019671
| Global Round : 5 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000382
| Global Round : 5 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000233
| Global Round : 5 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.062025
| Global Round : 5 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.007251
| Global Round : 5 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.061367
| Global Round : 5 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.001100
| Global Round : 5 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.022006
| Global Round : 5 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.022175
| Global Round : 5 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.002037
| Global Round : 5 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.007396
| Global Round : 5 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.003569
| Global Round : 5 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000093
| Global Round : 5 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.001010
| Global Round : 5 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.004082
| Global Round : 5 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000345
| Global Round : 5 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.040250
| Global Round : 5 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.008041
| Global Round : 5 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.178135
| Global Round : 5 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.133616
| Global Round : 5 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.121387
| Global Round : 5 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.002944
| Global Round : 5 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.019120
| Global Round : 5 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.001078
| Global Round : 5 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.003402
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6]})
------------------------------------------
------User: 8, Epoch: 5--------
------------------------------------------
| Global Round : 5 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.005862
| Global Round : 5 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000547
| Global Round : 5 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000218
| Global Round : 5 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000403
| Global Round : 5 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.001737
| Global Round : 5 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.011758
| Global Round : 5 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.044600
| Global Round : 5 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.024068
| Global Round : 5 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.002781
| Global Round : 5 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.003477
| Global Round : 5 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.005128
| Global Round : 5 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.307928
| Global Round : 5 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.414963
| Global Round : 5 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000242
| Global Round : 5 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.003584
| Global Round : 5 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000785
| Global Round : 5 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.005189
| Global Round : 5 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.029745
| Global Round : 5 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.002554
| Global Round : 5 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000941
| Global Round : 5 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.005397
| Global Round : 5 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.032474
| Global Round : 5 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000484
| Global Round : 5 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.001188
| Global Round : 5 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.001209
| Global Round : 5 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.007443
| Global Round : 5 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000014
| Global Round : 5 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.002436
| Global Round : 5 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000645
| Global Round : 5 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.005831
| Global Round : 5 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.001633
| Global Round : 5 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.005390
| Global Round : 5 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000128
| Global Round : 5 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.084537
| Global Round : 5 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.003466
| Global Round : 5 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.028382
| Global Round : 5 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.002566
| Global Round : 5 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.005096
| Global Round : 5 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.031414
| Global Round : 5 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.011519
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6]})
------------------------------------------
------User: 3, Epoch: 5--------
------------------------------------------
| Global Round : 5 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000229
| Global Round : 5 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.003917
| Global Round : 5 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000500
| Global Round : 5 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.027984
| Global Round : 5 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.005775
| Global Round : 5 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000424
| Global Round : 5 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.003735
| Global Round : 5 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000390
| Global Round : 5 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.002015
| Global Round : 5 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.001976
| Global Round : 5 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.001620
| Global Round : 5 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.011709
| Global Round : 5 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000676
| Global Round : 5 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.009303
| Global Round : 5 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.032153
| Global Round : 5 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000461
| Global Round : 5 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.137874
| Global Round : 5 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.014129
| Global Round : 5 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000207
| Global Round : 5 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.003589
| Global Round : 5 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.020296
| Global Round : 5 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000276
| Global Round : 5 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.028442
| Global Round : 5 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000644
| Global Round : 5 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000667
| Global Round : 5 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.002972
| Global Round : 5 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.002201
| Global Round : 5 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.031790
| Global Round : 5 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.027675
| Global Round : 5 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.016126
| Global Round : 5 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000277
| Global Round : 5 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.003975
| Global Round : 5 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.031453
| Global Round : 5 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.010593
| Global Round : 5 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000954
| Global Round : 5 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.001797
| Global Round : 5 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000515
| Global Round : 5 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.003304
| Global Round : 5 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.331020
| Global Round : 5 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.004968
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6]})
------------------------------------------
------User: 4, Epoch: 5--------
------------------------------------------
| Global Round : 5 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000223
| Global Round : 5 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.301733
| Global Round : 5 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.018791
| Global Round : 5 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.001240
| Global Round : 5 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.025988
| Global Round : 5 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000214
| Global Round : 5 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.144873
| Global Round : 5 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.026251
| Global Round : 5 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.008415
| Global Round : 5 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000845
| Global Round : 5 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.002361
| Global Round : 5 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000320
| Global Round : 5 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.017848
| Global Round : 5 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.022758
| Global Round : 5 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.037591
| Global Round : 5 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.002221
| Global Round : 5 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000191
| Global Round : 5 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.002393
| Global Round : 5 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.009591
| Global Round : 5 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.002175
| Global Round : 5 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.022517
| Global Round : 5 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.056541
| Global Round : 5 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.001415
| Global Round : 5 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.003975
| Global Round : 5 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000146
| Global Round : 5 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.001437
| Global Round : 5 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.003605
| Global Round : 5 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.015182
| Global Round : 5 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.001280
| Global Round : 5 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000461
| Global Round : 5 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000206
| Global Round : 5 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.001225
| Global Round : 5 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.003712
| Global Round : 5 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.054326
| Global Round : 5 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.002131
| Global Round : 5 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.085582
| Global Round : 5 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.217508
| Global Round : 5 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000446
| Global Round : 5 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.001057
| Global Round : 5 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.002974
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4]})
------------------------------------------
------User: 5, Epoch: 5--------
------------------------------------------
| Global Round : 5 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000070
| Global Round : 5 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.003636
| Global Round : 5 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000353
| Global Round : 5 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.009663
| Global Round : 5 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.042391
| Global Round : 5 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000218
| Global Round : 5 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000046
| Global Round : 5 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000269
| Global Round : 5 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.003652
| Global Round : 5 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.005142
| Global Round : 5 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.014210
| Global Round : 5 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.006642
| Global Round : 5 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.001515
| Global Round : 5 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.006174
| Global Round : 5 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.001108
| Global Round : 5 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000318
| Global Round : 5 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.504847
| Global Round : 5 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000563
| Global Round : 5 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.005216
| Global Round : 5 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.006126
| Global Round : 5 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.002036
| Global Round : 5 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.092514
| Global Round : 5 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.085764
| Global Round : 5 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000320
| Global Round : 5 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000218
| Global Round : 5 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.038848
| Global Round : 5 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.004807
| Global Round : 5 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000107
| Global Round : 5 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000212
| Global Round : 5 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.020757
| Global Round : 5 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.001551
| Global Round : 5 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.001629
| Global Round : 5 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000591
| Global Round : 5 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.002659
| Global Round : 5 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.004588
| Global Round : 5 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000127
| Global Round : 5 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.005933
| Global Round : 5 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.002375
| Global Round : 5 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000291
| Global Round : 5 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.020593
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4]})
------------------------------------------
------User: 6, Epoch: 5--------
------------------------------------------
| Global Round : 5 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000755
| Global Round : 5 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.001422
| Global Round : 5 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000149
| Global Round : 5 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.035983
| Global Round : 5 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.006915
| Global Round : 5 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000700
| Global Round : 5 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000366
| Global Round : 5 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.001568
| Global Round : 5 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.001563
| Global Round : 5 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.223100
| Global Round : 5 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000752
| Global Round : 5 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.016883
| Global Round : 5 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.003733
| Global Round : 5 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000828
| Global Round : 5 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.007597
| Global Round : 5 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000289
| Global Round : 5 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.025796
| Global Round : 5 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.019800
| Global Round : 5 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.004387
| Global Round : 5 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000757
| Global Round : 5 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.003042
| Global Round : 5 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.001331
| Global Round : 5 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.003939
| Global Round : 5 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.001444
| Global Round : 5 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.017454
| Global Round : 5 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.054831
| Global Round : 5 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.001140
| Global Round : 5 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.014990
| Global Round : 5 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000727
| Global Round : 5 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000689
| Global Round : 5 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.058366
| Global Round : 5 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.068738
| Global Round : 5 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.023222
| Global Round : 5 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.019256
| Global Round : 5 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.014581
| Global Round : 5 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.001443
| Global Round : 5 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.004116
| Global Round : 5 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000653
| Global Round : 5 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.100595
| Global Round : 5 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000243
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4]})
 
Avg Training Stats after 6 global rounds:
Training Loss : nan
Train Accuracy: 9910.00% 

Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 49.72% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.70%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 49.70 %
----Attack Model Testing----
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
 20%|        | 6/30 [22:18<1:29:38, 224.09s/it]Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

Selected users for the training: [9 5 1 8 3 6 2 4 7 0]
------------------------------------------
------User: 9, Epoch: 6--------
------------------------------------------
| Global Round : 6 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.011056
| Global Round : 6 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000999
| Global Round : 6 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000696
| Global Round : 6 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.008762
| Global Round : 6 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.050247
| Global Round : 6 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.001928
| Global Round : 6 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000778
| Global Round : 6 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000490
| Global Round : 6 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000529
| Global Round : 6 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.069296
| Global Round : 6 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.002245
| Global Round : 6 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000882
| Global Round : 6 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.003817
| Global Round : 6 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000112
| Global Round : 6 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.030944
| Global Round : 6 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.029289
| Global Round : 6 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.020301
| Global Round : 6 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.001568
| Global Round : 6 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.001973
| Global Round : 6 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000187
| Global Round : 6 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.057696
| Global Round : 6 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.003386
| Global Round : 6 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.013753
| Global Round : 6 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000781
| Global Round : 6 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000286
| Global Round : 6 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.004598
| Global Round : 6 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000047
| Global Round : 6 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.001138
| Global Round : 6 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000334
| Global Round : 6 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000407
| Global Round : 6 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000418
| Global Round : 6 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.002007
| Global Round : 6 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.004697
| Global Round : 6 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.004022
| Global Round : 6 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.061899
| Global Round : 6 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.143481
| Global Round : 6 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.005727
| Global Round : 6 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000162
| Global Round : 6 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.009991
| Global Round : 6 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.004056
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4]})
------------------------------------------
------User: 5, Epoch: 6--------
------------------------------------------
| Global Round : 6 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000393
| Global Round : 6 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.064448
| Global Round : 6 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000089
| Global Round : 6 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000543
| Global Round : 6 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.001179
| Global Round : 6 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.004486
| Global Round : 6 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000328
| Global Round : 6 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.149790
| Global Round : 6 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.044320
| Global Round : 6 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000296
| Global Round : 6 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.033719
| Global Round : 6 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.007368
| Global Round : 6 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.001115
| Global Round : 6 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.008272
| Global Round : 6 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.004535
| Global Round : 6 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000229
| Global Round : 6 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.013579
| Global Round : 6 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.003108
| Global Round : 6 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.001117
| Global Round : 6 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.004089
| Global Round : 6 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000489
| Global Round : 6 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.002824
| Global Round : 6 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.032955
| Global Round : 6 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.001416
| Global Round : 6 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000920
| Global Round : 6 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000830
| Global Round : 6 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000530
| Global Round : 6 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.203496
| Global Round : 6 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.007486
| Global Round : 6 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.249231
| Global Round : 6 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.030387
| Global Round : 6 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.001012
| Global Round : 6 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000189
| Global Round : 6 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000915
| Global Round : 6 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000514
| Global Round : 6 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000340
| Global Round : 6 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.026163
| Global Round : 6 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.004957
| Global Round : 6 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.005896
| Global Round : 6 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.017110
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4]})
------------------------------------------
------User: 1, Epoch: 6--------
------------------------------------------
| Global Round : 6 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.001986
| Global Round : 6 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.003541
| Global Round : 6 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.004337
| Global Round : 6 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000870
| Global Round : 6 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000650
| Global Round : 6 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000214
| Global Round : 6 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000725
| Global Round : 6 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.002118
| Global Round : 6 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.005802
| Global Round : 6 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000080
| Global Round : 6 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.001849
| Global Round : 6 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000594
| Global Round : 6 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000250
| Global Round : 6 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000695
| Global Round : 6 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000357
| Global Round : 6 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000375
| Global Round : 6 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000359
| Global Round : 6 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.024006
| Global Round : 6 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000911
| Global Round : 6 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000592
| Global Round : 6 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.059737
| Global Round : 6 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.015820
| Global Round : 6 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.016370
| Global Round : 6 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000455
| Global Round : 6 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.001367
| Global Round : 6 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.001505
| Global Round : 6 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.032651
| Global Round : 6 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.065364
| Global Round : 6 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.006888
| Global Round : 6 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000876
| Global Round : 6 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.005155
| Global Round : 6 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.028059
| Global Round : 6 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.065300
| Global Round : 6 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000708
| Global Round : 6 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.001805
| Global Round : 6 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000477
| Global Round : 6 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.075576
| Global Round : 6 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.013766
| Global Round : 6 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.018329
| Global Round : 6 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.060090
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4]})
------------------------------------------
------User: 8, Epoch: 6--------
------------------------------------------
| Global Round : 6 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.034549
| Global Round : 6 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.002513
| Global Round : 6 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.003976
| Global Round : 6 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.283506
| Global Round : 6 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.015034
| Global Round : 6 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.015805
| Global Round : 6 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000676
| Global Round : 6 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.002538
| Global Round : 6 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000531
| Global Round : 6 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.004503
| Global Round : 6 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.003203
| Global Round : 6 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.122656
| Global Round : 6 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.001692
| Global Round : 6 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.003999
| Global Round : 6 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.083048
| Global Round : 6 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.003133
| Global Round : 6 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.007436
| Global Round : 6 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.049614
| Global Round : 6 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.135662
| Global Round : 6 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.004415
| Global Round : 6 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.047147
| Global Round : 6 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.009536
| Global Round : 6 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.084895
| Global Round : 6 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000577
| Global Round : 6 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.006441
| Global Round : 6 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000342
| Global Round : 6 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000106
| Global Round : 6 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.001074
| Global Round : 6 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.011960
| Global Round : 6 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.001417
| Global Round : 6 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.005176
| Global Round : 6 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000914
| Global Round : 6 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.004383
| Global Round : 6 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000094
| Global Round : 6 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000356
| Global Round : 6 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000467
| Global Round : 6 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.001845
| Global Round : 6 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000732
| Global Round : 6 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.001578
| Global Round : 6 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.003489
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4]})
------------------------------------------
------User: 3, Epoch: 6--------
------------------------------------------
| Global Round : 6 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000322
| Global Round : 6 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000308
| Global Round : 6 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.002390
| Global Round : 6 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.009182
| Global Round : 6 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000210
| Global Round : 6 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000809
| Global Round : 6 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.003443
| Global Round : 6 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.015122
| Global Round : 6 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.004911
| Global Round : 6 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000200
| Global Round : 6 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000749
| Global Round : 6 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.188476
| Global Round : 6 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000260
| Global Round : 6 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.091061
| Global Round : 6 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.002259
| Global Round : 6 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.012464
| Global Round : 6 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.031669
| Global Round : 6 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.002698
| Global Round : 6 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.003517
| Global Round : 6 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.051542
| Global Round : 6 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000143
| Global Round : 6 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.028633
| Global Round : 6 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000482
| Global Round : 6 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000484
| Global Round : 6 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.020907
| Global Round : 6 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000372
| Global Round : 6 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000914
| Global Round : 6 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.097584
| Global Round : 6 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.004008
| Global Round : 6 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.004923
| Global Round : 6 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.003163
| Global Round : 6 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.005257
| Global Round : 6 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.005308
| Global Round : 6 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.006076
| Global Round : 6 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.001913
| Global Round : 6 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000884
| Global Round : 6 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.035515
| Global Round : 6 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.005494
| Global Round : 6 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000773
| Global Round : 6 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.069741
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4]})
------------------------------------------
------User: 6, Epoch: 6--------
------------------------------------------
| Global Round : 6 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000483
| Global Round : 6 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000245
| Global Round : 6 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000424
| Global Round : 6 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.005105
| Global Round : 6 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.006183
| Global Round : 6 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.018365
| Global Round : 6 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.005422
| Global Round : 6 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.006133
| Global Round : 6 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.001177
| Global Round : 6 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.003137
| Global Round : 6 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.012448
| Global Round : 6 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000195
| Global Round : 6 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000473
| Global Round : 6 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.001011
| Global Round : 6 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.001337
| Global Round : 6 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000093
| Global Round : 6 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000395
| Global Round : 6 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.005105
| Global Round : 6 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000079
| Global Round : 6 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.003215
| Global Round : 6 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.009879
| Global Round : 6 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.092214
| Global Round : 6 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.006662
| Global Round : 6 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.004651
| Global Round : 6 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000588
| Global Round : 6 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000175
| Global Round : 6 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.008882
| Global Round : 6 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.003057
| Global Round : 6 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.086367
| Global Round : 6 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.001263
| Global Round : 6 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000221
| Global Round : 6 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000416
| Global Round : 6 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.007773
| Global Round : 6 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000485
| Global Round : 6 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.004260
| Global Round : 6 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.013449
| Global Round : 6 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000198
| Global Round : 6 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.002400
| Global Round : 6 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000462
| Global Round : 6 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.429405
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4]})
------------------------------------------
------User: 2, Epoch: 6--------
------------------------------------------
| Global Round : 6 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.004712
| Global Round : 6 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.001593
| Global Round : 6 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000204
| Global Round : 6 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.012351
| Global Round : 6 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000141
| Global Round : 6 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.039077
| Global Round : 6 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.121043
| Global Round : 6 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.004244
| Global Round : 6 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.004853
| Global Round : 6 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.017836
| Global Round : 6 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.001422
| Global Round : 6 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000549
| Global Round : 6 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000308
| Global Round : 6 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.227225
| Global Round : 6 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.018215
| Global Round : 6 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.002547
| Global Round : 6 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.003696
| Global Round : 6 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.001197
| Global Round : 6 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000074
| Global Round : 6 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000401
| Global Round : 6 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.001352
| Global Round : 6 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000054
| Global Round : 6 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.069749
| Global Round : 6 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.003045
| Global Round : 6 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.004048
| Global Round : 6 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.027798
| Global Round : 6 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.015944
| Global Round : 6 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.073637
| Global Round : 6 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.009754
| Global Round : 6 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000481
| Global Round : 6 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.009776
| Global Round : 6 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.008341
| Global Round : 6 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.296837
| Global Round : 6 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000261
| Global Round : 6 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000478
| Global Round : 6 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.019254
| Global Round : 6 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.017250
| Global Round : 6 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.001406
| Global Round : 6 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.003203
| Global Round : 6 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.001836
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4]})
------------------------------------------
------User: 4, Epoch: 6--------
------------------------------------------
| Global Round : 6 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000066
| Global Round : 6 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.004157
| Global Round : 6 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000077
| Global Round : 6 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000911
| Global Round : 6 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000473
| Global Round : 6 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000511
| Global Round : 6 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000295
| Global Round : 6 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.026493
| Global Round : 6 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.005306
| Global Round : 6 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.011493
| Global Round : 6 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.001367
| Global Round : 6 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000069
| Global Round : 6 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.010840
| Global Round : 6 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000070
| Global Round : 6 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.012500
| Global Round : 6 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.016157
| Global Round : 6 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.010001
| Global Round : 6 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000034
| Global Round : 6 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.001357
| Global Round : 6 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.001919
| Global Round : 6 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000393
| Global Round : 6 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.164167
| Global Round : 6 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.003946
| Global Round : 6 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000261
| Global Round : 6 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.001277
| Global Round : 6 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.090397
| Global Round : 6 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000251
| Global Round : 6 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000546
| Global Round : 6 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.002600
| Global Round : 6 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.259598
| Global Round : 6 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000897
| Global Round : 6 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.003784
| Global Round : 6 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.017159
| Global Round : 6 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.002712
| Global Round : 6 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.001330
| Global Round : 6 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.010190
| Global Round : 6 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.003245
| Global Round : 6 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.001524
| Global Round : 6 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000451
| Global Round : 6 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000459
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0]})
------------------------------------------
------User: 7, Epoch: 6--------
------------------------------------------
| Global Round : 6 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000218
| Global Round : 6 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.003775
| Global Round : 6 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.001205
| Global Round : 6 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.019423
| Global Round : 6 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.013355
| Global Round : 6 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.013906
| Global Round : 6 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.203752
| Global Round : 6 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.016828
| Global Round : 6 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.005559
| Global Round : 6 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000412
| Global Round : 6 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.023513
| Global Round : 6 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.001327
| Global Round : 6 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.004327
| Global Round : 6 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.008924
| Global Round : 6 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000350
| Global Round : 6 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.080677
| Global Round : 6 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.002513
| Global Round : 6 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.007528
| Global Round : 6 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.002810
| Global Round : 6 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.004969
| Global Round : 6 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.004235
| Global Round : 6 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.001091
| Global Round : 6 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000045
| Global Round : 6 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000177
| Global Round : 6 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000141
| Global Round : 6 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.003936
| Global Round : 6 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.001355
| Global Round : 6 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000141
| Global Round : 6 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000117
| Global Round : 6 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000371
| Global Round : 6 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000118
| Global Round : 6 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.004116
| Global Round : 6 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.008762
| Global Round : 6 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.012809
| Global Round : 6 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.001500
| Global Round : 6 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.001004
| Global Round : 6 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.021770
| Global Round : 6 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000083
| Global Round : 6 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.004371
| Global Round : 6 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.002715
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0]})
------------------------------------------
------User: 0, Epoch: 6--------
------------------------------------------
| Global Round : 6 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.007088
| Global Round : 6 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.028645
| Global Round : 6 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.002462
| Global Round : 6 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.009067
| Global Round : 6 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.010031
| Global Round : 6 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.146120
| Global Round : 6 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000765
| Global Round : 6 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.007723
| Global Round : 6 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.004870
| Global Round : 6 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000988
| Global Round : 6 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.037326
| Global Round : 6 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.006799
| Global Round : 6 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000413
| Global Round : 6 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.001762
| Global Round : 6 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000376
| Global Round : 6 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.084682
| Global Round : 6 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.681002
| Global Round : 6 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.003713
| Global Round : 6 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.088433
| Global Round : 6 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000178
| Global Round : 6 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.001122
| Global Round : 6 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000906
| Global Round : 6 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.001217
| Global Round : 6 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.006208
| Global Round : 6 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.001622
| Global Round : 6 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000543
| Global Round : 6 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.001044
| Global Round : 6 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.006849
| Global Round : 6 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.019491
| Global Round : 6 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.005932
| Global Round : 6 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.003178
| Global Round : 6 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.014263
| Global Round : 6 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000160
| Global Round : 6 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000101
| Global Round : 6 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.008640
| Global Round : 6 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.007552
| Global Round : 6 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.008785
| Global Round : 6 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000075
| Global Round : 6 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.007533
| Global Round : 6 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.034172
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0]})
Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.10%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.10%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.10%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.10%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.10%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.10%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.10%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.10%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.10%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.10%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.10%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.10%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.10%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.10%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.10%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.10%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.10%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.10%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.10%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.10%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.10%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.10%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.10%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.10%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.10%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.10%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.10%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.10%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.10%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.10%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.10%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.10%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.10%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.10%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.10%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.10%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.10%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.10%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.10%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.10%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.10%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.10%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.10%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.10%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.10%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.10%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.10%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.10%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.10%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.10%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 49.10 %
----Attack Model Testing----
Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

 
Avg Training Stats after 7 global rounds:
Training Loss : nan
Train Accuracy: 9921.71% 

Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 49.86% | Val Loss: nan | Val Acc: 46.70%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 50.14% | Val Loss: nan | Val Acc: 46.70%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 50.14% | Val Loss: nan | Val Acc: 46.70%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 50.14% | Val Loss: nan | Val Acc: 46.70%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 50.14% | Val Loss: nan | Val Acc: 46.70%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 50.14% | Val Loss: nan | Val Acc: 46.70%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 50.14% | Val Loss: nan | Val Acc: 46.70%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 50.14% | Val Loss: nan | Val Acc: 46.70%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 50.14% | Val Loss: nan | Val Acc: 46.70%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 50.14% | Val Loss: nan | Val Acc: 46.70%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 50.14% | Val Loss: nan | Val Acc: 46.70%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 50.14% | Val Loss: nan | Val Acc: 46.70%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 50.14% | Val Loss: nan | Val Acc: 46.70%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 50.14% | Val Loss: nan | Val Acc: 46.70%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 50.14% | Val Loss: nan | Val Acc: 46.70%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 50.14% | Val Loss: nan | Val Acc: 46.70%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 50.14% | Val Loss: nan | Val Acc: 46.70%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 50.14% | Val Loss: nan | Val Acc: 46.70%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 50.14% | Val Loss: nan | Val Acc: 46.70%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 50.14% | Val Loss: nan | Val Acc: 46.70%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 50.14% | Val Loss: nan | Val Acc: 46.70%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 50.14% | Val Loss: nan | Val Acc: 46.70%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 50.14% | Val Loss: nan | Val Acc: 46.70%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 50.14% | Val Loss: nan | Val Acc: 46.70%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 50.14% | Val Loss: nan | Val Acc: 46.70%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 50.14% | Val Loss: nan | Val Acc: 46.70%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 50.14% | Val Loss: nan | Val Acc: 46.70%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 50.14% | Val Loss: nan | Val Acc: 46.70%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 50.14% | Val Loss: nan | Val Acc: 46.70%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 50.14% | Val Loss: nan | Val Acc: 46.70%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 50.14% | Val Loss: nan | Val Acc: 46.70%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 50.14% | Val Loss: nan | Val Acc: 46.70%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 50.14% | Val Loss: nan | Val Acc: 46.70%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 50.14% | Val Loss: nan | Val Acc: 46.70%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 50.14% | Val Loss: nan | Val Acc: 46.70%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 50.14% | Val Loss: nan | Val Acc: 46.70%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 50.14% | Val Loss: nan | Val Acc: 46.70%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 50.14% | Val Loss: nan | Val Acc: 46.70%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 50.14% | Val Loss: nan | Val Acc: 46.70%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 50.14% | Val Loss: nan | Val Acc: 46.70%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 50.14% | Val Loss: nan | Val Acc: 46.70%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 50.14% | Val Loss: nan | Val Acc: 46.70%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 50.14% | Val Loss: nan | Val Acc: 46.70%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 50.14% | Val Loss: nan | Val Acc: 46.70%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 50.14% | Val Loss: nan | Val Acc: 46.70%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 50.14% | Val Loss: nan | Val Acc: 46.70%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 50.14% | Val Loss: nan | Val Acc: 46.70%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 50.14% | Val Loss: nan | Val Acc: 46.70%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 50.14% | Val Loss: nan | Val Acc: 46.70%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 50.14% | Val Loss: nan | Val Acc: 46.70%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 46.70 %
----Attack Model Testing----
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
 23%|       | 7/30 [25:59<1:25:32, 223.17s/it]Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

Selected users for the training: [3 4 2 8 5 9 6 1 0 7]
------------------------------------------
------User: 3, Epoch: 7--------
------------------------------------------
| Global Round : 7 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.002565
| Global Round : 7 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.001054
| Global Round : 7 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.104048
| Global Round : 7 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000066
| Global Round : 7 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.002969
| Global Round : 7 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.007355
| Global Round : 7 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.001435
| Global Round : 7 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000388
| Global Round : 7 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.010589
| Global Round : 7 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.001648
| Global Round : 7 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000703
| Global Round : 7 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000110
| Global Round : 7 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.254605
| Global Round : 7 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.001088
| Global Round : 7 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.008329
| Global Round : 7 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.001993
| Global Round : 7 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000317
| Global Round : 7 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.003933
| Global Round : 7 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.025428
| Global Round : 7 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.001772
| Global Round : 7 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.013998
| Global Round : 7 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.008426
| Global Round : 7 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.010591
| Global Round : 7 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.009027
| Global Round : 7 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000852
| Global Round : 7 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.017074
| Global Round : 7 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.012005
| Global Round : 7 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000671
| Global Round : 7 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.001189
| Global Round : 7 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.004083
| Global Round : 7 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000055
| Global Round : 7 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000144
| Global Round : 7 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.006066
| Global Round : 7 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.032494
| Global Round : 7 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000115
| Global Round : 7 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.001024
| Global Round : 7 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000808
| Global Round : 7 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000115
| Global Round : 7 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000399
| Global Round : 7 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.019308
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0]})
------------------------------------------
------User: 4, Epoch: 7--------
------------------------------------------
| Global Round : 7 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000611
| Global Round : 7 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.001726
| Global Round : 7 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.014405
| Global Round : 7 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.001929
| Global Round : 7 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.005565
| Global Round : 7 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.001881
| Global Round : 7 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000209
| Global Round : 7 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.001355
| Global Round : 7 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.001052
| Global Round : 7 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.001051
| Global Round : 7 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.001281
| Global Round : 7 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.181859
| Global Round : 7 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.012966
| Global Round : 7 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.007379
| Global Round : 7 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.007256
| Global Round : 7 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.007784
| Global Round : 7 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.002655
| Global Round : 7 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.005720
| Global Round : 7 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000232
| Global Round : 7 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000176
| Global Round : 7 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.004203
| Global Round : 7 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000859
| Global Round : 7 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000378
| Global Round : 7 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000266
| Global Round : 7 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.002288
| Global Round : 7 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.058739
| Global Round : 7 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.001792
| Global Round : 7 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000294
| Global Round : 7 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.002480
| Global Round : 7 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.019359
| Global Round : 7 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000865
| Global Round : 7 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000325
| Global Round : 7 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.007595
| Global Round : 7 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000718
| Global Round : 7 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.008181
| Global Round : 7 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.004484
| Global Round : 7 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000581
| Global Round : 7 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.001244
| Global Round : 7 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.007550
| Global Round : 7 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.044804
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0]})
------------------------------------------
------User: 2, Epoch: 7--------
------------------------------------------
| Global Round : 7 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000676
| Global Round : 7 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.003579
| Global Round : 7 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000927
| Global Round : 7 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000471
| Global Round : 7 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.012847
| Global Round : 7 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000038
| Global Round : 7 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.009183
| Global Round : 7 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.006817
| Global Round : 7 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000228
| Global Round : 7 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000738
| Global Round : 7 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.001416
| Global Round : 7 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.022382
| Global Round : 7 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.011693
| Global Round : 7 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.009604
| Global Round : 7 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.034833
| Global Round : 7 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.002323
| Global Round : 7 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.044839
| Global Round : 7 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000678
| Global Round : 7 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.037414
| Global Round : 7 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.073922
| Global Round : 7 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.001015
| Global Round : 7 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000433
| Global Round : 7 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.023133
| Global Round : 7 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000411
| Global Round : 7 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000687
| Global Round : 7 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000231
| Global Round : 7 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.016957
| Global Round : 7 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.002588
| Global Round : 7 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000537
| Global Round : 7 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.059363
| Global Round : 7 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.010177
| Global Round : 7 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.015599
| Global Round : 7 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000507
| Global Round : 7 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.029819
| Global Round : 7 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000294
| Global Round : 7 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000017
| Global Round : 7 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000049
| Global Round : 7 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000018
| Global Round : 7 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.003287
| Global Round : 7 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000446
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0]})
------------------------------------------
------User: 8, Epoch: 7--------
------------------------------------------
| Global Round : 7 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.002148
| Global Round : 7 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000148
| Global Round : 7 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.001271
| Global Round : 7 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.010756
| Global Round : 7 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.001413
| Global Round : 7 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.013285
| Global Round : 7 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.066732
| Global Round : 7 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.004715
| Global Round : 7 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000090
| Global Round : 7 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.002757
| Global Round : 7 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000164
| Global Round : 7 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.003991
| Global Round : 7 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000740
| Global Round : 7 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000454
| Global Round : 7 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.002930
| Global Round : 7 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000340
| Global Round : 7 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000246
| Global Round : 7 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.002100
| Global Round : 7 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.002488
| Global Round : 7 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.012850
| Global Round : 7 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000303
| Global Round : 7 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.003497
| Global Round : 7 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000930
| Global Round : 7 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.021257
| Global Round : 7 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.018918
| Global Round : 7 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.007865
| Global Round : 7 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000059
| Global Round : 7 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000250
| Global Round : 7 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.005433
| Global Round : 7 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.006561
| Global Round : 7 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.015076
| Global Round : 7 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000080
| Global Round : 7 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.001943
| Global Round : 7 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000060
| Global Round : 7 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.002477
| Global Round : 7 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.617577
| Global Round : 7 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.006963
| Global Round : 7 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.013032
| Global Round : 7 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.005154
| Global Round : 7 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000171
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0]})
------------------------------------------
------User: 5, Epoch: 7--------
------------------------------------------
| Global Round : 7 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000838
| Global Round : 7 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.004053
| Global Round : 7 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000720
| Global Round : 7 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000405
| Global Round : 7 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.005735
| Global Round : 7 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.003790
| Global Round : 7 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000256
| Global Round : 7 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000734
| Global Round : 7 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000349
| Global Round : 7 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.012892
| Global Round : 7 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.004117
| Global Round : 7 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000219
| Global Round : 7 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.008171
| Global Round : 7 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.003430
| Global Round : 7 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000507
| Global Round : 7 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000079
| Global Round : 7 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000338
| Global Round : 7 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000093
| Global Round : 7 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000974
| Global Round : 7 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000393
| Global Round : 7 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.008300
| Global Round : 7 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.005938
| Global Round : 7 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.008080
| Global Round : 7 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000619
| Global Round : 7 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.003937
| Global Round : 7 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.001572
| Global Round : 7 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.023963
| Global Round : 7 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.003946
| Global Round : 7 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.041730
| Global Round : 7 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.001551
| Global Round : 7 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.013357
| Global Round : 7 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000914
| Global Round : 7 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.020093
| Global Round : 7 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.015195
| Global Round : 7 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.025016
| Global Round : 7 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000482
| Global Round : 7 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.013687
| Global Round : 7 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.001450
| Global Round : 7 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.002381
| Global Round : 7 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000095
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0]})
------------------------------------------
------User: 9, Epoch: 7--------
------------------------------------------
| Global Round : 7 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.010853
| Global Round : 7 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.006241
| Global Round : 7 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000097
| Global Round : 7 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000179
| Global Round : 7 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000439
| Global Round : 7 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.004655
| Global Round : 7 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000292
| Global Round : 7 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.059682
| Global Round : 7 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000224
| Global Round : 7 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000503
| Global Round : 7 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.008199
| Global Round : 7 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.003826
| Global Round : 7 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.145757
| Global Round : 7 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000344
| Global Round : 7 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.001083
| Global Round : 7 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.002616
| Global Round : 7 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.045067
| Global Round : 7 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.016686
| Global Round : 7 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000802
| Global Round : 7 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000167
| Global Round : 7 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.001121
| Global Round : 7 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.001799
| Global Round : 7 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.008345
| Global Round : 7 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.006118
| Global Round : 7 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.018326
| Global Round : 7 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.009216
| Global Round : 7 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000427
| Global Round : 7 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.082487
| Global Round : 7 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.028555
| Global Round : 7 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.008500
| Global Round : 7 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.003168
| Global Round : 7 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000196
| Global Round : 7 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000587
| Global Round : 7 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000362
| Global Round : 7 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.002870
| Global Round : 7 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.002677
| Global Round : 7 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000821
| Global Round : 7 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000253
| Global Round : 7 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000466
| Global Round : 7 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000666
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0]})
------------------------------------------
------User: 6, Epoch: 7--------
------------------------------------------
| Global Round : 7 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000823
| Global Round : 7 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000063
| Global Round : 7 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.036699
| Global Round : 7 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.030889
| Global Round : 7 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.001785
| Global Round : 7 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.111117
| Global Round : 7 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000381
| Global Round : 7 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.121213
| Global Round : 7 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.003519
| Global Round : 7 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000512
| Global Round : 7 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.001873
| Global Round : 7 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000610
| Global Round : 7 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.004159
| Global Round : 7 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.066368
| Global Round : 7 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.001681
| Global Round : 7 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.001028
| Global Round : 7 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.020138
| Global Round : 7 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000206
| Global Round : 7 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.028523
| Global Round : 7 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.006226
| Global Round : 7 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000205
| Global Round : 7 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.001400
| Global Round : 7 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.417096
| Global Round : 7 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000315
| Global Round : 7 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.002553
| Global Round : 7 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.002340
| Global Round : 7 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000141
| Global Round : 7 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.052984
| Global Round : 7 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.010204
| Global Round : 7 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.001167
| Global Round : 7 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.121477
| Global Round : 7 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.019825
| Global Round : 7 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.018608
| Global Round : 7 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.247410
| Global Round : 7 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.025216
| Global Round : 7 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000026
| Global Round : 7 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.038709
| Global Round : 7 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000411
| Global Round : 7 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000297
| Global Round : 7 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.039332
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0]})
------------------------------------------
------User: 1, Epoch: 7--------
------------------------------------------
| Global Round : 7 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.006273
| Global Round : 7 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000389
| Global Round : 7 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.015013
| Global Round : 7 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.064297
| Global Round : 7 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.006975
| Global Round : 7 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.001153
| Global Round : 7 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.002648
| Global Round : 7 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000970
| Global Round : 7 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.007729
| Global Round : 7 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.009604
| Global Round : 7 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.001522
| Global Round : 7 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.002835
| Global Round : 7 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.002065
| Global Round : 7 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.045032
| Global Round : 7 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.001201
| Global Round : 7 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000457
| Global Round : 7 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.061751
| Global Round : 7 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.016014
| Global Round : 7 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000468
| Global Round : 7 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000084
| Global Round : 7 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000628
| Global Round : 7 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.002892
| Global Round : 7 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000005
| Global Round : 7 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000214
| Global Round : 7 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.001745
| Global Round : 7 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000103
| Global Round : 7 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.006861
| Global Round : 7 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.072023
| Global Round : 7 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000067
| Global Round : 7 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.035528
| Global Round : 7 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.001193
| Global Round : 7 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.003231
| Global Round : 7 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000802
| Global Round : 7 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000750
| Global Round : 7 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.010897
| Global Round : 7 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.183868
| Global Round : 7 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.025298
| Global Round : 7 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.004224
| Global Round : 7 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.001415
| Global Round : 7 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.001760
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0]})
------------------------------------------
------User: 0, Epoch: 7--------
------------------------------------------
| Global Round : 7 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.013865
| Global Round : 7 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.011768
| Global Round : 7 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.001042
| Global Round : 7 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000230
| Global Round : 7 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.002462
| Global Round : 7 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.006576
| Global Round : 7 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.009309
| Global Round : 7 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000758
| Global Round : 7 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.001017
| Global Round : 7 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.002256
| Global Round : 7 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000149
| Global Round : 7 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000080
| Global Round : 7 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000720
| Global Round : 7 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.001416
| Global Round : 7 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.026130
| Global Round : 7 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.003108
| Global Round : 7 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000074
| Global Round : 7 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.001219
| Global Round : 7 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.012637
| Global Round : 7 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.009072
| Global Round : 7 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000753
| Global Round : 7 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.002629
| Global Round : 7 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.001194
| Global Round : 7 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000306
| Global Round : 7 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.002047
| Global Round : 7 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.001643
| Global Round : 7 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000124
| Global Round : 7 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.005013
| Global Round : 7 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000831
| Global Round : 7 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000663
| Global Round : 7 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.002637
| Global Round : 7 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.001124
| Global Round : 7 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.001988
| Global Round : 7 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.003582
| Global Round : 7 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.027290
| Global Round : 7 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.006522
| Global Round : 7 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.004468
| Global Round : 7 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.065585
| Global Round : 7 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.003498
| Global Round : 7 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.001886
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0]})
Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 49.73% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 49.80 %
----Attack Model Testing----
Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

------------------------------------------
------User: 7, Epoch: 7--------
------------------------------------------
| Global Round : 7 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000418
| Global Round : 7 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.005422
| Global Round : 7 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.097828
| Global Round : 7 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.015141
| Global Round : 7 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.001501
| Global Round : 7 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000895
| Global Round : 7 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000603
| Global Round : 7 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000009
| Global Round : 7 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.030499
| Global Round : 7 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000342
| Global Round : 7 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.002131
| Global Round : 7 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.077368
| Global Round : 7 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.004615
| Global Round : 7 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.001188
| Global Round : 7 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.008767
| Global Round : 7 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000075
| Global Round : 7 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.007234
| Global Round : 7 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000011
| Global Round : 7 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000349
| Global Round : 7 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000763
| Global Round : 7 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.002536
| Global Round : 7 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000288
| Global Round : 7 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.002726
| Global Round : 7 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000149
| Global Round : 7 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000736
| Global Round : 7 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.003154
| Global Round : 7 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.015860
| Global Round : 7 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000109
| Global Round : 7 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000060
| Global Round : 7 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.011633
| Global Round : 7 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.001551
| Global Round : 7 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000706
| Global Round : 7 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.009282
| Global Round : 7 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.003097
| Global Round : 7 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.021462
| Global Round : 7 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000211
| Global Round : 7 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.033272
| Global Round : 7 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.009031
| Global Round : 7 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.001524
| Global Round : 7 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.001322
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0]})
 
Avg Training Stats after 8 global rounds:
Training Loss : nan
Train Accuracy: 9931.50% 

Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 49.66% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 50.90 %
----Attack Model Testing----
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
 27%|       | 8/30 [29:40<1:21:34, 222.48s/it]Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

Selected users for the training: [6 5 3 1 8 0 2 7 4 9]
------------------------------------------
------User: 6, Epoch: 8--------
------------------------------------------
| Global Round : 8 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000640
| Global Round : 8 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000271
| Global Round : 8 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.001460
| Global Round : 8 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000083
| Global Round : 8 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.001226
| Global Round : 8 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.008540
| Global Round : 8 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000118
| Global Round : 8 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000294
| Global Round : 8 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.002216
| Global Round : 8 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000679
| Global Round : 8 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000611
| Global Round : 8 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000318
| Global Round : 8 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.001840
| Global Round : 8 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000184
| Global Round : 8 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000033
| Global Round : 8 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000469
| Global Round : 8 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000384
| Global Round : 8 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.002782
| Global Round : 8 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000199
| Global Round : 8 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.002751
| Global Round : 8 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000090
| Global Round : 8 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.003473
| Global Round : 8 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.003306
| Global Round : 8 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.006457
| Global Round : 8 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000860
| Global Round : 8 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.019845
| Global Round : 8 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.001964
| Global Round : 8 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000478
| Global Round : 8 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.002727
| Global Round : 8 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000026
| Global Round : 8 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.003837
| Global Round : 8 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.009685
| Global Round : 8 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.011695
| Global Round : 8 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.003549
| Global Round : 8 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.005267
| Global Round : 8 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.021850
| Global Round : 8 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.036984
| Global Round : 8 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.007677
| Global Round : 8 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000095
| Global Round : 8 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.001816
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0]})
------------------------------------------
------User: 5, Epoch: 8--------
------------------------------------------
| Global Round : 8 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.009585
| Global Round : 8 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.005212
| Global Round : 8 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.001601
| Global Round : 8 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.005154
| Global Round : 8 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.011435
| Global Round : 8 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.004265
| Global Round : 8 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.006558
| Global Round : 8 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.008620
| Global Round : 8 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000126
| Global Round : 8 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000075
| Global Round : 8 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000150
| Global Round : 8 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.003260
| Global Round : 8 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000032
| Global Round : 8 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000403
| Global Round : 8 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.001738
| Global Round : 8 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000026
| Global Round : 8 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.003912
| Global Round : 8 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000512
| Global Round : 8 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.003275
| Global Round : 8 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.001102
| Global Round : 8 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.001025
| Global Round : 8 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000964
| Global Round : 8 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.013631
| Global Round : 8 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.005343
| Global Round : 8 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.024626
| Global Round : 8 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000106
| Global Round : 8 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000341
| Global Round : 8 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.001715
| Global Round : 8 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.005047
| Global Round : 8 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000132
| Global Round : 8 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.001475
| Global Round : 8 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.001030
| Global Round : 8 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000188
| Global Round : 8 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000095
| Global Round : 8 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.001425
| Global Round : 8 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000300
| Global Round : 8 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.080052
| Global Round : 8 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000385
| Global Round : 8 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.001063
| Global Round : 8 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.001986
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0]})
------------------------------------------
------User: 3, Epoch: 8--------
------------------------------------------
| Global Round : 8 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000757
| Global Round : 8 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000327
| Global Round : 8 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000078
| Global Round : 8 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000113
| Global Round : 8 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000255
| Global Round : 8 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.138141
| Global Round : 8 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.001687
| Global Round : 8 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.001758
| Global Round : 8 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000422
| Global Round : 8 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000141
| Global Round : 8 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.006031
| Global Round : 8 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.002646
| Global Round : 8 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.001408
| Global Round : 8 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000967
| Global Round : 8 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.005411
| Global Round : 8 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.010444
| Global Round : 8 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.004234
| Global Round : 8 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000044
| Global Round : 8 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000363
| Global Round : 8 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000879
| Global Round : 8 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.001594
| Global Round : 8 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.001298
| Global Round : 8 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.002371
| Global Round : 8 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.005143
| Global Round : 8 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.001175
| Global Round : 8 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.280301
| Global Round : 8 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000388
| Global Round : 8 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.001329
| Global Round : 8 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.003676
| Global Round : 8 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000269
| Global Round : 8 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.014297
| Global Round : 8 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000239
| Global Round : 8 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000279
| Global Round : 8 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000756
| Global Round : 8 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.045319
| Global Round : 8 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000857
| Global Round : 8 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.008252
| Global Round : 8 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000014
| Global Round : 8 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000682
| Global Round : 8 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.007536
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0]})
------------------------------------------
------User: 1, Epoch: 8--------
------------------------------------------
| Global Round : 8 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.105242
| Global Round : 8 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.002676
| Global Round : 8 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000736
| Global Round : 8 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.027101
| Global Round : 8 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.001081
| Global Round : 8 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000304
| Global Round : 8 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000121
| Global Round : 8 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.005133
| Global Round : 8 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.004898
| Global Round : 8 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000836
| Global Round : 8 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.003353
| Global Round : 8 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.002091
| Global Round : 8 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.009202
| Global Round : 8 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.003308
| Global Round : 8 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.056629
| Global Round : 8 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.002492
| Global Round : 8 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.011990
| Global Round : 8 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.003955
| Global Round : 8 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000756
| Global Round : 8 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.013822
| Global Round : 8 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.004188
| Global Round : 8 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.003832
| Global Round : 8 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.001139
| Global Round : 8 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000039
| Global Round : 8 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000711
| Global Round : 8 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.009467
| Global Round : 8 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.005753
| Global Round : 8 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000289
| Global Round : 8 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000129
| Global Round : 8 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000318
| Global Round : 8 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000288
| Global Round : 8 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.001203
| Global Round : 8 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.005344
| Global Round : 8 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.001608
| Global Round : 8 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000367
| Global Round : 8 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.002759
| Global Round : 8 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.021482
| Global Round : 8 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.010823
| Global Round : 8 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.007057
| Global Round : 8 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000040
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0]})
------------------------------------------
------User: 8, Epoch: 8--------
------------------------------------------
| Global Round : 8 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000368
| Global Round : 8 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.002184
| Global Round : 8 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.002272
| Global Round : 8 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.001586
| Global Round : 8 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000986
| Global Round : 8 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.050723
| Global Round : 8 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000206
| Global Round : 8 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.113706
| Global Round : 8 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.004393
| Global Round : 8 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.001909
| Global Round : 8 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000230
| Global Round : 8 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.001503
| Global Round : 8 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000233
| Global Round : 8 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000413
| Global Round : 8 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000879
| Global Round : 8 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000384
| Global Round : 8 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.011850
| Global Round : 8 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000261
| Global Round : 8 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000282
| Global Round : 8 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000280
| Global Round : 8 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.009964
| Global Round : 8 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000023
| Global Round : 8 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.001873
| Global Round : 8 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000491
| Global Round : 8 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000095
| Global Round : 8 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.230539
| Global Round : 8 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000874
| Global Round : 8 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.003144
| Global Round : 8 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000710
| Global Round : 8 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000059
| Global Round : 8 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.003983
| Global Round : 8 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.002657
| Global Round : 8 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.007065
| Global Round : 8 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000306
| Global Round : 8 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000414
| Global Round : 8 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.003287
| Global Round : 8 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.001035
| Global Round : 8 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.003062
| Global Round : 8 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.001857
| Global Round : 8 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.003671
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0]})
------------------------------------------
------User: 0, Epoch: 8--------
------------------------------------------
| Global Round : 8 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.023826
| Global Round : 8 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000420
| Global Round : 8 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.003945
| Global Round : 8 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000995
| Global Round : 8 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000546
| Global Round : 8 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.009801
| Global Round : 8 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000191
| Global Round : 8 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000040
| Global Round : 8 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.026060
| Global Round : 8 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.003718
| Global Round : 8 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000275
| Global Round : 8 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.002762
| Global Round : 8 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000856
| Global Round : 8 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.010835
| Global Round : 8 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.091429
| Global Round : 8 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.001245
| Global Round : 8 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.002583
| Global Round : 8 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000107
| Global Round : 8 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.001433
| Global Round : 8 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.024618
| Global Round : 8 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.004892
| Global Round : 8 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000663
| Global Round : 8 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.004771
| Global Round : 8 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.027881
| Global Round : 8 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000937
| Global Round : 8 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.002610
| Global Round : 8 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.045612
| Global Round : 8 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000257
| Global Round : 8 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000315
| Global Round : 8 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.001235
| Global Round : 8 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.002578
| Global Round : 8 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000014
| Global Round : 8 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.002185
| Global Round : 8 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.004590
| Global Round : 8 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.004532
| Global Round : 8 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000950
| Global Round : 8 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000147
| Global Round : 8 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000068
| Global Round : 8 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000122
| Global Round : 8 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.002071
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0]})
Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.60%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 48.60 %
----Attack Model Testing----
Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

------------------------------------------
------User: 2, Epoch: 8--------
------------------------------------------
| Global Round : 8 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000590
| Global Round : 8 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000647
| Global Round : 8 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000344
| Global Round : 8 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000626
| Global Round : 8 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.006430
| Global Round : 8 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.015150
| Global Round : 8 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.001505
| Global Round : 8 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.003178
| Global Round : 8 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000250
| Global Round : 8 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.022507
| Global Round : 8 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000703
| Global Round : 8 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.127097
| Global Round : 8 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.005300
| Global Round : 8 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000486
| Global Round : 8 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000233
| Global Round : 8 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.012321
| Global Round : 8 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000050
| Global Round : 8 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.002219
| Global Round : 8 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000848
| Global Round : 8 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000370
| Global Round : 8 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.001454
| Global Round : 8 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000428
| Global Round : 8 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.001923
| Global Round : 8 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000185
| Global Round : 8 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.001040
| Global Round : 8 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000624
| Global Round : 8 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.004310
| Global Round : 8 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.010204
| Global Round : 8 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.003458
| Global Round : 8 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000669
| Global Round : 8 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.025735
| Global Round : 8 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.005389
| Global Round : 8 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.006314
| Global Round : 8 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000234
| Global Round : 8 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000092
| Global Round : 8 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.012958
| Global Round : 8 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000482
| Global Round : 8 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.001267
| Global Round : 8 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000735
| Global Round : 8 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000957
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0]})
------------------------------------------
------User: 7, Epoch: 8--------
------------------------------------------
| Global Round : 8 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.004158
| Global Round : 8 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000420
| Global Round : 8 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000066
| Global Round : 8 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000415
| Global Round : 8 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.001132
| Global Round : 8 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000402
| Global Round : 8 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.012473
| Global Round : 8 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.001529
| Global Round : 8 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000892
| Global Round : 8 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000085
| Global Round : 8 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000427
| Global Round : 8 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.015483
| Global Round : 8 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.001479
| Global Round : 8 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000373
| Global Round : 8 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000910
| Global Round : 8 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000424
| Global Round : 8 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.001438
| Global Round : 8 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.092297
| Global Round : 8 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000642
| Global Round : 8 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.001513
| Global Round : 8 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000615
| Global Round : 8 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.022487
| Global Round : 8 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000447
| Global Round : 8 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000011
| Global Round : 8 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000515
| Global Round : 8 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000533
| Global Round : 8 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000926
| Global Round : 8 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000297
| Global Round : 8 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.013774
| Global Round : 8 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.004198
| Global Round : 8 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.003226
| Global Round : 8 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.009537
| Global Round : 8 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000126
| Global Round : 8 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000333
| Global Round : 8 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.002117
| Global Round : 8 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.006423
| Global Round : 8 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000088
| Global Round : 8 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.001923
| Global Round : 8 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.001930
| Global Round : 8 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.011291
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0]})
------------------------------------------
------User: 4, Epoch: 8--------
------------------------------------------
| Global Round : 8 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.008467
| Global Round : 8 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.184630
| Global Round : 8 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.001394
| Global Round : 8 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000256
| Global Round : 8 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.002865
| Global Round : 8 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000086
| Global Round : 8 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000027
| Global Round : 8 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000096
| Global Round : 8 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.004808
| Global Round : 8 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.011525
| Global Round : 8 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000182
| Global Round : 8 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000068
| Global Round : 8 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.003373
| Global Round : 8 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000738
| Global Round : 8 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.003132
| Global Round : 8 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000398
| Global Round : 8 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.001661
| Global Round : 8 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.008434
| Global Round : 8 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.009741
| Global Round : 8 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.004398
| Global Round : 8 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.009463
| Global Round : 8 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.001218
| Global Round : 8 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.001496
| Global Round : 8 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.023808
| Global Round : 8 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.001634
| Global Round : 8 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000318
| Global Round : 8 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000517
| Global Round : 8 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000447
| Global Round : 8 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000270
| Global Round : 8 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.008731
| Global Round : 8 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.003936
| Global Round : 8 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.008410
| Global Round : 8 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.009696
| Global Round : 8 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.001508
| Global Round : 8 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.048214
| Global Round : 8 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000336
| Global Round : 8 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000436
| Global Round : 8 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.001897
| Global Round : 8 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000164
| Global Round : 8 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.003275
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 9, Epoch: 8--------
------------------------------------------
| Global Round : 8 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.002629
| Global Round : 8 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000775
| Global Round : 8 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.046949
| Global Round : 8 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.022811
| Global Round : 8 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000747
| Global Round : 8 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000995
| Global Round : 8 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.003555
| Global Round : 8 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000025
| Global Round : 8 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000164
| Global Round : 8 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000144
| Global Round : 8 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.001811
| Global Round : 8 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000304
| Global Round : 8 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.001076
| Global Round : 8 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.007208
| Global Round : 8 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.003506
| Global Round : 8 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.005057
| Global Round : 8 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000133
| Global Round : 8 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.005408
| Global Round : 8 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000164
| Global Round : 8 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.043890
| Global Round : 8 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.010914
| Global Round : 8 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.001004
| Global Round : 8 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.019687
| Global Round : 8 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000117
| Global Round : 8 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.003557
| Global Round : 8 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.004325
| Global Round : 8 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.011000
| Global Round : 8 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000251
| Global Round : 8 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000450
| Global Round : 8 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000982
| Global Round : 8 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000118
| Global Round : 8 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.027216
| Global Round : 8 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000181
| Global Round : 8 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.029130
| Global Round : 8 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.004380
| Global Round : 8 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.005890
| Global Round : 8 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.003289
| Global Round : 8 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.001493
| Global Round : 8 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.001049
| Global Round : 8 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.124438
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0]})
 
Avg Training Stats after 9 global rounds:
Training Loss : nan
Train Accuracy: 9938.44% 

Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.50%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.50%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.50%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.50%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.50%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.50%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.50%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.50%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.50%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.50%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.50%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.50%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.50%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.50%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.50%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.50%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.50%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.50%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.50%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.50%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.50%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.50%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.50%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.50%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.50%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.50%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.50%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.50%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.50%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.50%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.50%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.50%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.50%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.50%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.50%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.50%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.50%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.50%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.50%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.50%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.50%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.50%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.50%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.50%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.50%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.50%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.50%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.50%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.50%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.50%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 50.50 %
----Attack Model Testing----
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
 30%|       | 9/30 [33:20<1:17:31, 221.50s/it]Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

Selected users for the training: [4 5 6 1 0 8 7 9 2 3]
------------------------------------------
------User: 4, Epoch: 9--------
------------------------------------------
| Global Round : 9 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.007553
| Global Round : 9 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000444
| Global Round : 9 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.001081
| Global Round : 9 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.002094
| Global Round : 9 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.001441
| Global Round : 9 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000079
| Global Round : 9 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000236
| Global Round : 9 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.003851
| Global Round : 9 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000305
| Global Round : 9 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000076
| Global Round : 9 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000383
| Global Round : 9 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.002201
| Global Round : 9 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000085
| Global Round : 9 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000508
| Global Round : 9 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000572
| Global Round : 9 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.003012
| Global Round : 9 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000025
| Global Round : 9 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.001209
| Global Round : 9 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.005675
| Global Round : 9 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.005728
| Global Round : 9 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.003518
| Global Round : 9 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.184118
| Global Round : 9 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.006535
| Global Round : 9 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000069
| Global Round : 9 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.001043
| Global Round : 9 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.003014
| Global Round : 9 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000515
| Global Round : 9 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.001929
| Global Round : 9 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.013989
| Global Round : 9 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000081
| Global Round : 9 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.012760
| Global Round : 9 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.020649
| Global Round : 9 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.027694
| Global Round : 9 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000439
| Global Round : 9 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.003984
| Global Round : 9 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000245
| Global Round : 9 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.001492
| Global Round : 9 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000380
| Global Round : 9 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000064
| Global Round : 9 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.001518
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 5, Epoch: 9--------
------------------------------------------
| Global Round : 9 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000557
| Global Round : 9 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.002656
| Global Round : 9 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.003810
| Global Round : 9 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.009225
| Global Round : 9 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000210
| Global Round : 9 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000276
| Global Round : 9 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.001002
| Global Round : 9 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000878
| Global Round : 9 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.010961
| Global Round : 9 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.011116
| Global Round : 9 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.027225
| Global Round : 9 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000752
| Global Round : 9 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.002079
| Global Round : 9 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000176
| Global Round : 9 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000059
| Global Round : 9 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.006619
| Global Round : 9 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.005875
| Global Round : 9 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000672
| Global Round : 9 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000689
| Global Round : 9 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.002312
| Global Round : 9 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000053
| Global Round : 9 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.028105
| Global Round : 9 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.001887
| Global Round : 9 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.001312
| Global Round : 9 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.001970
| Global Round : 9 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.002801
| Global Round : 9 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000819
| Global Round : 9 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000331
| Global Round : 9 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.009966
| Global Round : 9 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.016101
| Global Round : 9 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000170
| Global Round : 9 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.003776
| Global Round : 9 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.008667
| Global Round : 9 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.001983
| Global Round : 9 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.010047
| Global Round : 9 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000025
| Global Round : 9 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.001725
| Global Round : 9 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000885
| Global Round : 9 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.002052
| Global Round : 9 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.040664
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 6, Epoch: 9--------
------------------------------------------
| Global Round : 9 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.001937
| Global Round : 9 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000189
| Global Round : 9 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.004915
| Global Round : 9 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000152
| Global Round : 9 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000341
| Global Round : 9 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000870
| Global Round : 9 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.001506
| Global Round : 9 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000134
| Global Round : 9 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000337
| Global Round : 9 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.004227
| Global Round : 9 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000050
| Global Round : 9 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000144
| Global Round : 9 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.003629
| Global Round : 9 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000140
| Global Round : 9 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000256
| Global Round : 9 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000161
| Global Round : 9 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000089
| Global Round : 9 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.002240
| Global Round : 9 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000191
| Global Round : 9 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.015447
| Global Round : 9 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000286
| Global Round : 9 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.002512
| Global Round : 9 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000051
| Global Round : 9 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000216
| Global Round : 9 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.007625
| Global Round : 9 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000186
| Global Round : 9 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.001525
| Global Round : 9 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000630
| Global Round : 9 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000894
| Global Round : 9 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.006880
| Global Round : 9 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.015023
| Global Round : 9 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.006350
| Global Round : 9 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.001525
| Global Round : 9 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.072344
| Global Round : 9 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000106
| Global Round : 9 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000192
| Global Round : 9 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.030152
| Global Round : 9 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.011710
| Global Round : 9 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000259
| Global Round : 9 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000356
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 1, Epoch: 9--------
------------------------------------------
| Global Round : 9 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000092
| Global Round : 9 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.003658
| Global Round : 9 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000452
| Global Round : 9 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000685
| Global Round : 9 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.018868
| Global Round : 9 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000015
| Global Round : 9 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.002254
| Global Round : 9 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000518
| Global Round : 9 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.004209
| Global Round : 9 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000323
| Global Round : 9 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.001283
| Global Round : 9 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000019
| Global Round : 9 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.001313
| Global Round : 9 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.001103
| Global Round : 9 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000326
| Global Round : 9 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.005630
| Global Round : 9 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.038489
| Global Round : 9 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000066
| Global Round : 9 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000919
| Global Round : 9 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.002128
| Global Round : 9 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000184
| Global Round : 9 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000516
| Global Round : 9 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000102
| Global Round : 9 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.004006
| Global Round : 9 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.001252
| Global Round : 9 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.001500
| Global Round : 9 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000348
| Global Round : 9 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000308
| Global Round : 9 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000047
| Global Round : 9 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000149
| Global Round : 9 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000697
| Global Round : 9 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000193
| Global Round : 9 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000357
| Global Round : 9 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000991
| Global Round : 9 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.001061
| Global Round : 9 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000097
| Global Round : 9 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.006781
| Global Round : 9 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.026368
| Global Round : 9 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.001343
| Global Round : 9 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000426
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 0, Epoch: 9--------
------------------------------------------
| Global Round : 9 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.002394
| Global Round : 9 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.005849
| Global Round : 9 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.003764
| Global Round : 9 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.001526
| Global Round : 9 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000188
| Global Round : 9 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.019966
| Global Round : 9 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000375
| Global Round : 9 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000081
| Global Round : 9 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000593
| Global Round : 9 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.002059
| Global Round : 9 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.002655
| Global Round : 9 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.085630
| Global Round : 9 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000094
| Global Round : 9 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000601
| Global Round : 9 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.001118
| Global Round : 9 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000565
| Global Round : 9 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.016882
| Global Round : 9 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.001442
| Global Round : 9 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.002009
| Global Round : 9 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.004312
| Global Round : 9 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000828
| Global Round : 9 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000137
| Global Round : 9 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.001193
| Global Round : 9 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.003146
| Global Round : 9 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000880
| Global Round : 9 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.144353
| Global Round : 9 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.001831
| Global Round : 9 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.004157
| Global Round : 9 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.006886
| Global Round : 9 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.007726
| Global Round : 9 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.046853
| Global Round : 9 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.038123
| Global Round : 9 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.003377
| Global Round : 9 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.006132
| Global Round : 9 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000702
| Global Round : 9 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000629
| Global Round : 9 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.033088
| Global Round : 9 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000069
| Global Round : 9 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.035499
| Global Round : 9 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.001420
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0]})
Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 49.76% | Val Loss: nan | Val Acc: 52.40%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 49.90% | Val Loss: nan | Val Acc: 52.40%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 49.90% | Val Loss: nan | Val Acc: 52.40%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 49.90% | Val Loss: nan | Val Acc: 52.40%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 49.90% | Val Loss: nan | Val Acc: 52.40%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 49.90% | Val Loss: nan | Val Acc: 52.40%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 49.90% | Val Loss: nan | Val Acc: 52.40%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 49.90% | Val Loss: nan | Val Acc: 52.40%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 49.90% | Val Loss: nan | Val Acc: 52.40%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 49.90% | Val Loss: nan | Val Acc: 52.40%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 49.90% | Val Loss: nan | Val Acc: 52.40%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 49.90% | Val Loss: nan | Val Acc: 52.40%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 49.90% | Val Loss: nan | Val Acc: 52.40%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 49.90% | Val Loss: nan | Val Acc: 52.40%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 49.90% | Val Loss: nan | Val Acc: 52.40%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 49.90% | Val Loss: nan | Val Acc: 52.40%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 49.90% | Val Loss: nan | Val Acc: 52.40%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 49.90% | Val Loss: nan | Val Acc: 52.40%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 49.90% | Val Loss: nan | Val Acc: 52.40%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 49.90% | Val Loss: nan | Val Acc: 52.40%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 49.90% | Val Loss: nan | Val Acc: 52.40%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 49.90% | Val Loss: nan | Val Acc: 52.40%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 49.90% | Val Loss: nan | Val Acc: 52.40%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 49.90% | Val Loss: nan | Val Acc: 52.40%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 49.90% | Val Loss: nan | Val Acc: 52.40%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 49.90% | Val Loss: nan | Val Acc: 52.40%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 49.90% | Val Loss: nan | Val Acc: 52.40%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 49.90% | Val Loss: nan | Val Acc: 52.40%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 49.90% | Val Loss: nan | Val Acc: 52.40%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 49.90% | Val Loss: nan | Val Acc: 52.40%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 49.90% | Val Loss: nan | Val Acc: 52.40%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 49.90% | Val Loss: nan | Val Acc: 52.40%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 49.90% | Val Loss: nan | Val Acc: 52.40%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 49.90% | Val Loss: nan | Val Acc: 52.40%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 49.90% | Val Loss: nan | Val Acc: 52.40%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 49.90% | Val Loss: nan | Val Acc: 52.40%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 49.90% | Val Loss: nan | Val Acc: 52.40%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 49.90% | Val Loss: nan | Val Acc: 52.40%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 49.90% | Val Loss: nan | Val Acc: 52.40%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 49.90% | Val Loss: nan | Val Acc: 52.40%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 49.90% | Val Loss: nan | Val Acc: 52.40%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 49.90% | Val Loss: nan | Val Acc: 52.40%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 49.90% | Val Loss: nan | Val Acc: 52.40%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 49.90% | Val Loss: nan | Val Acc: 52.40%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 49.90% | Val Loss: nan | Val Acc: 52.40%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 49.90% | Val Loss: nan | Val Acc: 52.40%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 49.90% | Val Loss: nan | Val Acc: 52.40%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 49.90% | Val Loss: nan | Val Acc: 52.40%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 49.90% | Val Loss: nan | Val Acc: 52.40%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 49.90% | Val Loss: nan | Val Acc: 52.40%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 52.40 %
----Attack Model Testing----
Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

------------------------------------------
------User: 8, Epoch: 9--------
------------------------------------------
| Global Round : 9 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000791
| Global Round : 9 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.001862
| Global Round : 9 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000249
| Global Round : 9 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.009474
| Global Round : 9 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.002544
| Global Round : 9 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.002850
| Global Round : 9 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.004167
| Global Round : 9 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.004967
| Global Round : 9 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000349
| Global Round : 9 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.007579
| Global Round : 9 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000151
| Global Round : 9 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000772
| Global Round : 9 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000640
| Global Round : 9 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.139237
| Global Round : 9 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.009467
| Global Round : 9 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000730
| Global Round : 9 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000797
| Global Round : 9 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000387
| Global Round : 9 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.088150
| Global Round : 9 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000207
| Global Round : 9 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.046243
| Global Round : 9 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.001788
| Global Round : 9 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000059
| Global Round : 9 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.001504
| Global Round : 9 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000178
| Global Round : 9 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.001254
| Global Round : 9 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.396601
| Global Round : 9 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000857
| Global Round : 9 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000061
| Global Round : 9 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.001527
| Global Round : 9 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.001733
| Global Round : 9 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000413
| Global Round : 9 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.005056
| Global Round : 9 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.002454
| Global Round : 9 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.031764
| Global Round : 9 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.002346
| Global Round : 9 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.003720
| Global Round : 9 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.001209
| Global Round : 9 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000885
| Global Round : 9 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.019868
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 7, Epoch: 9--------
------------------------------------------
| Global Round : 9 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.040277
| Global Round : 9 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000354
| Global Round : 9 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.001412
| Global Round : 9 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.019774
| Global Round : 9 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000219
| Global Round : 9 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000231
| Global Round : 9 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.067787
| Global Round : 9 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000405
| Global Round : 9 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000101
| Global Round : 9 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000777
| Global Round : 9 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.004076
| Global Round : 9 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.001391
| Global Round : 9 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000420
| Global Round : 9 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000328
| Global Round : 9 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.002826
| Global Round : 9 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000109
| Global Round : 9 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.003049
| Global Round : 9 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000076
| Global Round : 9 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000101
| Global Round : 9 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.008319
| Global Round : 9 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000075
| Global Round : 9 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000143
| Global Round : 9 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000261
| Global Round : 9 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.001809
| Global Round : 9 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000085
| Global Round : 9 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.010535
| Global Round : 9 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.003217
| Global Round : 9 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.006865
| Global Round : 9 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000840
| Global Round : 9 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.002864
| Global Round : 9 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.005015
| Global Round : 9 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000811
| Global Round : 9 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.009649
| Global Round : 9 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.009828
| Global Round : 9 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000160
| Global Round : 9 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000060
| Global Round : 9 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.001021
| Global Round : 9 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000282
| Global Round : 9 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000934
| Global Round : 9 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000675
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 9, Epoch: 9--------
------------------------------------------
| Global Round : 9 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000326
| Global Round : 9 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000979
| Global Round : 9 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000226
| Global Round : 9 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000937
| Global Round : 9 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.067780
| Global Round : 9 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.068527
| Global Round : 9 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000740
| Global Round : 9 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.005354
| Global Round : 9 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.001919
| Global Round : 9 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000423
| Global Round : 9 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000226
| Global Round : 9 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000069
| Global Round : 9 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000143
| Global Round : 9 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.006514
| Global Round : 9 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000525
| Global Round : 9 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.004995
| Global Round : 9 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.004101
| Global Round : 9 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.056522
| Global Round : 9 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000157
| Global Round : 9 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.007347
| Global Round : 9 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000571
| Global Round : 9 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000136
| Global Round : 9 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000059
| Global Round : 9 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.028047
| Global Round : 9 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.004249
| Global Round : 9 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.001851
| Global Round : 9 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000154
| Global Round : 9 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000525
| Global Round : 9 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000894
| Global Round : 9 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.001263
| Global Round : 9 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.049545
| Global Round : 9 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.018299
| Global Round : 9 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000088
| Global Round : 9 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000194
| Global Round : 9 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.001073
| Global Round : 9 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000149
| Global Round : 9 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000087
| Global Round : 9 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.001127
| Global Round : 9 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.008651
| Global Round : 9 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000812
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 2, Epoch: 9--------
------------------------------------------
| Global Round : 9 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.002104
| Global Round : 9 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000486
| Global Round : 9 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.008040
| Global Round : 9 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000165
| Global Round : 9 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000083
| Global Round : 9 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.009224
| Global Round : 9 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.001639
| Global Round : 9 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000619
| Global Round : 9 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.001210
| Global Round : 9 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000112
| Global Round : 9 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000330
| Global Round : 9 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.004581
| Global Round : 9 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000017
| Global Round : 9 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.001759
| Global Round : 9 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.001633
| Global Round : 9 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.007544
| Global Round : 9 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000317
| Global Round : 9 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000256
| Global Round : 9 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.002619
| Global Round : 9 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000252
| Global Round : 9 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.003975
| Global Round : 9 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000464
| Global Round : 9 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000170
| Global Round : 9 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000056
| Global Round : 9 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.018173
| Global Round : 9 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.001043
| Global Round : 9 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.009436
| Global Round : 9 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000072
| Global Round : 9 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000045
| Global Round : 9 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.002106
| Global Round : 9 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.006508
| Global Round : 9 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.002923
| Global Round : 9 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.003682
| Global Round : 9 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000309
| Global Round : 9 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000077
| Global Round : 9 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.052827
| Global Round : 9 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.011988
| Global Round : 9 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000799
| Global Round : 9 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000163
| Global Round : 9 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000666
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 3, Epoch: 9--------
------------------------------------------
| Global Round : 9 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000368
| Global Round : 9 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000226
| Global Round : 9 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.001485
| Global Round : 9 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000422
| Global Round : 9 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.007050
| Global Round : 9 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000508
| Global Round : 9 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000130
| Global Round : 9 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.002471
| Global Round : 9 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000726
| Global Round : 9 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000443
| Global Round : 9 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000157
| Global Round : 9 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000262
| Global Round : 9 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000148
| Global Round : 9 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.022827
| Global Round : 9 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000269
| Global Round : 9 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000086
| Global Round : 9 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000146
| Global Round : 9 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000198
| Global Round : 9 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.006786
| Global Round : 9 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.010245
| Global Round : 9 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000734
| Global Round : 9 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000098
| Global Round : 9 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000737
| Global Round : 9 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.002039
| Global Round : 9 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000226
| Global Round : 9 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.020286
| Global Round : 9 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.004542
| Global Round : 9 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000455
| Global Round : 9 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.003204
| Global Round : 9 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000626
| Global Round : 9 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000783
| Global Round : 9 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.001399
| Global Round : 9 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000063
| Global Round : 9 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.001829
| Global Round : 9 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000986
| Global Round : 9 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.005069
| Global Round : 9 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000843
| Global Round : 9 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.054779
| Global Round : 9 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.001151
| Global Round : 9 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000107
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0]})
 
Avg Training Stats after 10 global rounds:
Training Loss : nan
Train Accuracy: 9944.40% 

Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 49.78% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 48.90 %
----Attack Model Testing----
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
 33%|      | 10/30 [37:01<1:13:50, 221.54s/it]Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

Selected users for the training: [6 5 2 9 7 4 3 1 0 8]
------------------------------------------
------User: 6, Epoch: 10--------
------------------------------------------
| Global Round : 10 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.008685
| Global Round : 10 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000052
| Global Round : 10 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.007889
| Global Round : 10 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000055
| Global Round : 10 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.014970
| Global Round : 10 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.003817
| Global Round : 10 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000667
| Global Round : 10 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000416
| Global Round : 10 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000063
| Global Round : 10 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000285
| Global Round : 10 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.055853
| Global Round : 10 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.007720
| Global Round : 10 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000622
| Global Round : 10 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.001315
| Global Round : 10 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000137
| Global Round : 10 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.001324
| Global Round : 10 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000035
| Global Round : 10 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000634
| Global Round : 10 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.001450
| Global Round : 10 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000045
| Global Round : 10 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000118
| Global Round : 10 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.002866
| Global Round : 10 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.002891
| Global Round : 10 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.002116
| Global Round : 10 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.003786
| Global Round : 10 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000591
| Global Round : 10 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.002271
| Global Round : 10 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.001520
| Global Round : 10 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000619
| Global Round : 10 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000089
| Global Round : 10 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.002701
| Global Round : 10 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000157
| Global Round : 10 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000727
| Global Round : 10 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.006207
| Global Round : 10 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000685
| Global Round : 10 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000243
| Global Round : 10 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.006539
| Global Round : 10 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000411
| Global Round : 10 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.004626
| Global Round : 10 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.004120
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 5, Epoch: 10--------
------------------------------------------
| Global Round : 10 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000364
| Global Round : 10 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000068
| Global Round : 10 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000070
| Global Round : 10 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.004913
| Global Round : 10 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.003892
| Global Round : 10 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000370
| Global Round : 10 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000598
| Global Round : 10 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000308
| Global Round : 10 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000101
| Global Round : 10 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000741
| Global Round : 10 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000415
| Global Round : 10 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.005301
| Global Round : 10 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.026724
| Global Round : 10 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000392
| Global Round : 10 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000251
| Global Round : 10 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000602
| Global Round : 10 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000230
| Global Round : 10 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000180
| Global Round : 10 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.004011
| Global Round : 10 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000323
| Global Round : 10 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000094
| Global Round : 10 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000006
| Global Round : 10 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.008310
| Global Round : 10 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.160255
| Global Round : 10 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000185
| Global Round : 10 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000399
| Global Round : 10 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.020011
| Global Round : 10 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.001459
| Global Round : 10 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000018
| Global Round : 10 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000286
| Global Round : 10 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000770
| Global Round : 10 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000231
| Global Round : 10 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000659
| Global Round : 10 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.001612
| Global Round : 10 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.010084
| Global Round : 10 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000675
| Global Round : 10 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.036291
| Global Round : 10 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.003211
| Global Round : 10 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000373
| Global Round : 10 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.003340
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 2, Epoch: 10--------
------------------------------------------
| Global Round : 10 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000198
| Global Round : 10 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.004228
| Global Round : 10 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000079
| Global Round : 10 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.013127
| Global Round : 10 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000500
| Global Round : 10 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000860
| Global Round : 10 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.001454
| Global Round : 10 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000935
| Global Round : 10 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000424
| Global Round : 10 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.002512
| Global Round : 10 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.004870
| Global Round : 10 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000136
| Global Round : 10 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.006926
| Global Round : 10 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.001023
| Global Round : 10 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.009259
| Global Round : 10 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.010632
| Global Round : 10 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.003487
| Global Round : 10 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000237
| Global Round : 10 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.108229
| Global Round : 10 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000202
| Global Round : 10 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.002736
| Global Round : 10 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000604
| Global Round : 10 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000143
| Global Round : 10 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000545
| Global Round : 10 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.011393
| Global Round : 10 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000054
| Global Round : 10 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.002540
| Global Round : 10 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.004606
| Global Round : 10 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.004261
| Global Round : 10 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.007757
| Global Round : 10 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000647
| Global Round : 10 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000245
| Global Round : 10 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000070
| Global Round : 10 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.017802
| Global Round : 10 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000453
| Global Round : 10 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000466
| Global Round : 10 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000539
| Global Round : 10 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.001745
| Global Round : 10 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000924
| Global Round : 10 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.001104
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 9, Epoch: 10--------
------------------------------------------
| Global Round : 10 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.003144
| Global Round : 10 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.013344
| Global Round : 10 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000058
| Global Round : 10 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000025
| Global Round : 10 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.008271
| Global Round : 10 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000248
| Global Round : 10 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.059609
| Global Round : 10 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000159
| Global Round : 10 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000201
| Global Round : 10 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.001226
| Global Round : 10 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.028929
| Global Round : 10 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000188
| Global Round : 10 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.004244
| Global Round : 10 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000042
| Global Round : 10 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.001969
| Global Round : 10 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.001521
| Global Round : 10 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.010001
| Global Round : 10 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000141
| Global Round : 10 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.002089
| Global Round : 10 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000376
| Global Round : 10 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000912
| Global Round : 10 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.001430
| Global Round : 10 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000128
| Global Round : 10 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000601
| Global Round : 10 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.006004
| Global Round : 10 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000432
| Global Round : 10 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.003730
| Global Round : 10 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.108071
| Global Round : 10 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.014605
| Global Round : 10 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000159
| Global Round : 10 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.001456
| Global Round : 10 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000222
| Global Round : 10 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000099
| Global Round : 10 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.001060
| Global Round : 10 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000080
| Global Round : 10 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000082
| Global Round : 10 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.005223
| Global Round : 10 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000075
| Global Round : 10 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000277
| Global Round : 10 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.016072
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 7, Epoch: 10--------
------------------------------------------
| Global Round : 10 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.025012
| Global Round : 10 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000947
| Global Round : 10 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000161
| Global Round : 10 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.005174
| Global Round : 10 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000266
| Global Round : 10 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000359
| Global Round : 10 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000114
| Global Round : 10 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000597
| Global Round : 10 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000063
| Global Round : 10 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000425
| Global Round : 10 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000872
| Global Round : 10 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000158
| Global Round : 10 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000082
| Global Round : 10 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000439
| Global Round : 10 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000521
| Global Round : 10 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.001230
| Global Round : 10 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000657
| Global Round : 10 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000181
| Global Round : 10 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.034943
| Global Round : 10 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.004218
| Global Round : 10 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.002037
| Global Round : 10 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000716
| Global Round : 10 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.002703
| Global Round : 10 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.006420
| Global Round : 10 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.108852
| Global Round : 10 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.045158
| Global Round : 10 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.005782
| Global Round : 10 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.002122
| Global Round : 10 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000229
| Global Round : 10 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000227
| Global Round : 10 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.011148
| Global Round : 10 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.003134
| Global Round : 10 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000266
| Global Round : 10 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000537
| Global Round : 10 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.003346
| Global Round : 10 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.001492
| Global Round : 10 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.020313
| Global Round : 10 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000619
| Global Round : 10 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.001273
| Global Round : 10 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000374
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 4, Epoch: 10--------
------------------------------------------
| Global Round : 10 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.001596
| Global Round : 10 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000057
| Global Round : 10 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000073
| Global Round : 10 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.001894
| Global Round : 10 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.001170
| Global Round : 10 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.003812
| Global Round : 10 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000520
| Global Round : 10 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.013743
| Global Round : 10 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000301
| Global Round : 10 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.005600
| Global Round : 10 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.001134
| Global Round : 10 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.007082
| Global Round : 10 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000197
| Global Round : 10 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.002979
| Global Round : 10 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000179
| Global Round : 10 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000618
| Global Round : 10 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.020184
| Global Round : 10 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000067
| Global Round : 10 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000272
| Global Round : 10 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.002315
| Global Round : 10 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.006631
| Global Round : 10 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.002758
| Global Round : 10 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000082
| Global Round : 10 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000525
| Global Round : 10 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000023
| Global Round : 10 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.046643
| Global Round : 10 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.001274
| Global Round : 10 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000523
| Global Round : 10 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.003450
| Global Round : 10 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.006269
| Global Round : 10 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000747
| Global Round : 10 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.002610
| Global Round : 10 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.004571
| Global Round : 10 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.002347
| Global Round : 10 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.015041
| Global Round : 10 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.007860
| Global Round : 10 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000717
| Global Round : 10 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.001183
| Global Round : 10 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000014
| Global Round : 10 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000020
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 3, Epoch: 10--------
------------------------------------------
| Global Round : 10 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000548
| Global Round : 10 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000202
| Global Round : 10 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000295
| Global Round : 10 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000201
| Global Round : 10 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000022
| Global Round : 10 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000284
| Global Round : 10 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.002247
| Global Round : 10 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000264
| Global Round : 10 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000041
| Global Round : 10 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000102
| Global Round : 10 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000562
| Global Round : 10 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.009161
| Global Round : 10 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000122
| Global Round : 10 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.002045
| Global Round : 10 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.008555
| Global Round : 10 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.002398
| Global Round : 10 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.001439
| Global Round : 10 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000052
| Global Round : 10 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000537
| Global Round : 10 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000749
| Global Round : 10 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.008344
| Global Round : 10 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.001300
| Global Round : 10 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.001631
| Global Round : 10 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.001466
| Global Round : 10 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000513
| Global Round : 10 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.001026
| Global Round : 10 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000324
| Global Round : 10 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000037
| Global Round : 10 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000168
| Global Round : 10 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.001176
| Global Round : 10 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000561
| Global Round : 10 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000497
| Global Round : 10 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000464
| Global Round : 10 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.005763
| Global Round : 10 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.018760
| Global Round : 10 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.001815
| Global Round : 10 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000071
| Global Round : 10 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.056824
| Global Round : 10 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000339
| Global Round : 10 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.003892
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 1, Epoch: 10--------
------------------------------------------
| Global Round : 10 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000138
| Global Round : 10 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000100
| Global Round : 10 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.002231
| Global Round : 10 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000297
| Global Round : 10 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000223
| Global Round : 10 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.001798
| Global Round : 10 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000400
| Global Round : 10 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.001233
| Global Round : 10 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000074
| Global Round : 10 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.006385
| Global Round : 10 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000600
| Global Round : 10 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000358
| Global Round : 10 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.002732
| Global Round : 10 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000525
| Global Round : 10 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.012940
| Global Round : 10 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.007444
| Global Round : 10 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000256
| Global Round : 10 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000022
| Global Round : 10 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000983
| Global Round : 10 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000076
| Global Round : 10 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000317
| Global Round : 10 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000883
| Global Round : 10 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000071
| Global Round : 10 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.007290
| Global Round : 10 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.024122
| Global Round : 10 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000502
| Global Round : 10 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000600
| Global Round : 10 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.003368
| Global Round : 10 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.002878
| Global Round : 10 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000053
| Global Round : 10 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000278
| Global Round : 10 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000466
| Global Round : 10 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.009230
| Global Round : 10 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.001931
| Global Round : 10 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000920
| Global Round : 10 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000191
| Global Round : 10 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000854
| Global Round : 10 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.002367
| Global Round : 10 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000097
| Global Round : 10 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000315
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 0, Epoch: 10--------
------------------------------------------
| Global Round : 10 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000143
| Global Round : 10 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.002101
| Global Round : 10 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.005212
| Global Round : 10 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000278
| Global Round : 10 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000059
| Global Round : 10 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000092
| Global Round : 10 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000266
| Global Round : 10 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.001467
| Global Round : 10 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.002472
| Global Round : 10 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000970
| Global Round : 10 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.034572
| Global Round : 10 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000311
| Global Round : 10 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000254
| Global Round : 10 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000830
| Global Round : 10 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.002013
| Global Round : 10 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.001011
| Global Round : 10 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.006120
| Global Round : 10 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000212
| Global Round : 10 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.003255
| Global Round : 10 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.011660
| Global Round : 10 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.009367
| Global Round : 10 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000343
| Global Round : 10 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.002709
| Global Round : 10 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000047
| Global Round : 10 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000060
| Global Round : 10 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000866
| Global Round : 10 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.001505
| Global Round : 10 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000363
| Global Round : 10 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000089
| Global Round : 10 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.003056
| Global Round : 10 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000248
| Global Round : 10 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000241
| Global Round : 10 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.001999
| Global Round : 10 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000808
| Global Round : 10 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000160
| Global Round : 10 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000084
| Global Round : 10 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.001659
| Global Round : 10 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.002198
| Global Round : 10 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000671
| Global Round : 10 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000759
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0]})
Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.60%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.60%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.60%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.60%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.60%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.60%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.60%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.60%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.60%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.60%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.60%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.60%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.60%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.60%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.60%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.60%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.60%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.60%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.60%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.60%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.60%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.60%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.60%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.60%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.60%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.60%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.60%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.60%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.60%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.60%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.60%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.60%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.60%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.60%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.60%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.60%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.60%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.60%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.60%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.60%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.60%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.60%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.60%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.60%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.60%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.60%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.60%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.60%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.60%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.60%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 51.60 %
----Attack Model Testing----
Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

------------------------------------------
------User: 8, Epoch: 10--------
------------------------------------------
| Global Round : 10 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000341
| Global Round : 10 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000648
| Global Round : 10 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.003944
| Global Round : 10 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000147
| Global Round : 10 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000921
| Global Round : 10 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000422
| Global Round : 10 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.005073
| Global Round : 10 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.001231
| Global Round : 10 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000185
| Global Round : 10 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000367
| Global Round : 10 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.031093
| Global Round : 10 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.007659
| Global Round : 10 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.001516
| Global Round : 10 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000109
| Global Round : 10 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.011099
| Global Round : 10 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000136
| Global Round : 10 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000680
| Global Round : 10 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000380
| Global Round : 10 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000159
| Global Round : 10 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.009500
| Global Round : 10 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.037514
| Global Round : 10 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.007443
| Global Round : 10 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000361
| Global Round : 10 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.001734
| Global Round : 10 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.001383
| Global Round : 10 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000108
| Global Round : 10 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.005870
| Global Round : 10 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.061307
| Global Round : 10 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000375
| Global Round : 10 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.001430
| Global Round : 10 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.012230
| Global Round : 10 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.002236
| Global Round : 10 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000259
| Global Round : 10 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.002225
| Global Round : 10 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.004008
| Global Round : 10 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.001349
| Global Round : 10 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.001517
| Global Round : 10 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000249
| Global Round : 10 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000152
| Global Round : 10 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000451
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0]})
 
Avg Training Stats after 11 global rounds:
Training Loss : nan
Train Accuracy: 9949.45% 

Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 49.71% | Val Loss: nan | Val Acc: 50.30%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.30%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.30%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.30%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.30%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.30%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.30%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.30%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.30%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.30%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.30%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.30%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.30%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.30%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.30%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.30%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.30%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.30%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.30%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.30%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.30%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.30%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.30%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.30%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.30%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.30%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.30%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.30%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.30%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.30%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.30%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.30%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.30%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.30%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.30%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.30%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.30%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.30%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.30%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.30%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.30%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.30%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.30%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.30%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.30%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.30%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.30%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.30%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.30%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 49.99% | Val Loss: nan | Val Acc: 50.30%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 50.30 %
----Attack Model Testing----
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
 37%|      | 11/30 [40:44<1:10:18, 222.03s/it]Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

Selected users for the training: [9 2 1 0 5 7 8 3 6 4]
------------------------------------------
------User: 9, Epoch: 11--------
------------------------------------------
| Global Round : 11 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000639
| Global Round : 11 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.004941
| Global Round : 11 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.007757
| Global Round : 11 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000018
| Global Round : 11 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000263
| Global Round : 11 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000090
| Global Round : 11 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000041
| Global Round : 11 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000180
| Global Round : 11 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000457
| Global Round : 11 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.005764
| Global Round : 11 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.004093
| Global Round : 11 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000142
| Global Round : 11 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.020069
| Global Round : 11 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000202
| Global Round : 11 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.002606
| Global Round : 11 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000562
| Global Round : 11 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000185
| Global Round : 11 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.004162
| Global Round : 11 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.003519
| Global Round : 11 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.001555
| Global Round : 11 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000544
| Global Round : 11 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000617
| Global Round : 11 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.036937
| Global Round : 11 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000504
| Global Round : 11 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.009605
| Global Round : 11 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.001040
| Global Round : 11 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000323
| Global Round : 11 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000453
| Global Round : 11 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000281
| Global Round : 11 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000253
| Global Round : 11 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000165
| Global Round : 11 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.017939
| Global Round : 11 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.002462
| Global Round : 11 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.013512
| Global Round : 11 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000078
| Global Round : 11 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.006527
| Global Round : 11 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000934
| Global Round : 11 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000046
| Global Round : 11 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.001080
| Global Round : 11 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000210
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 2, Epoch: 11--------
------------------------------------------
| Global Round : 11 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.001085
| Global Round : 11 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.008276
| Global Round : 11 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.008176
| Global Round : 11 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000560
| Global Round : 11 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000051
| Global Round : 11 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.003035
| Global Round : 11 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000363
| Global Round : 11 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.001349
| Global Round : 11 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000597
| Global Round : 11 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000091
| Global Round : 11 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000421
| Global Round : 11 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000129
| Global Round : 11 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.004553
| Global Round : 11 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000732
| Global Round : 11 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.001777
| Global Round : 11 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000097
| Global Round : 11 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.004361
| Global Round : 11 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.008076
| Global Round : 11 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000091
| Global Round : 11 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000100
| Global Round : 11 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.001454
| Global Round : 11 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000111
| Global Round : 11 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000983
| Global Round : 11 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000415
| Global Round : 11 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000253
| Global Round : 11 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000205
| Global Round : 11 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.001356
| Global Round : 11 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.001334
| Global Round : 11 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000290
| Global Round : 11 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000128
| Global Round : 11 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.002301
| Global Round : 11 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000319
| Global Round : 11 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000207
| Global Round : 11 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.008659
| Global Round : 11 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000839
| Global Round : 11 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000478
| Global Round : 11 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.087437
| Global Round : 11 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000120
| Global Round : 11 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000368
| Global Round : 11 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.023146
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 1, Epoch: 11--------
------------------------------------------
| Global Round : 11 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.011375
| Global Round : 11 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000084
| Global Round : 11 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000192
| Global Round : 11 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.002665
| Global Round : 11 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.001594
| Global Round : 11 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.001826
| Global Round : 11 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.020664
| Global Round : 11 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000679
| Global Round : 11 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000471
| Global Round : 11 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000571
| Global Round : 11 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.001730
| Global Round : 11 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.001124
| Global Round : 11 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.001322
| Global Round : 11 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000304
| Global Round : 11 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000023
| Global Round : 11 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000970
| Global Round : 11 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.004097
| Global Round : 11 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000426
| Global Round : 11 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000231
| Global Round : 11 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.005788
| Global Round : 11 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.001116
| Global Round : 11 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000211
| Global Round : 11 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000155
| Global Round : 11 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.002939
| Global Round : 11 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.002753
| Global Round : 11 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000053
| Global Round : 11 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000443
| Global Round : 11 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.001859
| Global Round : 11 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000731
| Global Round : 11 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000111
| Global Round : 11 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000596
| Global Round : 11 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000031
| Global Round : 11 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000195
| Global Round : 11 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000052
| Global Round : 11 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.002014
| Global Round : 11 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000280
| Global Round : 11 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000160
| Global Round : 11 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.001774
| Global Round : 11 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000417
| Global Round : 11 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000304
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 0, Epoch: 11--------
------------------------------------------
| Global Round : 11 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000157
| Global Round : 11 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.001817
| Global Round : 11 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.004455
| Global Round : 11 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000501
| Global Round : 11 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.001391
| Global Round : 11 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000415
| Global Round : 11 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.001416
| Global Round : 11 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000974
| Global Round : 11 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000387
| Global Round : 11 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000484
| Global Round : 11 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000025
| Global Round : 11 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.001764
| Global Round : 11 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.002778
| Global Round : 11 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.005200
| Global Round : 11 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.001418
| Global Round : 11 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000416
| Global Round : 11 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000067
| Global Round : 11 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000077
| Global Round : 11 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000185
| Global Round : 11 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.001467
| Global Round : 11 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.008190
| Global Round : 11 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.012395
| Global Round : 11 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.001625
| Global Round : 11 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.056993
| Global Round : 11 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.001679
| Global Round : 11 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000420
| Global Round : 11 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000228
| Global Round : 11 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000090
| Global Round : 11 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000182
| Global Round : 11 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000711
| Global Round : 11 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000784
| Global Round : 11 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.003655
| Global Round : 11 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000099
| Global Round : 11 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000022
| Global Round : 11 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.014529
| Global Round : 11 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000222
| Global Round : 11 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000727
| Global Round : 11 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000153
| Global Round : 11 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000046
| Global Round : 11 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000258
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0]})
Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.10%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.10%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.10%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.10%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.10%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.10%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.10%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.10%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.10%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.10%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.10%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.10%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.10%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.10%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.10%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.10%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.10%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.10%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.10%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.10%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.10%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.10%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.10%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.10%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.10%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.10%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.10%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.10%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.10%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.10%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.10%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.10%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.10%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.10%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.10%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.10%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.10%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.10%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.10%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.10%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.10%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.10%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.10%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.10%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.10%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.10%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.10%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.10%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.10%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.10%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 48.10 %
----Attack Model Testing----
Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

------------------------------------------
------User: 5, Epoch: 11--------
------------------------------------------
| Global Round : 11 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.012206
| Global Round : 11 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000392
| Global Round : 11 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.001166
| Global Round : 11 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000041
| Global Round : 11 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000383
| Global Round : 11 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000062
| Global Round : 11 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.004232
| Global Round : 11 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000057
| Global Round : 11 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.001258
| Global Round : 11 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.002628
| Global Round : 11 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.001393
| Global Round : 11 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000054
| Global Round : 11 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000086
| Global Round : 11 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000153
| Global Round : 11 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000056
| Global Round : 11 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000050
| Global Round : 11 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.004446
| Global Round : 11 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.003992
| Global Round : 11 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.001628
| Global Round : 11 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000066
| Global Round : 11 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000254
| Global Round : 11 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.013905
| Global Round : 11 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.001001
| Global Round : 11 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000197
| Global Round : 11 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.002197
| Global Round : 11 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.001214
| Global Round : 11 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.019711
| Global Round : 11 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.007445
| Global Round : 11 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000551
| Global Round : 11 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.012976
| Global Round : 11 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000679
| Global Round : 11 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.012241
| Global Round : 11 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000422
| Global Round : 11 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.001285
| Global Round : 11 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.006193
| Global Round : 11 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.001405
| Global Round : 11 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.002230
| Global Round : 11 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000729
| Global Round : 11 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.003471
| Global Round : 11 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000666
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 7, Epoch: 11--------
------------------------------------------
| Global Round : 11 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000124
| Global Round : 11 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000930
| Global Round : 11 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.002511
| Global Round : 11 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000066
| Global Round : 11 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000575
| Global Round : 11 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000093
| Global Round : 11 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.001459
| Global Round : 11 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000284
| Global Round : 11 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000063
| Global Round : 11 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000551
| Global Round : 11 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.016652
| Global Round : 11 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000060
| Global Round : 11 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000385
| Global Round : 11 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000932
| Global Round : 11 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000995
| Global Round : 11 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.002993
| Global Round : 11 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.004627
| Global Round : 11 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.002412
| Global Round : 11 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.001758
| Global Round : 11 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000082
| Global Round : 11 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000142
| Global Round : 11 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.017204
| Global Round : 11 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.004120
| Global Round : 11 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000964
| Global Round : 11 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000112
| Global Round : 11 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000083
| Global Round : 11 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000271
| Global Round : 11 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.001206
| Global Round : 11 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000011
| Global Round : 11 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000132
| Global Round : 11 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.060041
| Global Round : 11 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.001336
| Global Round : 11 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000370
| Global Round : 11 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000272
| Global Round : 11 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.004701
| Global Round : 11 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000037
| Global Round : 11 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.004383
| Global Round : 11 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000013
| Global Round : 11 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.019398
| Global Round : 11 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000067
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 8, Epoch: 11--------
------------------------------------------
| Global Round : 11 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000738
| Global Round : 11 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.043573
| Global Round : 11 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.001560
| Global Round : 11 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000310
| Global Round : 11 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.002576
| Global Round : 11 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.002502
| Global Round : 11 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.004016
| Global Round : 11 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000020
| Global Round : 11 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000188
| Global Round : 11 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000423
| Global Round : 11 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.010213
| Global Round : 11 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000031
| Global Round : 11 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000465
| Global Round : 11 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000165
| Global Round : 11 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000277
| Global Round : 11 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000096
| Global Round : 11 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000210
| Global Round : 11 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.001274
| Global Round : 11 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000463
| Global Round : 11 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.038358
| Global Round : 11 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000220
| Global Round : 11 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.002104
| Global Round : 11 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.001100
| Global Round : 11 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000767
| Global Round : 11 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000185
| Global Round : 11 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.001819
| Global Round : 11 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000010
| Global Round : 11 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.003790
| Global Round : 11 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000292
| Global Round : 11 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.001429
| Global Round : 11 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.002080
| Global Round : 11 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.012487
| Global Round : 11 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.029399
| Global Round : 11 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.017705
| Global Round : 11 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.001575
| Global Round : 11 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000194
| Global Round : 11 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000274
| Global Round : 11 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.002832
| Global Round : 11 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000251
| Global Round : 11 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.007329
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 3, Epoch: 11--------
------------------------------------------
| Global Round : 11 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000877
| Global Round : 11 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.002549
| Global Round : 11 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000671
| Global Round : 11 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000061
| Global Round : 11 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.024695
| Global Round : 11 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000225
| Global Round : 11 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.001324
| Global Round : 11 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000191
| Global Round : 11 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.002609
| Global Round : 11 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.002210
| Global Round : 11 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.001671
| Global Round : 11 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000199
| Global Round : 11 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.062082
| Global Round : 11 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000160
| Global Round : 11 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000439
| Global Round : 11 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000008
| Global Round : 11 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.018703
| Global Round : 11 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000423
| Global Round : 11 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.001356
| Global Round : 11 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.001600
| Global Round : 11 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.006591
| Global Round : 11 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.016555
| Global Round : 11 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000178
| Global Round : 11 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000057
| Global Round : 11 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000154
| Global Round : 11 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.004606
| Global Round : 11 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000292
| Global Round : 11 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.001505
| Global Round : 11 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000200
| Global Round : 11 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000170
| Global Round : 11 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.006381
| Global Round : 11 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.009895
| Global Round : 11 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000234
| Global Round : 11 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000091
| Global Round : 11 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000388
| Global Round : 11 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.001131
| Global Round : 11 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000821
| Global Round : 11 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.004858
| Global Round : 11 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.003936
| Global Round : 11 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.004710
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 6, Epoch: 11--------
------------------------------------------
| Global Round : 11 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.003549
| Global Round : 11 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.005511
| Global Round : 11 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.008203
| Global Round : 11 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000189
| Global Round : 11 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000469
| Global Round : 11 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000653
| Global Round : 11 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000870
| Global Round : 11 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000086
| Global Round : 11 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.009532
| Global Round : 11 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.001741
| Global Round : 11 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000683
| Global Round : 11 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000275
| Global Round : 11 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.001868
| Global Round : 11 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.085440
| Global Round : 11 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000216
| Global Round : 11 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.002923
| Global Round : 11 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.003114
| Global Round : 11 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000442
| Global Round : 11 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000464
| Global Round : 11 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000685
| Global Round : 11 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000280
| Global Round : 11 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.026715
| Global Round : 11 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.001553
| Global Round : 11 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.002244
| Global Round : 11 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.001691
| Global Round : 11 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.005560
| Global Round : 11 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.001245
| Global Round : 11 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000423
| Global Round : 11 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.056332
| Global Round : 11 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000199
| Global Round : 11 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.002009
| Global Round : 11 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.010057
| Global Round : 11 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.111778
| Global Round : 11 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.001487
| Global Round : 11 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.002445
| Global Round : 11 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000974
| Global Round : 11 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.001291
| Global Round : 11 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000392
| Global Round : 11 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.004243
| Global Round : 11 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.001834
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 4, Epoch: 11--------
------------------------------------------
| Global Round : 11 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.001300
| Global Round : 11 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000506
| Global Round : 11 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000897
| Global Round : 11 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000061
| Global Round : 11 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.007451
| Global Round : 11 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000222
| Global Round : 11 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000829
| Global Round : 11 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.044135
| Global Round : 11 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.028421
| Global Round : 11 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000480
| Global Round : 11 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000048
| Global Round : 11 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.010265
| Global Round : 11 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000037
| Global Round : 11 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000267
| Global Round : 11 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000283
| Global Round : 11 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.001628
| Global Round : 11 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.001011
| Global Round : 11 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.003531
| Global Round : 11 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000702
| Global Round : 11 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000159
| Global Round : 11 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000669
| Global Round : 11 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000744
| Global Round : 11 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.001138
| Global Round : 11 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.001039
| Global Round : 11 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.002029
| Global Round : 11 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000396
| Global Round : 11 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000427
| Global Round : 11 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000846
| Global Round : 11 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000095
| Global Round : 11 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000462
| Global Round : 11 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.006351
| Global Round : 11 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.004755
| Global Round : 11 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000583
| Global Round : 11 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.001110
| Global Round : 11 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000802
| Global Round : 11 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.002058
| Global Round : 11 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.001759
| Global Round : 11 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000568
| Global Round : 11 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.026965
| Global Round : 11 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.002139
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
 
Avg Training Stats after 12 global rounds:
Training Loss : nan
Train Accuracy: 9953.67% 

Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 50.70 %
----Attack Model Testing----
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
 40%|      | 12/30 [44:20<1:06:00, 220.05s/it]Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

Selected users for the training: [2 8 3 5 7 9 4 0 6 1]
------------------------------------------
------User: 2, Epoch: 12--------
------------------------------------------
| Global Round : 12 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000388
| Global Round : 12 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000245
| Global Round : 12 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000181
| Global Round : 12 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000794
| Global Round : 12 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.002009
| Global Round : 12 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000187
| Global Round : 12 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.003351
| Global Round : 12 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000068
| Global Round : 12 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000315
| Global Round : 12 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000083
| Global Round : 12 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000242
| Global Round : 12 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000057
| Global Round : 12 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.002435
| Global Round : 12 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.018119
| Global Round : 12 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.004275
| Global Round : 12 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.001103
| Global Round : 12 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.001206
| Global Round : 12 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.001149
| Global Round : 12 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000190
| Global Round : 12 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000027
| Global Round : 12 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.013823
| Global Round : 12 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.004323
| Global Round : 12 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000869
| Global Round : 12 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000094
| Global Round : 12 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000536
| Global Round : 12 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.045970
| Global Round : 12 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.007754
| Global Round : 12 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000117
| Global Round : 12 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.001051
| Global Round : 12 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.001390
| Global Round : 12 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.007656
| Global Round : 12 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.039540
| Global Round : 12 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000066
| Global Round : 12 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000096
| Global Round : 12 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.003920
| Global Round : 12 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.028006
| Global Round : 12 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.001370
| Global Round : 12 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.007219
| Global Round : 12 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000051
| Global Round : 12 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000671
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 8, Epoch: 12--------
------------------------------------------
| Global Round : 12 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.012072
| Global Round : 12 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.003368
| Global Round : 12 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.012507
| Global Round : 12 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000990
| Global Round : 12 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.021900
| Global Round : 12 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000588
| Global Round : 12 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.007462
| Global Round : 12 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.017295
| Global Round : 12 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000246
| Global Round : 12 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000629
| Global Round : 12 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.001524
| Global Round : 12 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.001235
| Global Round : 12 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.001050
| Global Round : 12 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000314
| Global Round : 12 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000544
| Global Round : 12 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000038
| Global Round : 12 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000044
| Global Round : 12 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000022
| Global Round : 12 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.001706
| Global Round : 12 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000103
| Global Round : 12 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000353
| Global Round : 12 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.015619
| Global Round : 12 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000938
| Global Round : 12 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000578
| Global Round : 12 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.001429
| Global Round : 12 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.008618
| Global Round : 12 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000004
| Global Round : 12 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.001358
| Global Round : 12 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000451
| Global Round : 12 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.004092
| Global Round : 12 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000179
| Global Round : 12 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000300
| Global Round : 12 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.011798
| Global Round : 12 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000900
| Global Round : 12 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000074
| Global Round : 12 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000021
| Global Round : 12 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.004932
| Global Round : 12 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000276
| Global Round : 12 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000062
| Global Round : 12 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000256
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 3, Epoch: 12--------
------------------------------------------
| Global Round : 12 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000123
| Global Round : 12 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.001899
| Global Round : 12 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.003487
| Global Round : 12 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000063
| Global Round : 12 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.005785
| Global Round : 12 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.001112
| Global Round : 12 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000220
| Global Round : 12 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.010157
| Global Round : 12 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000572
| Global Round : 12 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.001801
| Global Round : 12 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000420
| Global Round : 12 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000709
| Global Round : 12 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.006940
| Global Round : 12 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.156268
| Global Round : 12 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.038587
| Global Round : 12 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000387
| Global Round : 12 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000231
| Global Round : 12 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.001637
| Global Round : 12 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000826
| Global Round : 12 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000163
| Global Round : 12 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000093
| Global Round : 12 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.003491
| Global Round : 12 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.002107
| Global Round : 12 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000469
| Global Round : 12 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.030494
| Global Round : 12 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.003357
| Global Round : 12 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.001162
| Global Round : 12 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.001144
| Global Round : 12 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000581
| Global Round : 12 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.016088
| Global Round : 12 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.017507
| Global Round : 12 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.002642
| Global Round : 12 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.002042
| Global Round : 12 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.006460
| Global Round : 12 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.015675
| Global Round : 12 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.001812
| Global Round : 12 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.005761
| Global Round : 12 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000140
| Global Round : 12 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000775
| Global Round : 12 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.063060
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 5, Epoch: 12--------
------------------------------------------
| Global Round : 12 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.016085
| Global Round : 12 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.001252
| Global Round : 12 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000861
| Global Round : 12 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000049
| Global Round : 12 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000557
| Global Round : 12 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000026
| Global Round : 12 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.009356
| Global Round : 12 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000034
| Global Round : 12 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000076
| Global Round : 12 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000095
| Global Round : 12 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000228
| Global Round : 12 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000027
| Global Round : 12 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.001158
| Global Round : 12 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000224
| Global Round : 12 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000260
| Global Round : 12 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000008
| Global Round : 12 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000034
| Global Round : 12 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.012772
| Global Round : 12 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.004758
| Global Round : 12 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.004212
| Global Round : 12 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000076
| Global Round : 12 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000139
| Global Round : 12 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000968
| Global Round : 12 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000789
| Global Round : 12 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000822
| Global Round : 12 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.002345
| Global Round : 12 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000016
| Global Round : 12 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.003641
| Global Round : 12 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.004293
| Global Round : 12 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.005752
| Global Round : 12 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000012
| Global Round : 12 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000164
| Global Round : 12 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000502
| Global Round : 12 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.150171
| Global Round : 12 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000232
| Global Round : 12 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.002320
| Global Round : 12 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.035032
| Global Round : 12 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000127
| Global Round : 12 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.001649
| Global Round : 12 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000825
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 7, Epoch: 12--------
------------------------------------------
| Global Round : 12 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000489
| Global Round : 12 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000390
| Global Round : 12 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000200
| Global Round : 12 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000603
| Global Round : 12 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000990
| Global Round : 12 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.005647
| Global Round : 12 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000385
| Global Round : 12 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.002291
| Global Round : 12 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000153
| Global Round : 12 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.012018
| Global Round : 12 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.010448
| Global Round : 12 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000311
| Global Round : 12 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000361
| Global Round : 12 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.001944
| Global Round : 12 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.003975
| Global Round : 12 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.079642
| Global Round : 12 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.007283
| Global Round : 12 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000493
| Global Round : 12 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.003983
| Global Round : 12 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.003703
| Global Round : 12 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000049
| Global Round : 12 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.001248
| Global Round : 12 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.003590
| Global Round : 12 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000303
| Global Round : 12 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000424
| Global Round : 12 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.020545
| Global Round : 12 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.059842
| Global Round : 12 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000136
| Global Round : 12 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000198
| Global Round : 12 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.001277
| Global Round : 12 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.046638
| Global Round : 12 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000181
| Global Round : 12 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000703
| Global Round : 12 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.019101
| Global Round : 12 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.005887
| Global Round : 12 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.019686
| Global Round : 12 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000343
| Global Round : 12 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.003581
| Global Round : 12 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000558
| Global Round : 12 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.001359
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 9, Epoch: 12--------
------------------------------------------
| Global Round : 12 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000208
| Global Round : 12 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000022
| Global Round : 12 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.002274
| Global Round : 12 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.001321
| Global Round : 12 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000114
| Global Round : 12 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.008979
| Global Round : 12 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000167
| Global Round : 12 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000652
| Global Round : 12 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.001224
| Global Round : 12 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000167
| Global Round : 12 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000298
| Global Round : 12 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000079
| Global Round : 12 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000228
| Global Round : 12 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000527
| Global Round : 12 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.003247
| Global Round : 12 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000006
| Global Round : 12 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000964
| Global Round : 12 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000083
| Global Round : 12 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000210
| Global Round : 12 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000272
| Global Round : 12 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.001411
| Global Round : 12 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000308
| Global Round : 12 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000030
| Global Round : 12 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.020304
| Global Round : 12 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.001736
| Global Round : 12 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.006814
| Global Round : 12 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000401
| Global Round : 12 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000652
| Global Round : 12 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000627
| Global Round : 12 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.014177
| Global Round : 12 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.100246
| Global Round : 12 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.005602
| Global Round : 12 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000322
| Global Round : 12 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.002364
| Global Round : 12 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000596
| Global Round : 12 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.001256
| Global Round : 12 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000080
| Global Round : 12 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000412
| Global Round : 12 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000870
| Global Round : 12 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.009532
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 4, Epoch: 12--------
------------------------------------------
| Global Round : 12 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000018
| Global Round : 12 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.003232
| Global Round : 12 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.008659
| Global Round : 12 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000041
| Global Round : 12 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.002407
| Global Round : 12 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000432
| Global Round : 12 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.004015
| Global Round : 12 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000216
| Global Round : 12 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.003981
| Global Round : 12 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.003574
| Global Round : 12 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000161
| Global Round : 12 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.002979
| Global Round : 12 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000099
| Global Round : 12 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000916
| Global Round : 12 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000161
| Global Round : 12 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000093
| Global Round : 12 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000322
| Global Round : 12 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.061591
| Global Round : 12 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.001971
| Global Round : 12 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000832
| Global Round : 12 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000867
| Global Round : 12 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000119
| Global Round : 12 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.001598
| Global Round : 12 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000139
| Global Round : 12 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.003728
| Global Round : 12 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.008760
| Global Round : 12 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.001078
| Global Round : 12 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.010139
| Global Round : 12 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.014918
| Global Round : 12 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000613
| Global Round : 12 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000107
| Global Round : 12 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.001846
| Global Round : 12 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.006047
| Global Round : 12 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.001129
| Global Round : 12 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000122
| Global Round : 12 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000096
| Global Round : 12 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000924
| Global Round : 12 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000228
| Global Round : 12 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.002585
| Global Round : 12 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000096
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 0, Epoch: 12--------
------------------------------------------
| Global Round : 12 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000319
| Global Round : 12 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000724
| Global Round : 12 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000347
| Global Round : 12 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000598
| Global Round : 12 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.010890
| Global Round : 12 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000290
| Global Round : 12 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000181
| Global Round : 12 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000266
| Global Round : 12 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000178
| Global Round : 12 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.018049
| Global Round : 12 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000639
| Global Round : 12 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000797
| Global Round : 12 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.006376
| Global Round : 12 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000147
| Global Round : 12 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000159
| Global Round : 12 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.033275
| Global Round : 12 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000169
| Global Round : 12 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000030
| Global Round : 12 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000805
| Global Round : 12 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.003371
| Global Round : 12 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.001061
| Global Round : 12 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.004625
| Global Round : 12 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.001144
| Global Round : 12 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000378
| Global Round : 12 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000101
| Global Round : 12 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.001118
| Global Round : 12 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.002935
| Global Round : 12 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.002632
| Global Round : 12 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000095
| Global Round : 12 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.001780
| Global Round : 12 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000275
| Global Round : 12 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.002460
| Global Round : 12 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.004729
| Global Round : 12 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.001197
| Global Round : 12 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000742
| Global Round : 12 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.004566
| Global Round : 12 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.006959
| Global Round : 12 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.012061
| Global Round : 12 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.002939
| Global Round : 12 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000080
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 50.00% | Val Loss: nan | Val Acc: 50.00%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 50.00 %
----Attack Model Testing----
Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

------------------------------------------
------User: 6, Epoch: 12--------
------------------------------------------
| Global Round : 12 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000011
| Global Round : 12 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.001748
| Global Round : 12 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000658
| Global Round : 12 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000257
| Global Round : 12 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.016540
| Global Round : 12 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000358
| Global Round : 12 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.005130
| Global Round : 12 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000439
| Global Round : 12 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.002745
| Global Round : 12 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.001962
| Global Round : 12 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000789
| Global Round : 12 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000154
| Global Round : 12 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000588
| Global Round : 12 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.001028
| Global Round : 12 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.001701
| Global Round : 12 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.003958
| Global Round : 12 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000137
| Global Round : 12 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.003556
| Global Round : 12 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000165
| Global Round : 12 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.037715
| Global Round : 12 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.019185
| Global Round : 12 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000222
| Global Round : 12 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000262
| Global Round : 12 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.013122
| Global Round : 12 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000051
| Global Round : 12 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.003639
| Global Round : 12 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000206
| Global Round : 12 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000042
| Global Round : 12 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.001185
| Global Round : 12 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000327
| Global Round : 12 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000342
| Global Round : 12 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000182
| Global Round : 12 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000286
| Global Round : 12 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.001359
| Global Round : 12 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000330
| Global Round : 12 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000203
| Global Round : 12 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000930
| Global Round : 12 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000081
| Global Round : 12 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.001710
| Global Round : 12 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.006608
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 1, Epoch: 12--------
------------------------------------------
| Global Round : 12 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000643
| Global Round : 12 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000208
| Global Round : 12 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.001254
| Global Round : 12 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000246
| Global Round : 12 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000490
| Global Round : 12 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.001721
| Global Round : 12 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.005965
| Global Round : 12 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000365
| Global Round : 12 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000105
| Global Round : 12 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000183
| Global Round : 12 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000520
| Global Round : 12 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.005789
| Global Round : 12 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000061
| Global Round : 12 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.002081
| Global Round : 12 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.013163
| Global Round : 12 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.005559
| Global Round : 12 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000282
| Global Round : 12 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000259
| Global Round : 12 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000790
| Global Round : 12 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000031
| Global Round : 12 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000025
| Global Round : 12 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.011690
| Global Round : 12 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000581
| Global Round : 12 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.002761
| Global Round : 12 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000810
| Global Round : 12 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.032060
| Global Round : 12 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.001392
| Global Round : 12 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.001662
| Global Round : 12 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.005764
| Global Round : 12 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000488
| Global Round : 12 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.001084
| Global Round : 12 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000101
| Global Round : 12 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.001169
| Global Round : 12 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.001188
| Global Round : 12 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000018
| Global Round : 12 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000101
| Global Round : 12 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.019898
| Global Round : 12 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.003835
| Global Round : 12 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000361
| Global Round : 12 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000734
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
 
Avg Training Stats after 13 global rounds:
Training Loss : nan
Train Accuracy: 9956.77% 

Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 49.72% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 50.40 %
----Attack Model Testing----
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
 43%|     | 13/30 [48:06<1:02:51, 221.86s/it]Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

Selected users for the training: [7 6 0 8 3 2 1 5 4 9]
------------------------------------------
------User: 7, Epoch: 13--------
------------------------------------------
| Global Round : 13 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.004080
| Global Round : 13 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000205
| Global Round : 13 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000158
| Global Round : 13 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000544
| Global Round : 13 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000442
| Global Round : 13 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000106
| Global Round : 13 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.001696
| Global Round : 13 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000905
| Global Round : 13 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.001379
| Global Round : 13 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000668
| Global Round : 13 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.001014
| Global Round : 13 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000211
| Global Round : 13 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.001175
| Global Round : 13 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000043
| Global Round : 13 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000011
| Global Round : 13 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000216
| Global Round : 13 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.001442
| Global Round : 13 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000120
| Global Round : 13 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000401
| Global Round : 13 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000025
| Global Round : 13 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.001084
| Global Round : 13 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.005059
| Global Round : 13 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000185
| Global Round : 13 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000181
| Global Round : 13 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.001211
| Global Round : 13 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.001242
| Global Round : 13 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000149
| Global Round : 13 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.002405
| Global Round : 13 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000340
| Global Round : 13 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.001682
| Global Round : 13 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.001721
| Global Round : 13 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.002451
| Global Round : 13 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.001685
| Global Round : 13 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.017837
| Global Round : 13 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.001208
| Global Round : 13 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000333
| Global Round : 13 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000845
| Global Round : 13 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000399
| Global Round : 13 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.005584
| Global Round : 13 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.005721
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 6, Epoch: 13--------
------------------------------------------
| Global Round : 13 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000941
| Global Round : 13 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000828
| Global Round : 13 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000202
| Global Round : 13 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000202
| Global Round : 13 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.001613
| Global Round : 13 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000986
| Global Round : 13 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.001386
| Global Round : 13 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.034621
| Global Round : 13 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000003
| Global Round : 13 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.001543
| Global Round : 13 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000474
| Global Round : 13 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.047642
| Global Round : 13 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.008017
| Global Round : 13 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000649
| Global Round : 13 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.001585
| Global Round : 13 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000094
| Global Round : 13 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000479
| Global Round : 13 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000946
| Global Round : 13 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000168
| Global Round : 13 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000056
| Global Round : 13 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000381
| Global Round : 13 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000434
| Global Round : 13 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000416
| Global Round : 13 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000112
| Global Round : 13 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000392
| Global Round : 13 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000059
| Global Round : 13 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.002484
| Global Round : 13 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000758
| Global Round : 13 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000258
| Global Round : 13 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000058
| Global Round : 13 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.004459
| Global Round : 13 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.002968
| Global Round : 13 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000103
| Global Round : 13 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000219
| Global Round : 13 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000072
| Global Round : 13 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.001911
| Global Round : 13 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000182
| Global Round : 13 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.001193
| Global Round : 13 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.046330
| Global Round : 13 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000387
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 0, Epoch: 13--------
------------------------------------------
| Global Round : 13 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000012
| Global Round : 13 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.021031
| Global Round : 13 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000852
| Global Round : 13 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.005730
| Global Round : 13 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.014136
| Global Round : 13 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000830
| Global Round : 13 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000184
| Global Round : 13 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000219
| Global Round : 13 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000216
| Global Round : 13 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000156
| Global Round : 13 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.002770
| Global Round : 13 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000177
| Global Round : 13 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000188
| Global Round : 13 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000830
| Global Round : 13 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000090
| Global Round : 13 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000210
| Global Round : 13 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000019
| Global Round : 13 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.012426
| Global Round : 13 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.001781
| Global Round : 13 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000490
| Global Round : 13 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000587
| Global Round : 13 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.025293
| Global Round : 13 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000806
| Global Round : 13 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.009478
| Global Round : 13 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000339
| Global Round : 13 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000518
| Global Round : 13 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.028299
| Global Round : 13 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.001324
| Global Round : 13 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.002680
| Global Round : 13 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.003788
| Global Round : 13 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000342
| Global Round : 13 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000845
| Global Round : 13 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.001995
| Global Round : 13 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000105
| Global Round : 13 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.002014
| Global Round : 13 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000011
| Global Round : 13 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000489
| Global Round : 13 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.001188
| Global Round : 13 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.001685
| Global Round : 13 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.001427
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 49.75% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 50.80 %
----Attack Model Testing----
Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

------------------------------------------
------User: 8, Epoch: 13--------
------------------------------------------
| Global Round : 13 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000021
| Global Round : 13 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.001229
| Global Round : 13 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000777
| Global Round : 13 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.015975
| Global Round : 13 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.002120
| Global Round : 13 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000302
| Global Round : 13 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000014
| Global Round : 13 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000129
| Global Round : 13 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000413
| Global Round : 13 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000154
| Global Round : 13 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000402
| Global Round : 13 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000288
| Global Round : 13 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.004493
| Global Round : 13 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.006174
| Global Round : 13 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.004744
| Global Round : 13 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000126
| Global Round : 13 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.003758
| Global Round : 13 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000220
| Global Round : 13 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000057
| Global Round : 13 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000098
| Global Round : 13 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.019562
| Global Round : 13 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000171
| Global Round : 13 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.002707
| Global Round : 13 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.001096
| Global Round : 13 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.044100
| Global Round : 13 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.001047
| Global Round : 13 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000248
| Global Round : 13 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000080
| Global Round : 13 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.001461
| Global Round : 13 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.057377
| Global Round : 13 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.004601
| Global Round : 13 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000473
| Global Round : 13 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000096
| Global Round : 13 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000050
| Global Round : 13 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000286
| Global Round : 13 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000107
| Global Round : 13 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000370
| Global Round : 13 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000072
| Global Round : 13 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000302
| Global Round : 13 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.002080
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 3, Epoch: 13--------
------------------------------------------
| Global Round : 13 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.012920
| Global Round : 13 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000518
| Global Round : 13 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.001905
| Global Round : 13 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000234
| Global Round : 13 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000157
| Global Round : 13 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.002886
| Global Round : 13 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.001076
| Global Round : 13 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000009
| Global Round : 13 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000857
| Global Round : 13 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000035
| Global Round : 13 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.003817
| Global Round : 13 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000049
| Global Round : 13 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000045
| Global Round : 13 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000034
| Global Round : 13 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.004372
| Global Round : 13 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.001076
| Global Round : 13 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000922
| Global Round : 13 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000050
| Global Round : 13 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000117
| Global Round : 13 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000137
| Global Round : 13 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000082
| Global Round : 13 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000030
| Global Round : 13 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000360
| Global Round : 13 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000365
| Global Round : 13 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000423
| Global Round : 13 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.002425
| Global Round : 13 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000596
| Global Round : 13 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.001691
| Global Round : 13 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000216
| Global Round : 13 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000032
| Global Round : 13 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000482
| Global Round : 13 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000273
| Global Round : 13 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.020367
| Global Round : 13 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.005928
| Global Round : 13 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000079
| Global Round : 13 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.004651
| Global Round : 13 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000098
| Global Round : 13 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000577
| Global Round : 13 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.006928
| Global Round : 13 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000140
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 2, Epoch: 13--------
------------------------------------------
| Global Round : 13 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000741
| Global Round : 13 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000070
| Global Round : 13 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000280
| Global Round : 13 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000348
| Global Round : 13 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000676
| Global Round : 13 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000182
| Global Round : 13 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000132
| Global Round : 13 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000108
| Global Round : 13 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000247
| Global Round : 13 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.001619
| Global Round : 13 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.003563
| Global Round : 13 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000668
| Global Round : 13 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000215
| Global Round : 13 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.007366
| Global Round : 13 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000628
| Global Round : 13 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000249
| Global Round : 13 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.004691
| Global Round : 13 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.005452
| Global Round : 13 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.004095
| Global Round : 13 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000053
| Global Round : 13 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000439
| Global Round : 13 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000130
| Global Round : 13 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.001492
| Global Round : 13 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.002860
| Global Round : 13 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000604
| Global Round : 13 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.012328
| Global Round : 13 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000528
| Global Round : 13 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000027
| Global Round : 13 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.001952
| Global Round : 13 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.004207
| Global Round : 13 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000919
| Global Round : 13 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000520
| Global Round : 13 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.030404
| Global Round : 13 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.019033
| Global Round : 13 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000238
| Global Round : 13 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000089
| Global Round : 13 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000209
| Global Round : 13 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.006818
| Global Round : 13 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000224
| Global Round : 13 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000366
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 1, Epoch: 13--------
------------------------------------------
| Global Round : 13 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000817
| Global Round : 13 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000018
| Global Round : 13 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.008070
| Global Round : 13 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.001326
| Global Round : 13 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.006060
| Global Round : 13 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000143
| Global Round : 13 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.001641
| Global Round : 13 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000389
| Global Round : 13 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.001392
| Global Round : 13 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.001174
| Global Round : 13 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000965
| Global Round : 13 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.018229
| Global Round : 13 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.001990
| Global Round : 13 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000118
| Global Round : 13 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000134
| Global Round : 13 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000084
| Global Round : 13 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000199
| Global Round : 13 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000075
| Global Round : 13 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.002141
| Global Round : 13 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.001996
| Global Round : 13 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.002732
| Global Round : 13 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000215
| Global Round : 13 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000602
| Global Round : 13 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.002162
| Global Round : 13 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.002025
| Global Round : 13 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.002632
| Global Round : 13 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.001369
| Global Round : 13 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000196
| Global Round : 13 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.001058
| Global Round : 13 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.001169
| Global Round : 13 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.001183
| Global Round : 13 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000535
| Global Round : 13 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000473
| Global Round : 13 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000320
| Global Round : 13 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000086
| Global Round : 13 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.003337
| Global Round : 13 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.020899
| Global Round : 13 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.001851
| Global Round : 13 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000310
| Global Round : 13 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.006146
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 5, Epoch: 13--------
------------------------------------------
| Global Round : 13 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000643
| Global Round : 13 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.005380
| Global Round : 13 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.007611
| Global Round : 13 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000247
| Global Round : 13 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.001362
| Global Round : 13 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000293
| Global Round : 13 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.001651
| Global Round : 13 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000062
| Global Round : 13 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.001146
| Global Round : 13 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.004892
| Global Round : 13 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000432
| Global Round : 13 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000077
| Global Round : 13 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.001108
| Global Round : 13 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000035
| Global Round : 13 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.001304
| Global Round : 13 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000067
| Global Round : 13 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.002799
| Global Round : 13 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000034
| Global Round : 13 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.003970
| Global Round : 13 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000048
| Global Round : 13 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000252
| Global Round : 13 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.001145
| Global Round : 13 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000027
| Global Round : 13 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000147
| Global Round : 13 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000060
| Global Round : 13 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.065423
| Global Round : 13 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000042
| Global Round : 13 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000131
| Global Round : 13 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.002489
| Global Round : 13 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.001585
| Global Round : 13 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000097
| Global Round : 13 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.014961
| Global Round : 13 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000910
| Global Round : 13 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.001674
| Global Round : 13 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000123
| Global Round : 13 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000196
| Global Round : 13 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.024612
| Global Round : 13 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000082
| Global Round : 13 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.001936
| Global Round : 13 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000978
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 4, Epoch: 13--------
------------------------------------------
| Global Round : 13 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000174
| Global Round : 13 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000184
| Global Round : 13 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000071
| Global Round : 13 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000084
| Global Round : 13 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000320
| Global Round : 13 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000456
| Global Round : 13 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.003229
| Global Round : 13 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.001133
| Global Round : 13 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.008454
| Global Round : 13 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.001355
| Global Round : 13 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.002198
| Global Round : 13 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000226
| Global Round : 13 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.003626
| Global Round : 13 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000232
| Global Round : 13 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.004339
| Global Round : 13 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000124
| Global Round : 13 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000803
| Global Round : 13 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.002193
| Global Round : 13 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000532
| Global Round : 13 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000272
| Global Round : 13 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000010
| Global Round : 13 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000155
| Global Round : 13 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000642
| Global Round : 13 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000076
| Global Round : 13 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.007774
| Global Round : 13 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.001835
| Global Round : 13 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.005202
| Global Round : 13 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000101
| Global Round : 13 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.001369
| Global Round : 13 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000099
| Global Round : 13 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.002754
| Global Round : 13 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000153
| Global Round : 13 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.004965
| Global Round : 13 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000638
| Global Round : 13 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000060
| Global Round : 13 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000106
| Global Round : 13 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.009091
| Global Round : 13 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000290
| Global Round : 13 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.357788
| Global Round : 13 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000014
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 9, Epoch: 13--------
------------------------------------------
| Global Round : 13 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000323
| Global Round : 13 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000196
| Global Round : 13 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000189
| Global Round : 13 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000184
| Global Round : 13 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.001248
| Global Round : 13 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.003961
| Global Round : 13 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000085
| Global Round : 13 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000760
| Global Round : 13 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000213
| Global Round : 13 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.021167
| Global Round : 13 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.002197
| Global Round : 13 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000061
| Global Round : 13 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000074
| Global Round : 13 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.004646
| Global Round : 13 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000268
| Global Round : 13 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000032
| Global Round : 13 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000104
| Global Round : 13 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.002660
| Global Round : 13 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000047
| Global Round : 13 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000308
| Global Round : 13 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.001826
| Global Round : 13 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000285
| Global Round : 13 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.022684
| Global Round : 13 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000250
| Global Round : 13 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.002529
| Global Round : 13 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.002444
| Global Round : 13 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000355
| Global Round : 13 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.031800
| Global Round : 13 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.001786
| Global Round : 13 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.001526
| Global Round : 13 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000051
| Global Round : 13 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.003549
| Global Round : 13 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000138
| Global Round : 13 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000485
| Global Round : 13 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000464
| Global Round : 13 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.002484
| Global Round : 13 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000166
| Global Round : 13 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000080
| Global Round : 13 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000298
| Global Round : 13 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000162
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
 
Avg Training Stats after 14 global rounds:
Training Loss : nan
Train Accuracy: 9959.86% 

Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 49.72% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 50.70 %
----Attack Model Testing----
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
 47%|     | 14/30 [51:46<59:00, 221.29s/it]  Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

Selected users for the training: [3 2 4 6 7 5 9 0 8 1]
------------------------------------------
------User: 3, Epoch: 14--------
------------------------------------------
| Global Round : 14 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000696
| Global Round : 14 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.003198
| Global Round : 14 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.002382
| Global Round : 14 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000972
| Global Round : 14 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000048
| Global Round : 14 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.001223
| Global Round : 14 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000057
| Global Round : 14 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000077
| Global Round : 14 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000167
| Global Round : 14 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000221
| Global Round : 14 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000275
| Global Round : 14 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000145
| Global Round : 14 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.001437
| Global Round : 14 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.021100
| Global Round : 14 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000728
| Global Round : 14 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000032
| Global Round : 14 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.036400
| Global Round : 14 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000164
| Global Round : 14 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000119
| Global Round : 14 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000155
| Global Round : 14 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000123
| Global Round : 14 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.003257
| Global Round : 14 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000084
| Global Round : 14 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000363
| Global Round : 14 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000446
| Global Round : 14 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000968
| Global Round : 14 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000106
| Global Round : 14 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000357
| Global Round : 14 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000247
| Global Round : 14 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000492
| Global Round : 14 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000695
| Global Round : 14 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000008
| Global Round : 14 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000038
| Global Round : 14 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000914
| Global Round : 14 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000335
| Global Round : 14 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000366
| Global Round : 14 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.002258
| Global Round : 14 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.004472
| Global Round : 14 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.001906
| Global Round : 14 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000544
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 2, Epoch: 14--------
------------------------------------------
| Global Round : 14 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000044
| Global Round : 14 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.011246
| Global Round : 14 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.003694
| Global Round : 14 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000257
| Global Round : 14 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000243
| Global Round : 14 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.009415
| Global Round : 14 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000393
| Global Round : 14 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000098
| Global Round : 14 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000226
| Global Round : 14 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.002521
| Global Round : 14 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000578
| Global Round : 14 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.004266
| Global Round : 14 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000933
| Global Round : 14 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000107
| Global Round : 14 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.001092
| Global Round : 14 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000298
| Global Round : 14 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000226
| Global Round : 14 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.002217
| Global Round : 14 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000217
| Global Round : 14 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000764
| Global Round : 14 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000061
| Global Round : 14 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.001258
| Global Round : 14 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000730
| Global Round : 14 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.005443
| Global Round : 14 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000095
| Global Round : 14 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000696
| Global Round : 14 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000016
| Global Round : 14 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.005809
| Global Round : 14 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000024
| Global Round : 14 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.001598
| Global Round : 14 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.002741
| Global Round : 14 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.002524
| Global Round : 14 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.001925
| Global Round : 14 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000432
| Global Round : 14 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000234
| Global Round : 14 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000869
| Global Round : 14 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000164
| Global Round : 14 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.003871
| Global Round : 14 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000962
| Global Round : 14 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.002121
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 4, Epoch: 14--------
------------------------------------------
| Global Round : 14 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000084
| Global Round : 14 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.004882
| Global Round : 14 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000041
| Global Round : 14 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000136
| Global Round : 14 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000768
| Global Round : 14 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.002547
| Global Round : 14 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.001764
| Global Round : 14 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.002153
| Global Round : 14 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000502
| Global Round : 14 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000433
| Global Round : 14 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000099
| Global Round : 14 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.022835
| Global Round : 14 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000505
| Global Round : 14 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000842
| Global Round : 14 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000913
| Global Round : 14 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000254
| Global Round : 14 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000919
| Global Round : 14 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.006452
| Global Round : 14 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000215
| Global Round : 14 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000347
| Global Round : 14 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000099
| Global Round : 14 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000346
| Global Round : 14 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.005419
| Global Round : 14 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000361
| Global Round : 14 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.001728
| Global Round : 14 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000278
| Global Round : 14 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.002650
| Global Round : 14 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.046641
| Global Round : 14 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000201
| Global Round : 14 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.001058
| Global Round : 14 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.002299
| Global Round : 14 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000711
| Global Round : 14 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.010409
| Global Round : 14 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000738
| Global Round : 14 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.017726
| Global Round : 14 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000221
| Global Round : 14 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000114
| Global Round : 14 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000969
| Global Round : 14 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.003040
| Global Round : 14 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000030
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 6, Epoch: 14--------
------------------------------------------
| Global Round : 14 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.003784
| Global Round : 14 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.002669
| Global Round : 14 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000457
| Global Round : 14 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000040
| Global Round : 14 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000080
| Global Round : 14 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.003243
| Global Round : 14 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000171
| Global Round : 14 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000078
| Global Round : 14 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.001851
| Global Round : 14 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.001109
| Global Round : 14 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.001396
| Global Round : 14 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.005045
| Global Round : 14 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000566
| Global Round : 14 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000798
| Global Round : 14 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.001741
| Global Round : 14 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000064
| Global Round : 14 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.016653
| Global Round : 14 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000112
| Global Round : 14 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000550
| Global Round : 14 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000387
| Global Round : 14 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000260
| Global Round : 14 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000792
| Global Round : 14 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.003283
| Global Round : 14 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.001939
| Global Round : 14 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.003359
| Global Round : 14 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000059
| Global Round : 14 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.001592
| Global Round : 14 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.001237
| Global Round : 14 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000360
| Global Round : 14 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.001872
| Global Round : 14 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000868
| Global Round : 14 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000188
| Global Round : 14 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.001789
| Global Round : 14 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.001391
| Global Round : 14 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000105
| Global Round : 14 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.002294
| Global Round : 14 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.032431
| Global Round : 14 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.003615
| Global Round : 14 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000394
| Global Round : 14 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000029
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 7, Epoch: 14--------
------------------------------------------
| Global Round : 14 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.003835
| Global Round : 14 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000960
| Global Round : 14 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.001052
| Global Round : 14 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000611
| Global Round : 14 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.007099
| Global Round : 14 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000201
| Global Round : 14 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000672
| Global Round : 14 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000185
| Global Round : 14 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.001538
| Global Round : 14 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.001283
| Global Round : 14 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000993
| Global Round : 14 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000139
| Global Round : 14 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000524
| Global Round : 14 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000405
| Global Round : 14 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000050
| Global Round : 14 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000418
| Global Round : 14 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.004851
| Global Round : 14 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.001253
| Global Round : 14 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.001187
| Global Round : 14 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000011
| Global Round : 14 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000119
| Global Round : 14 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000052
| Global Round : 14 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.014027
| Global Round : 14 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.001920
| Global Round : 14 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000271
| Global Round : 14 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000192
| Global Round : 14 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000090
| Global Round : 14 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000208
| Global Round : 14 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000285
| Global Round : 14 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.001434
| Global Round : 14 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000748
| Global Round : 14 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000232
| Global Round : 14 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000008
| Global Round : 14 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000107
| Global Round : 14 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000045
| Global Round : 14 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000653
| Global Round : 14 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000048
| Global Round : 14 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000115
| Global Round : 14 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000409
| Global Round : 14 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.005403
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 5, Epoch: 14--------
------------------------------------------
| Global Round : 14 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000979
| Global Round : 14 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000278
| Global Round : 14 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000071
| Global Round : 14 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000126
| Global Round : 14 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000657
| Global Round : 14 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000131
| Global Round : 14 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.024246
| Global Round : 14 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000920
| Global Round : 14 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000185
| Global Round : 14 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.001740
| Global Round : 14 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.040331
| Global Round : 14 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000223
| Global Round : 14 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.004354
| Global Round : 14 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.001334
| Global Round : 14 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000584
| Global Round : 14 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.027741
| Global Round : 14 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.005843
| Global Round : 14 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.006089
| Global Round : 14 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000074
| Global Round : 14 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.003842
| Global Round : 14 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000112
| Global Round : 14 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.001099
| Global Round : 14 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000021
| Global Round : 14 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.003575
| Global Round : 14 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000027
| Global Round : 14 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000818
| Global Round : 14 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.003781
| Global Round : 14 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.002150
| Global Round : 14 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000135
| Global Round : 14 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000314
| Global Round : 14 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000415
| Global Round : 14 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000156
| Global Round : 14 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.001334
| Global Round : 14 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000407
| Global Round : 14 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000066
| Global Round : 14 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000186
| Global Round : 14 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.001446
| Global Round : 14 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000974
| Global Round : 14 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.004191
| Global Round : 14 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.025374
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 9, Epoch: 14--------
------------------------------------------
| Global Round : 14 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000628
| Global Round : 14 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000010
| Global Round : 14 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.001016
| Global Round : 14 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000133
| Global Round : 14 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000023
| Global Round : 14 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000091
| Global Round : 14 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000054
| Global Round : 14 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.021289
| Global Round : 14 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000229
| Global Round : 14 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.002082
| Global Round : 14 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000330
| Global Round : 14 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.011369
| Global Round : 14 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000308
| Global Round : 14 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000434
| Global Round : 14 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.002322
| Global Round : 14 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.011744
| Global Round : 14 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.003434
| Global Round : 14 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000123
| Global Round : 14 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.003393
| Global Round : 14 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000311
| Global Round : 14 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000053
| Global Round : 14 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000096
| Global Round : 14 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.002130
| Global Round : 14 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000778
| Global Round : 14 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000510
| Global Round : 14 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000477
| Global Round : 14 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.004900
| Global Round : 14 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.002961
| Global Round : 14 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000224
| Global Round : 14 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.001506
| Global Round : 14 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000609
| Global Round : 14 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000170
| Global Round : 14 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000084
| Global Round : 14 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000256
| Global Round : 14 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000218
| Global Round : 14 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000473
| Global Round : 14 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000744
| Global Round : 14 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000664
| Global Round : 14 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000007
| Global Round : 14 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000410
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 0, Epoch: 14--------
------------------------------------------
| Global Round : 14 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.005836
| Global Round : 14 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000529
| Global Round : 14 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000061
| Global Round : 14 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.030484
| Global Round : 14 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000414
| Global Round : 14 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000533
| Global Round : 14 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.006233
| Global Round : 14 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.003079
| Global Round : 14 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.001220
| Global Round : 14 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000006
| Global Round : 14 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000055
| Global Round : 14 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000066
| Global Round : 14 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.004049
| Global Round : 14 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.012487
| Global Round : 14 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000411
| Global Round : 14 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.001537
| Global Round : 14 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000031
| Global Round : 14 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.001004
| Global Round : 14 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.002475
| Global Round : 14 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.001009
| Global Round : 14 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.035837
| Global Round : 14 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000429
| Global Round : 14 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.003927
| Global Round : 14 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000237
| Global Round : 14 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.001493
| Global Round : 14 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000060
| Global Round : 14 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.009581
| Global Round : 14 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000028
| Global Round : 14 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000162
| Global Round : 14 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000364
| Global Round : 14 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000029
| Global Round : 14 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000240
| Global Round : 14 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000214
| Global Round : 14 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000400
| Global Round : 14 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.001034
| Global Round : 14 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000760
| Global Round : 14 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.057291
| Global Round : 14 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000975
| Global Round : 14 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.001999
| Global Round : 14 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000199
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 49.50 %
----Attack Model Testing----
Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

------------------------------------------
------User: 8, Epoch: 14--------
------------------------------------------
| Global Round : 14 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000036
| Global Round : 14 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.004315
| Global Round : 14 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000747
| Global Round : 14 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000111
| Global Round : 14 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000078
| Global Round : 14 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000595
| Global Round : 14 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000779
| Global Round : 14 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.001268
| Global Round : 14 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000482
| Global Round : 14 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000132
| Global Round : 14 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.001159
| Global Round : 14 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.011996
| Global Round : 14 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000216
| Global Round : 14 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000321
| Global Round : 14 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000095
| Global Round : 14 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000723
| Global Round : 14 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000035
| Global Round : 14 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000124
| Global Round : 14 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000058
| Global Round : 14 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000301
| Global Round : 14 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.003178
| Global Round : 14 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000127
| Global Round : 14 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.009155
| Global Round : 14 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000108
| Global Round : 14 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.006945
| Global Round : 14 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.012566
| Global Round : 14 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000306
| Global Round : 14 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000647
| Global Round : 14 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000305
| Global Round : 14 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.001086
| Global Round : 14 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000967
| Global Round : 14 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.003700
| Global Round : 14 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.002624
| Global Round : 14 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.013800
| Global Round : 14 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000170
| Global Round : 14 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000306
| Global Round : 14 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000373
| Global Round : 14 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.005458
| Global Round : 14 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000025
| Global Round : 14 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000184
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 1, Epoch: 14--------
------------------------------------------
| Global Round : 14 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.005141
| Global Round : 14 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000094
| Global Round : 14 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000581
| Global Round : 14 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.002913
| Global Round : 14 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.001635
| Global Round : 14 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000109
| Global Round : 14 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000479
| Global Round : 14 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.002523
| Global Round : 14 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000207
| Global Round : 14 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000018
| Global Round : 14 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000028
| Global Round : 14 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000068
| Global Round : 14 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000853
| Global Round : 14 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000517
| Global Round : 14 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000023
| Global Round : 14 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000239
| Global Round : 14 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000737
| Global Round : 14 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000099
| Global Round : 14 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000251
| Global Round : 14 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000167
| Global Round : 14 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.002987
| Global Round : 14 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000814
| Global Round : 14 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000367
| Global Round : 14 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.004190
| Global Round : 14 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000076
| Global Round : 14 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000137
| Global Round : 14 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000224
| Global Round : 14 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000451
| Global Round : 14 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000165
| Global Round : 14 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000258
| Global Round : 14 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.002447
| Global Round : 14 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.002240
| Global Round : 14 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.014360
| Global Round : 14 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.001996
| Global Round : 14 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000106
| Global Round : 14 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000674
| Global Round : 14 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000254
| Global Round : 14 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000790
| Global Round : 14 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000552
| Global Round : 14 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000390
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
 
Avg Training Stats after 15 global rounds:
Training Loss : nan
Train Accuracy: 9962.53% 

Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 49.72% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 50.70 %
----Attack Model Testing----
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
 50%|     | 15/30 [55:26<55:16, 221.08s/it]Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

Selected users for the training: [5 4 1 3 2 6 0 9 8 7]
------------------------------------------
------User: 5, Epoch: 15--------
------------------------------------------
| Global Round : 15 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000029
| Global Round : 15 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000258
| Global Round : 15 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.004226
| Global Round : 15 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000105
| Global Round : 15 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.002755
| Global Round : 15 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000077
| Global Round : 15 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.003222
| Global Round : 15 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.001910
| Global Round : 15 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000577
| Global Round : 15 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.004042
| Global Round : 15 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.002104
| Global Round : 15 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000163
| Global Round : 15 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.001906
| Global Round : 15 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000772
| Global Round : 15 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000444
| Global Round : 15 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000557
| Global Round : 15 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000868
| Global Round : 15 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.124779
| Global Round : 15 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000045
| Global Round : 15 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000540
| Global Round : 15 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000319
| Global Round : 15 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.002113
| Global Round : 15 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000340
| Global Round : 15 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000076
| Global Round : 15 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000576
| Global Round : 15 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000737
| Global Round : 15 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.002578
| Global Round : 15 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.002393
| Global Round : 15 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000164
| Global Round : 15 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.016434
| Global Round : 15 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.019290
| Global Round : 15 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.001201
| Global Round : 15 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000575
| Global Round : 15 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.010613
| Global Round : 15 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000292
| Global Round : 15 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.003378
| Global Round : 15 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.004041
| Global Round : 15 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000663
| Global Round : 15 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.005934
| Global Round : 15 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.002035
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 4, Epoch: 15--------
------------------------------------------
| Global Round : 15 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.004550
| Global Round : 15 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.004862
| Global Round : 15 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.002109
| Global Round : 15 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.001479
| Global Round : 15 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000530
| Global Round : 15 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000237
| Global Round : 15 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.002384
| Global Round : 15 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000047
| Global Round : 15 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.002069
| Global Round : 15 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000095
| Global Round : 15 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000386
| Global Round : 15 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000334
| Global Round : 15 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.001415
| Global Round : 15 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000218
| Global Round : 15 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.001213
| Global Round : 15 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000137
| Global Round : 15 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000613
| Global Round : 15 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000133
| Global Round : 15 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.001978
| Global Round : 15 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000058
| Global Round : 15 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000714
| Global Round : 15 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.001646
| Global Round : 15 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000024
| Global Round : 15 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000100
| Global Round : 15 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000198
| Global Round : 15 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000165
| Global Round : 15 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000543
| Global Round : 15 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000463
| Global Round : 15 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000228
| Global Round : 15 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000148
| Global Round : 15 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.003034
| Global Round : 15 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.001692
| Global Round : 15 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000197
| Global Round : 15 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000952
| Global Round : 15 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.001697
| Global Round : 15 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.005541
| Global Round : 15 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000590
| Global Round : 15 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000010
| Global Round : 15 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000791
| Global Round : 15 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000770
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 1, Epoch: 15--------
------------------------------------------
| Global Round : 15 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000980
| Global Round : 15 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000705
| Global Round : 15 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000038
| Global Round : 15 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000251
| Global Round : 15 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000326
| Global Round : 15 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000290
| Global Round : 15 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.002753
| Global Round : 15 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000115
| Global Round : 15 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.001020
| Global Round : 15 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000074
| Global Round : 15 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.005617
| Global Round : 15 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.001385
| Global Round : 15 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000933
| Global Round : 15 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000035
| Global Round : 15 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000106
| Global Round : 15 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000180
| Global Round : 15 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000704
| Global Round : 15 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.001195
| Global Round : 15 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000068
| Global Round : 15 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.001151
| Global Round : 15 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000561
| Global Round : 15 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.011751
| Global Round : 15 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.002445
| Global Round : 15 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.003323
| Global Round : 15 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000074
| Global Round : 15 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.001431
| Global Round : 15 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000169
| Global Round : 15 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000588
| Global Round : 15 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000181
| Global Round : 15 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.015965
| Global Round : 15 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.001182
| Global Round : 15 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000051
| Global Round : 15 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.001480
| Global Round : 15 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000206
| Global Round : 15 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000111
| Global Round : 15 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000206
| Global Round : 15 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000105
| Global Round : 15 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000051
| Global Round : 15 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.003633
| Global Round : 15 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000223
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 3, Epoch: 15--------
------------------------------------------
| Global Round : 15 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.001137
| Global Round : 15 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000945
| Global Round : 15 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000934
| Global Round : 15 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000304
| Global Round : 15 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.001927
| Global Round : 15 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000012
| Global Round : 15 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000074
| Global Round : 15 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000234
| Global Round : 15 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.001179
| Global Round : 15 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000332
| Global Round : 15 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000089
| Global Round : 15 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000444
| Global Round : 15 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000207
| Global Round : 15 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000036
| Global Round : 15 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.005093
| Global Round : 15 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000566
| Global Round : 15 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000926
| Global Round : 15 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000071
| Global Round : 15 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.001533
| Global Round : 15 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000012
| Global Round : 15 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000066
| Global Round : 15 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000033
| Global Round : 15 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.002151
| Global Round : 15 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000205
| Global Round : 15 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000949
| Global Round : 15 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000018
| Global Round : 15 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000236
| Global Round : 15 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000280
| Global Round : 15 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.012317
| Global Round : 15 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000504
| Global Round : 15 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.013517
| Global Round : 15 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000070
| Global Round : 15 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.014046
| Global Round : 15 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000563
| Global Round : 15 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000067
| Global Round : 15 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000265
| Global Round : 15 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000517
| Global Round : 15 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.002916
| Global Round : 15 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000483
| Global Round : 15 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000088
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 2, Epoch: 15--------
------------------------------------------
| Global Round : 15 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000242
| Global Round : 15 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000721
| Global Round : 15 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000197
| Global Round : 15 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.007215
| Global Round : 15 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000005
| Global Round : 15 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000348
| Global Round : 15 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.016326
| Global Round : 15 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000040
| Global Round : 15 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000788
| Global Round : 15 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000259
| Global Round : 15 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.001110
| Global Round : 15 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.003337
| Global Round : 15 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.004727
| Global Round : 15 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000387
| Global Round : 15 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000887
| Global Round : 15 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.001099
| Global Round : 15 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000756
| Global Round : 15 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000936
| Global Round : 15 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000039
| Global Round : 15 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000037
| Global Round : 15 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000089
| Global Round : 15 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000972
| Global Round : 15 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000460
| Global Round : 15 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000012
| Global Round : 15 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000472
| Global Round : 15 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.003639
| Global Round : 15 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000646
| Global Round : 15 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000246
| Global Round : 15 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000675
| Global Round : 15 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.001261
| Global Round : 15 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000678
| Global Round : 15 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.002824
| Global Round : 15 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.020302
| Global Round : 15 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000066
| Global Round : 15 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.002014
| Global Round : 15 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000147
| Global Round : 15 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000807
| Global Round : 15 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000014
| Global Round : 15 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.003733
| Global Round : 15 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.010447
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 6, Epoch: 15--------
------------------------------------------
| Global Round : 15 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000328
| Global Round : 15 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.002638
| Global Round : 15 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.002390
| Global Round : 15 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000658
| Global Round : 15 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.002385
| Global Round : 15 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000147
| Global Round : 15 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000129
| Global Round : 15 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000701
| Global Round : 15 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000382
| Global Round : 15 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.023542
| Global Round : 15 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000138
| Global Round : 15 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.008861
| Global Round : 15 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000033
| Global Round : 15 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.021466
| Global Round : 15 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000439
| Global Round : 15 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000447
| Global Round : 15 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000572
| Global Round : 15 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.005978
| Global Round : 15 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000619
| Global Round : 15 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000252
| Global Round : 15 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000916
| Global Round : 15 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000085
| Global Round : 15 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000396
| Global Round : 15 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000669
| Global Round : 15 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000554
| Global Round : 15 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000058
| Global Round : 15 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.002074
| Global Round : 15 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000364
| Global Round : 15 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.002663
| Global Round : 15 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.005255
| Global Round : 15 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000745
| Global Round : 15 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000535
| Global Round : 15 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.002782
| Global Round : 15 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.009612
| Global Round : 15 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000695
| Global Round : 15 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.001477
| Global Round : 15 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000241
| Global Round : 15 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000237
| Global Round : 15 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.001685
| Global Round : 15 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000928
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 0, Epoch: 15--------
------------------------------------------
| Global Round : 15 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000132
| Global Round : 15 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.002593
| Global Round : 15 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000995
| Global Round : 15 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000081
| Global Round : 15 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000513
| Global Round : 15 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.008039
| Global Round : 15 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000357
| Global Round : 15 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000087
| Global Round : 15 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.001463
| Global Round : 15 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000559
| Global Round : 15 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.001477
| Global Round : 15 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000041
| Global Round : 15 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.001913
| Global Round : 15 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000062
| Global Round : 15 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000877
| Global Round : 15 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000031
| Global Round : 15 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.001850
| Global Round : 15 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000512
| Global Round : 15 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000193
| Global Round : 15 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000125
| Global Round : 15 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.001977
| Global Round : 15 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000054
| Global Round : 15 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.029144
| Global Round : 15 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.015503
| Global Round : 15 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000940
| Global Round : 15 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.009466
| Global Round : 15 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000803
| Global Round : 15 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000029
| Global Round : 15 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000943
| Global Round : 15 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.002312
| Global Round : 15 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.001362
| Global Round : 15 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.005848
| Global Round : 15 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000028
| Global Round : 15 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000672
| Global Round : 15 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.004296
| Global Round : 15 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000193
| Global Round : 15 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000286
| Global Round : 15 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000103
| Global Round : 15 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.001337
| Global Round : 15 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000289
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.60%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.60%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.60%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.60%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.60%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.60%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.60%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.60%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.60%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.60%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.60%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.60%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.60%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.60%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.60%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.60%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.60%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.60%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.60%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.60%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.60%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.60%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.60%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.60%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.60%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.60%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.60%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.60%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.60%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.60%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.60%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.60%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.60%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.60%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.60%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.60%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.60%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.60%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.60%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.60%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.60%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.60%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.60%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.60%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.60%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.60%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.60%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.60%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.60%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.60%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 49.60 %
----Attack Model Testing----
Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

------------------------------------------
------User: 9, Epoch: 15--------
------------------------------------------
| Global Round : 15 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000149
| Global Round : 15 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.005137
| Global Round : 15 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000094
| Global Round : 15 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.010255
| Global Round : 15 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.001949
| Global Round : 15 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000007
| Global Round : 15 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000221
| Global Round : 15 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000135
| Global Round : 15 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000304
| Global Round : 15 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.010612
| Global Round : 15 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.020577
| Global Round : 15 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000134
| Global Round : 15 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.011398
| Global Round : 15 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000187
| Global Round : 15 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.001713
| Global Round : 15 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000428
| Global Round : 15 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000096
| Global Round : 15 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000017
| Global Round : 15 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000085
| Global Round : 15 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000346
| Global Round : 15 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.001570
| Global Round : 15 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.001062
| Global Round : 15 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000064
| Global Round : 15 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000019
| Global Round : 15 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000269
| Global Round : 15 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.016327
| Global Round : 15 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.002212
| Global Round : 15 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.001227
| Global Round : 15 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.004185
| Global Round : 15 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000567
| Global Round : 15 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.002177
| Global Round : 15 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.002264
| Global Round : 15 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000221
| Global Round : 15 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000254
| Global Round : 15 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000843
| Global Round : 15 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000836
| Global Round : 15 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000009
| Global Round : 15 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000051
| Global Round : 15 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.004815
| Global Round : 15 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.019567
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 8, Epoch: 15--------
------------------------------------------
| Global Round : 15 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000163
| Global Round : 15 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000038
| Global Round : 15 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000959
| Global Round : 15 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000329
| Global Round : 15 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.019711
| Global Round : 15 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000047
| Global Round : 15 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.002350
| Global Round : 15 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000095
| Global Round : 15 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.017347
| Global Round : 15 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.006255
| Global Round : 15 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.001338
| Global Round : 15 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000080
| Global Round : 15 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000951
| Global Round : 15 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.001222
| Global Round : 15 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000514
| Global Round : 15 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.001152
| Global Round : 15 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000086
| Global Round : 15 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000711
| Global Round : 15 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000224
| Global Round : 15 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000128
| Global Round : 15 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.080227
| Global Round : 15 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.053428
| Global Round : 15 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.001393
| Global Round : 15 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.018696
| Global Round : 15 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.003240
| Global Round : 15 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000163
| Global Round : 15 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.015986
| Global Round : 15 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.009861
| Global Round : 15 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.003031
| Global Round : 15 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000030
| Global Round : 15 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.004481
| Global Round : 15 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000372
| Global Round : 15 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000429
| Global Round : 15 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.001882
| Global Round : 15 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.008420
| Global Round : 15 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000019
| Global Round : 15 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000465
| Global Round : 15 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000864
| Global Round : 15 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.020793
| Global Round : 15 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.015941
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 7, Epoch: 15--------
------------------------------------------
| Global Round : 15 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000305
| Global Round : 15 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000024
| Global Round : 15 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000401
| Global Round : 15 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000472
| Global Round : 15 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000021
| Global Round : 15 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000360
| Global Round : 15 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.001559
| Global Round : 15 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.002291
| Global Round : 15 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000611
| Global Round : 15 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000518
| Global Round : 15 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000250
| Global Round : 15 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.014177
| Global Round : 15 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000844
| Global Round : 15 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000589
| Global Round : 15 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000014
| Global Round : 15 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000049
| Global Round : 15 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000605
| Global Round : 15 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000346
| Global Round : 15 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.001131
| Global Round : 15 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.002396
| Global Round : 15 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.002322
| Global Round : 15 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000096
| Global Round : 15 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.002769
| Global Round : 15 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000028
| Global Round : 15 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.003001
| Global Round : 15 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000019
| Global Round : 15 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.004756
| Global Round : 15 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.001964
| Global Round : 15 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000212
| Global Round : 15 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000743
| Global Round : 15 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.016661
| Global Round : 15 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000616
| Global Round : 15 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000071
| Global Round : 15 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000116
| Global Round : 15 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.012811
| Global Round : 15 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000029
| Global Round : 15 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.007502
| Global Round : 15 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.002378
| Global Round : 15 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000051
| Global Round : 15 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000390
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
 
Avg Training Stats after 16 global rounds:
Training Loss : nan
Train Accuracy: 9964.88% 

Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 49.68% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 49.98% | Val Loss: nan | Val Acc: 50.40%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 50.40 %
----Attack Model Testing----
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
 53%|    | 16/30 [59:03<51:17, 219.82s/it]Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

Selected users for the training: [4 2 6 9 5 0 7 1 3 8]
------------------------------------------
------User: 4, Epoch: 16--------
------------------------------------------
| Global Round : 16 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000931
| Global Round : 16 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000008
| Global Round : 16 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.004851
| Global Round : 16 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.001214
| Global Round : 16 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000514
| Global Round : 16 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000158
| Global Round : 16 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.001428
| Global Round : 16 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000486
| Global Round : 16 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000616
| Global Round : 16 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000134
| Global Round : 16 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000115
| Global Round : 16 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.021935
| Global Round : 16 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000067
| Global Round : 16 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000135
| Global Round : 16 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000175
| Global Round : 16 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000715
| Global Round : 16 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000278
| Global Round : 16 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.011349
| Global Round : 16 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000236
| Global Round : 16 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000198
| Global Round : 16 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.002389
| Global Round : 16 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000254
| Global Round : 16 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000930
| Global Round : 16 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.003925
| Global Round : 16 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000129
| Global Round : 16 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000065
| Global Round : 16 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000628
| Global Round : 16 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.004484
| Global Round : 16 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000185
| Global Round : 16 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000755
| Global Round : 16 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000190
| Global Round : 16 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000494
| Global Round : 16 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.004642
| Global Round : 16 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.001529
| Global Round : 16 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000248
| Global Round : 16 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000507
| Global Round : 16 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000117
| Global Round : 16 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000218
| Global Round : 16 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000957
| Global Round : 16 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000374
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 2, Epoch: 16--------
------------------------------------------
| Global Round : 16 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.007038
| Global Round : 16 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000922
| Global Round : 16 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.001367
| Global Round : 16 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000637
| Global Round : 16 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000112
| Global Round : 16 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.002594
| Global Round : 16 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.001316
| Global Round : 16 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000418
| Global Round : 16 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000036
| Global Round : 16 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.004778
| Global Round : 16 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000026
| Global Round : 16 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000638
| Global Round : 16 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000625
| Global Round : 16 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000697
| Global Round : 16 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000007
| Global Round : 16 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000089
| Global Round : 16 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000403
| Global Round : 16 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.002280
| Global Round : 16 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.002068
| Global Round : 16 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.001229
| Global Round : 16 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.049670
| Global Round : 16 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000125
| Global Round : 16 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000342
| Global Round : 16 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000835
| Global Round : 16 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.001219
| Global Round : 16 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000321
| Global Round : 16 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000129
| Global Round : 16 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.005990
| Global Round : 16 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.001170
| Global Round : 16 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.013327
| Global Round : 16 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000758
| Global Round : 16 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000163
| Global Round : 16 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000178
| Global Round : 16 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000121
| Global Round : 16 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.001688
| Global Round : 16 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000639
| Global Round : 16 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000466
| Global Round : 16 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000180
| Global Round : 16 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000043
| Global Round : 16 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000315
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 6, Epoch: 16--------
------------------------------------------
| Global Round : 16 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000070
| Global Round : 16 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000360
| Global Round : 16 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000437
| Global Round : 16 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.001213
| Global Round : 16 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000442
| Global Round : 16 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000223
| Global Round : 16 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000150
| Global Round : 16 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.013853
| Global Round : 16 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000058
| Global Round : 16 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000619
| Global Round : 16 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000128
| Global Round : 16 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.001090
| Global Round : 16 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000209
| Global Round : 16 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000154
| Global Round : 16 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000182
| Global Round : 16 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.001862
| Global Round : 16 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000300
| Global Round : 16 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000243
| Global Round : 16 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000142
| Global Round : 16 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000278
| Global Round : 16 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.010553
| Global Round : 16 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.005151
| Global Round : 16 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000557
| Global Round : 16 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000098
| Global Round : 16 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000475
| Global Round : 16 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000318
| Global Round : 16 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000324
| Global Round : 16 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000326
| Global Round : 16 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000512
| Global Round : 16 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.001382
| Global Round : 16 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000094
| Global Round : 16 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000645
| Global Round : 16 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.001832
| Global Round : 16 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.001583
| Global Round : 16 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.001185
| Global Round : 16 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000209
| Global Round : 16 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000222
| Global Round : 16 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000012
| Global Round : 16 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.001242
| Global Round : 16 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000835
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 9, Epoch: 16--------
------------------------------------------
| Global Round : 16 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000039
| Global Round : 16 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000621
| Global Round : 16 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.003473
| Global Round : 16 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000409
| Global Round : 16 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000058
| Global Round : 16 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000329
| Global Round : 16 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000024
| Global Round : 16 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.259579
| Global Round : 16 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.001263
| Global Round : 16 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000448
| Global Round : 16 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000128
| Global Round : 16 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000040
| Global Round : 16 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.009090
| Global Round : 16 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.039211
| Global Round : 16 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000131
| Global Round : 16 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000269
| Global Round : 16 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000060
| Global Round : 16 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000149
| Global Round : 16 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000169
| Global Round : 16 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000072
| Global Round : 16 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.018472
| Global Round : 16 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.003228
| Global Round : 16 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000943
| Global Round : 16 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000057
| Global Round : 16 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.016238
| Global Round : 16 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.001016
| Global Round : 16 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.008918
| Global Round : 16 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.002794
| Global Round : 16 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000063
| Global Round : 16 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000406
| Global Round : 16 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000481
| Global Round : 16 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000832
| Global Round : 16 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000093
| Global Round : 16 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000146
| Global Round : 16 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000247
| Global Round : 16 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000024
| Global Round : 16 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000262
| Global Round : 16 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000228
| Global Round : 16 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.002190
| Global Round : 16 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.007524
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 5, Epoch: 16--------
------------------------------------------
| Global Round : 16 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000191
| Global Round : 16 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000132
| Global Round : 16 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000101
| Global Round : 16 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.003849
| Global Round : 16 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000371
| Global Round : 16 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.022855
| Global Round : 16 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000429
| Global Round : 16 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000022
| Global Round : 16 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000178
| Global Round : 16 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.010114
| Global Round : 16 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.003518
| Global Round : 16 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000150
| Global Round : 16 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.001121
| Global Round : 16 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.002406
| Global Round : 16 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000035
| Global Round : 16 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000188
| Global Round : 16 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.006877
| Global Round : 16 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000040
| Global Round : 16 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000046
| Global Round : 16 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000967
| Global Round : 16 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000159
| Global Round : 16 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000917
| Global Round : 16 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000031
| Global Round : 16 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.006390
| Global Round : 16 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.002109
| Global Round : 16 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.001081
| Global Round : 16 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.002811
| Global Round : 16 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000067
| Global Round : 16 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000771
| Global Round : 16 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000188
| Global Round : 16 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000459
| Global Round : 16 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.001625
| Global Round : 16 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000932
| Global Round : 16 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000879
| Global Round : 16 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.003363
| Global Round : 16 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000592
| Global Round : 16 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.006335
| Global Round : 16 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000359
| Global Round : 16 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.003448
| Global Round : 16 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.001103
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 0, Epoch: 16--------
------------------------------------------
| Global Round : 16 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000108
| Global Round : 16 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.008226
| Global Round : 16 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.009354
| Global Round : 16 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000524
| Global Round : 16 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000820
| Global Round : 16 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000767
| Global Round : 16 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000154
| Global Round : 16 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000034
| Global Round : 16 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000030
| Global Round : 16 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000242
| Global Round : 16 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000226
| Global Round : 16 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000065
| Global Round : 16 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.002421
| Global Round : 16 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.014587
| Global Round : 16 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000238
| Global Round : 16 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.013538
| Global Round : 16 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000042
| Global Round : 16 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000850
| Global Round : 16 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000030
| Global Round : 16 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.002966
| Global Round : 16 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.001876
| Global Round : 16 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000730
| Global Round : 16 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000521
| Global Round : 16 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000172
| Global Round : 16 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000128
| Global Round : 16 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.003041
| Global Round : 16 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000398
| Global Round : 16 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000560
| Global Round : 16 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.002308
| Global Round : 16 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.003790
| Global Round : 16 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000301
| Global Round : 16 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000067
| Global Round : 16 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.001230
| Global Round : 16 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000434
| Global Round : 16 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000225
| Global Round : 16 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000152
| Global Round : 16 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000222
| Global Round : 16 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.001270
| Global Round : 16 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.001293
| Global Round : 16 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.003546
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.50%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 49.50 %
----Attack Model Testing----
Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

------------------------------------------
------User: 7, Epoch: 16--------
------------------------------------------
| Global Round : 16 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000539
| Global Round : 16 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.002404
| Global Round : 16 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000027
| Global Round : 16 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000757
| Global Round : 16 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.003896
| Global Round : 16 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.003386
| Global Round : 16 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.005839
| Global Round : 16 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000104
| Global Round : 16 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.002160
| Global Round : 16 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000028
| Global Round : 16 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000105
| Global Round : 16 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000033
| Global Round : 16 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000073
| Global Round : 16 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.008389
| Global Round : 16 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000520
| Global Round : 16 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000054
| Global Round : 16 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.003209
| Global Round : 16 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000207
| Global Round : 16 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000933
| Global Round : 16 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000638
| Global Round : 16 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000796
| Global Round : 16 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.002728
| Global Round : 16 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.002239
| Global Round : 16 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.007401
| Global Round : 16 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.002382
| Global Round : 16 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000527
| Global Round : 16 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.003253
| Global Round : 16 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.002139
| Global Round : 16 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000163
| Global Round : 16 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.008828
| Global Round : 16 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.004050
| Global Round : 16 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.002730
| Global Round : 16 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.002160
| Global Round : 16 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000153
| Global Round : 16 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000018
| Global Round : 16 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000617
| Global Round : 16 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000651
| Global Round : 16 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000316
| Global Round : 16 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000006
| Global Round : 16 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.001213
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 1, Epoch: 16--------
------------------------------------------
| Global Round : 16 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000261
| Global Round : 16 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000398
| Global Round : 16 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.001108
| Global Round : 16 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000024
| Global Round : 16 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.001964
| Global Round : 16 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.002525
| Global Round : 16 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000108
| Global Round : 16 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000144
| Global Round : 16 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000105
| Global Round : 16 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000019
| Global Round : 16 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.001988
| Global Round : 16 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000087
| Global Round : 16 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.026110
| Global Round : 16 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000011
| Global Round : 16 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.002315
| Global Round : 16 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.012495
| Global Round : 16 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.001258
| Global Round : 16 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000374
| Global Round : 16 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.001458
| Global Round : 16 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000045
| Global Round : 16 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000173
| Global Round : 16 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.001334
| Global Round : 16 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000012
| Global Round : 16 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000562
| Global Round : 16 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000025
| Global Round : 16 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000390
| Global Round : 16 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000406
| Global Round : 16 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.001163
| Global Round : 16 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.078220
| Global Round : 16 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.001431
| Global Round : 16 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.008940
| Global Round : 16 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.021557
| Global Round : 16 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.005580
| Global Round : 16 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.002270
| Global Round : 16 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000342
| Global Round : 16 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000283
| Global Round : 16 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.002384
| Global Round : 16 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.002829
| Global Round : 16 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000371
| Global Round : 16 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000215
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 3, Epoch: 16--------
------------------------------------------
| Global Round : 16 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000408
| Global Round : 16 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000067
| Global Round : 16 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000025
| Global Round : 16 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.003865
| Global Round : 16 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.002475
| Global Round : 16 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000144
| Global Round : 16 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000042
| Global Round : 16 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.007871
| Global Round : 16 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.001134
| Global Round : 16 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000701
| Global Round : 16 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.005066
| Global Round : 16 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000196
| Global Round : 16 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.001249
| Global Round : 16 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.002811
| Global Round : 16 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000249
| Global Round : 16 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000062
| Global Round : 16 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.003995
| Global Round : 16 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.001524
| Global Round : 16 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.001664
| Global Round : 16 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.001104
| Global Round : 16 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000418
| Global Round : 16 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.011849
| Global Round : 16 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000656
| Global Round : 16 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.003371
| Global Round : 16 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.036493
| Global Round : 16 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000557
| Global Round : 16 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.003024
| Global Round : 16 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000020
| Global Round : 16 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.001290
| Global Round : 16 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.002314
| Global Round : 16 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.012595
| Global Round : 16 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000196
| Global Round : 16 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000022
| Global Round : 16 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000095
| Global Round : 16 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000072
| Global Round : 16 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000771
| Global Round : 16 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000490
| Global Round : 16 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.002526
| Global Round : 16 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000348
| Global Round : 16 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000158
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 8, Epoch: 16--------
------------------------------------------
| Global Round : 16 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000148
| Global Round : 16 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000234
| Global Round : 16 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000136
| Global Round : 16 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000180
| Global Round : 16 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000052
| Global Round : 16 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000068
| Global Round : 16 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000068
| Global Round : 16 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000023
| Global Round : 16 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000077
| Global Round : 16 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000206
| Global Round : 16 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000280
| Global Round : 16 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000605
| Global Round : 16 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.002676
| Global Round : 16 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.001810
| Global Round : 16 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000242
| Global Round : 16 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.002176
| Global Round : 16 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000047
| Global Round : 16 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000951
| Global Round : 16 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000162
| Global Round : 16 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.014934
| Global Round : 16 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.001089
| Global Round : 16 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.003054
| Global Round : 16 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000226
| Global Round : 16 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000184
| Global Round : 16 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000006
| Global Round : 16 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.002067
| Global Round : 16 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000578
| Global Round : 16 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000118
| Global Round : 16 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000635
| Global Round : 16 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000704
| Global Round : 16 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.002560
| Global Round : 16 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.003044
| Global Round : 16 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000031
| Global Round : 16 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000064
| Global Round : 16 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000311
| Global Round : 16 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000436
| Global Round : 16 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000043
| Global Round : 16 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.011537
| Global Round : 16 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000764
| Global Round : 16 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.002412
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
 
Avg Training Stats after 17 global rounds:
Training Loss : nan
Train Accuracy: 9966.94% 

Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 49.66% | Val Loss: nan | Val Acc: 51.50%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.50%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.50%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.50%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.50%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.50%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.50%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.50%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.50%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.50%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.50%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.50%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.50%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.50%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.50%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.50%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.50%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.50%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.50%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.50%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.50%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.50%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.50%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.50%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.50%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.50%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.50%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.50%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.50%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.50%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.50%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.50%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.50%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.50%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.50%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.50%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.50%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.50%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.50%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.50%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.50%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.50%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.50%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.50%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.50%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.50%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.50%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.50%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.50%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.50%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 51.50 %
----Attack Model Testing----
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
 57%|    | 17/30 [1:02:50<48:03, 221.78s/it]Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

Selected users for the training: [7 4 3 6 5 0 9 8 2 1]
------------------------------------------
------User: 7, Epoch: 17--------
------------------------------------------
| Global Round : 17 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000010
| Global Round : 17 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000425
| Global Round : 17 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000742
| Global Round : 17 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000130
| Global Round : 17 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000023
| Global Round : 17 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000791
| Global Round : 17 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000156
| Global Round : 17 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000128
| Global Round : 17 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000211
| Global Round : 17 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000096
| Global Round : 17 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000192
| Global Round : 17 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000614
| Global Round : 17 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.003275
| Global Round : 17 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.001023
| Global Round : 17 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000197
| Global Round : 17 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000010
| Global Round : 17 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000121
| Global Round : 17 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.001792
| Global Round : 17 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000608
| Global Round : 17 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000015
| Global Round : 17 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000061
| Global Round : 17 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.004953
| Global Round : 17 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.003269
| Global Round : 17 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000339
| Global Round : 17 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000040
| Global Round : 17 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000066
| Global Round : 17 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.002715
| Global Round : 17 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.001113
| Global Round : 17 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000247
| Global Round : 17 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000251
| Global Round : 17 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000135
| Global Round : 17 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000951
| Global Round : 17 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000033
| Global Round : 17 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000286
| Global Round : 17 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000070
| Global Round : 17 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000140
| Global Round : 17 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.057861
| Global Round : 17 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.001282
| Global Round : 17 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000197
| Global Round : 17 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.001206
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 4, Epoch: 17--------
------------------------------------------
| Global Round : 17 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000229
| Global Round : 17 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.003489
| Global Round : 17 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.006745
| Global Round : 17 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000051
| Global Round : 17 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000556
| Global Round : 17 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000404
| Global Round : 17 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.005255
| Global Round : 17 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.003782
| Global Round : 17 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000408
| Global Round : 17 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000172
| Global Round : 17 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.007397
| Global Round : 17 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000882
| Global Round : 17 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.009345
| Global Round : 17 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000012
| Global Round : 17 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000232
| Global Round : 17 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000072
| Global Round : 17 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000374
| Global Round : 17 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000196
| Global Round : 17 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.002810
| Global Round : 17 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000312
| Global Round : 17 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000480
| Global Round : 17 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000056
| Global Round : 17 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.007568
| Global Round : 17 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000340
| Global Round : 17 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000108
| Global Round : 17 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.004663
| Global Round : 17 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000406
| Global Round : 17 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000173
| Global Round : 17 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000199
| Global Round : 17 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000034
| Global Round : 17 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000337
| Global Round : 17 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000862
| Global Round : 17 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.003550
| Global Round : 17 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000787
| Global Round : 17 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000328
| Global Round : 17 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000215
| Global Round : 17 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.001274
| Global Round : 17 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.004687
| Global Round : 17 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000040
| Global Round : 17 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.001175
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 3, Epoch: 17--------
------------------------------------------
| Global Round : 17 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.026674
| Global Round : 17 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000021
| Global Round : 17 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000570
| Global Round : 17 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000071
| Global Round : 17 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.003346
| Global Round : 17 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000208
| Global Round : 17 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000154
| Global Round : 17 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000123
| Global Round : 17 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.009341
| Global Round : 17 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.007787
| Global Round : 17 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.002419
| Global Round : 17 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.002208
| Global Round : 17 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000164
| Global Round : 17 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000379
| Global Round : 17 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000209
| Global Round : 17 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000601
| Global Round : 17 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000681
| Global Round : 17 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.008112
| Global Round : 17 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000623
| Global Round : 17 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000494
| Global Round : 17 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000771
| Global Round : 17 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.001170
| Global Round : 17 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.009338
| Global Round : 17 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000541
| Global Round : 17 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.001906
| Global Round : 17 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000256
| Global Round : 17 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000081
| Global Round : 17 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000132
| Global Round : 17 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.002425
| Global Round : 17 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000854
| Global Round : 17 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000123
| Global Round : 17 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000375
| Global Round : 17 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000209
| Global Round : 17 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.005151
| Global Round : 17 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.001446
| Global Round : 17 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.001528
| Global Round : 17 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.008790
| Global Round : 17 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000052
| Global Round : 17 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000218
| Global Round : 17 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000500
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 6, Epoch: 17--------
------------------------------------------
| Global Round : 17 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000044
| Global Round : 17 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000100
| Global Round : 17 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000544
| Global Round : 17 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000158
| Global Round : 17 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000019
| Global Round : 17 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.001342
| Global Round : 17 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.001259
| Global Round : 17 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000644
| Global Round : 17 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.001085
| Global Round : 17 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000109
| Global Round : 17 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.011051
| Global Round : 17 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.006487
| Global Round : 17 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.001111
| Global Round : 17 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000818
| Global Round : 17 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000324
| Global Round : 17 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000164
| Global Round : 17 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000418
| Global Round : 17 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.002736
| Global Round : 17 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000137
| Global Round : 17 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000239
| Global Round : 17 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000019
| Global Round : 17 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.003052
| Global Round : 17 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000287
| Global Round : 17 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000127
| Global Round : 17 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.001081
| Global Round : 17 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.003775
| Global Round : 17 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.001187
| Global Round : 17 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.004494
| Global Round : 17 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000089
| Global Round : 17 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000648
| Global Round : 17 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000345
| Global Round : 17 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.005402
| Global Round : 17 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000563
| Global Round : 17 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.006294
| Global Round : 17 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000235
| Global Round : 17 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000219
| Global Round : 17 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.005447
| Global Round : 17 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000069
| Global Round : 17 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000276
| Global Round : 17 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000190
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 5, Epoch: 17--------
------------------------------------------
| Global Round : 17 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.001072
| Global Round : 17 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.001306
| Global Round : 17 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000011
| Global Round : 17 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000136
| Global Round : 17 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.001473
| Global Round : 17 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000354
| Global Round : 17 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000147
| Global Round : 17 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000495
| Global Round : 17 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.006555
| Global Round : 17 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.003907
| Global Round : 17 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000654
| Global Round : 17 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.001845
| Global Round : 17 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000258
| Global Round : 17 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.001811
| Global Round : 17 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000851
| Global Round : 17 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.014939
| Global Round : 17 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000005
| Global Round : 17 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.001194
| Global Round : 17 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.004485
| Global Round : 17 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.001131
| Global Round : 17 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.018898
| Global Round : 17 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000034
| Global Round : 17 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.001563
| Global Round : 17 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000059
| Global Round : 17 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.016317
| Global Round : 17 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.006334
| Global Round : 17 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.001005
| Global Round : 17 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000113
| Global Round : 17 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000436
| Global Round : 17 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.001174
| Global Round : 17 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000072
| Global Round : 17 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.001119
| Global Round : 17 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000571
| Global Round : 17 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.002027
| Global Round : 17 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.027167
| Global Round : 17 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.007361
| Global Round : 17 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000306
| Global Round : 17 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000232
| Global Round : 17 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.010912
| Global Round : 17 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000342
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 0, Epoch: 17--------
------------------------------------------
| Global Round : 17 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000308
| Global Round : 17 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000397
| Global Round : 17 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000050
| Global Round : 17 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000527
| Global Round : 17 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000357
| Global Round : 17 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.001651
| Global Round : 17 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000842
| Global Round : 17 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.001972
| Global Round : 17 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.002988
| Global Round : 17 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000233
| Global Round : 17 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000386
| Global Round : 17 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000168
| Global Round : 17 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000040
| Global Round : 17 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000050
| Global Round : 17 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000089
| Global Round : 17 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.005212
| Global Round : 17 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000453
| Global Round : 17 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.017327
| Global Round : 17 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.001128
| Global Round : 17 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.021405
| Global Round : 17 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000331
| Global Round : 17 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000179
| Global Round : 17 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000230
| Global Round : 17 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.001198
| Global Round : 17 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000943
| Global Round : 17 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000276
| Global Round : 17 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000068
| Global Round : 17 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000841
| Global Round : 17 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000063
| Global Round : 17 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.007858
| Global Round : 17 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000499
| Global Round : 17 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000457
| Global Round : 17 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.002352
| Global Round : 17 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000804
| Global Round : 17 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000165
| Global Round : 17 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.013966
| Global Round : 17 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000492
| Global Round : 17 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000283
| Global Round : 17 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000420
| Global Round : 17 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000165
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 49.68% | Val Loss: nan | Val Acc: 52.10%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 49.91% | Val Loss: nan | Val Acc: 52.10%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 49.91% | Val Loss: nan | Val Acc: 52.10%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 49.91% | Val Loss: nan | Val Acc: 52.10%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 49.91% | Val Loss: nan | Val Acc: 52.10%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 49.91% | Val Loss: nan | Val Acc: 52.10%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 49.91% | Val Loss: nan | Val Acc: 52.10%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 49.91% | Val Loss: nan | Val Acc: 52.10%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 49.91% | Val Loss: nan | Val Acc: 52.10%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 49.91% | Val Loss: nan | Val Acc: 52.10%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 49.91% | Val Loss: nan | Val Acc: 52.10%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 49.91% | Val Loss: nan | Val Acc: 52.10%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 49.91% | Val Loss: nan | Val Acc: 52.10%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 49.91% | Val Loss: nan | Val Acc: 52.10%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 49.91% | Val Loss: nan | Val Acc: 52.10%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 49.91% | Val Loss: nan | Val Acc: 52.10%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 49.91% | Val Loss: nan | Val Acc: 52.10%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 49.91% | Val Loss: nan | Val Acc: 52.10%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 49.91% | Val Loss: nan | Val Acc: 52.10%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 49.91% | Val Loss: nan | Val Acc: 52.10%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 49.91% | Val Loss: nan | Val Acc: 52.10%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 49.91% | Val Loss: nan | Val Acc: 52.10%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 49.91% | Val Loss: nan | Val Acc: 52.10%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 49.91% | Val Loss: nan | Val Acc: 52.10%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 49.91% | Val Loss: nan | Val Acc: 52.10%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 49.91% | Val Loss: nan | Val Acc: 52.10%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 49.91% | Val Loss: nan | Val Acc: 52.10%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 49.91% | Val Loss: nan | Val Acc: 52.10%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 49.91% | Val Loss: nan | Val Acc: 52.10%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 49.91% | Val Loss: nan | Val Acc: 52.10%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 49.91% | Val Loss: nan | Val Acc: 52.10%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 49.91% | Val Loss: nan | Val Acc: 52.10%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 49.91% | Val Loss: nan | Val Acc: 52.10%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 49.91% | Val Loss: nan | Val Acc: 52.10%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 49.91% | Val Loss: nan | Val Acc: 52.10%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 49.91% | Val Loss: nan | Val Acc: 52.10%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 49.91% | Val Loss: nan | Val Acc: 52.10%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 49.91% | Val Loss: nan | Val Acc: 52.10%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 49.91% | Val Loss: nan | Val Acc: 52.10%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 49.91% | Val Loss: nan | Val Acc: 52.10%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 49.91% | Val Loss: nan | Val Acc: 52.10%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 49.91% | Val Loss: nan | Val Acc: 52.10%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 49.91% | Val Loss: nan | Val Acc: 52.10%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 49.91% | Val Loss: nan | Val Acc: 52.10%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 49.91% | Val Loss: nan | Val Acc: 52.10%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 49.91% | Val Loss: nan | Val Acc: 52.10%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 49.91% | Val Loss: nan | Val Acc: 52.10%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 49.91% | Val Loss: nan | Val Acc: 52.10%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 49.91% | Val Loss: nan | Val Acc: 52.10%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 49.91% | Val Loss: nan | Val Acc: 52.10%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 52.10 %
----Attack Model Testing----
Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

------------------------------------------
------User: 9, Epoch: 17--------
------------------------------------------
| Global Round : 17 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.007886
| Global Round : 17 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000270
| Global Round : 17 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.001396
| Global Round : 17 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000350
| Global Round : 17 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000105
| Global Round : 17 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.008300
| Global Round : 17 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000048
| Global Round : 17 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000276
| Global Round : 17 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.016221
| Global Round : 17 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000418
| Global Round : 17 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.001610
| Global Round : 17 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.001099
| Global Round : 17 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.001472
| Global Round : 17 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000085
| Global Round : 17 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.001443
| Global Round : 17 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000783
| Global Round : 17 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000728
| Global Round : 17 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000579
| Global Round : 17 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000046
| Global Round : 17 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000792
| Global Round : 17 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000299
| Global Round : 17 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000014
| Global Round : 17 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000027
| Global Round : 17 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000091
| Global Round : 17 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000077
| Global Round : 17 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000129
| Global Round : 17 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.002906
| Global Round : 17 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.002527
| Global Round : 17 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.005939
| Global Round : 17 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000111
| Global Round : 17 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000078
| Global Round : 17 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000857
| Global Round : 17 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000035
| Global Round : 17 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.003383
| Global Round : 17 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000045
| Global Round : 17 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.001705
| Global Round : 17 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000197
| Global Round : 17 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000440
| Global Round : 17 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.001368
| Global Round : 17 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.024919
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 8, Epoch: 17--------
------------------------------------------
| Global Round : 17 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.001949
| Global Round : 17 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000246
| Global Round : 17 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000145
| Global Round : 17 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.008579
| Global Round : 17 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.003097
| Global Round : 17 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000679
| Global Round : 17 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000093
| Global Round : 17 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000340
| Global Round : 17 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000342
| Global Round : 17 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000810
| Global Round : 17 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000262
| Global Round : 17 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.003928
| Global Round : 17 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000018
| Global Round : 17 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000260
| Global Round : 17 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000440
| Global Round : 17 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000189
| Global Round : 17 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.002426
| Global Round : 17 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000807
| Global Round : 17 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000840
| Global Round : 17 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.001082
| Global Round : 17 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.002268
| Global Round : 17 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.004966
| Global Round : 17 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000141
| Global Round : 17 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000299
| Global Round : 17 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000183
| Global Round : 17 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000110
| Global Round : 17 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000303
| Global Round : 17 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000074
| Global Round : 17 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000024
| Global Round : 17 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000246
| Global Round : 17 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000308
| Global Round : 17 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000260
| Global Round : 17 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.004638
| Global Round : 17 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000301
| Global Round : 17 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000170
| Global Round : 17 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.001023
| Global Round : 17 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000116
| Global Round : 17 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000326
| Global Round : 17 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000155
| Global Round : 17 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.007215
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 2, Epoch: 17--------
------------------------------------------
| Global Round : 17 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000436
| Global Round : 17 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000181
| Global Round : 17 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.015194
| Global Round : 17 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000035
| Global Round : 17 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.001614
| Global Round : 17 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000478
| Global Round : 17 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000389
| Global Round : 17 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000163
| Global Round : 17 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.013913
| Global Round : 17 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000023
| Global Round : 17 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000153
| Global Round : 17 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000091
| Global Round : 17 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.003653
| Global Round : 17 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.002165
| Global Round : 17 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000150
| Global Round : 17 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000890
| Global Round : 17 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000221
| Global Round : 17 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000358
| Global Round : 17 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000430
| Global Round : 17 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000814
| Global Round : 17 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000149
| Global Round : 17 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.001207
| Global Round : 17 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000058
| Global Round : 17 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.001383
| Global Round : 17 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000631
| Global Round : 17 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000228
| Global Round : 17 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.004348
| Global Round : 17 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.004471
| Global Round : 17 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.001321
| Global Round : 17 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000695
| Global Round : 17 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000045
| Global Round : 17 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.001273
| Global Round : 17 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000028
| Global Round : 17 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.001171
| Global Round : 17 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000415
| Global Round : 17 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.006050
| Global Round : 17 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.013059
| Global Round : 17 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000540
| Global Round : 17 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.014439
| Global Round : 17 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000030
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 1, Epoch: 17--------
------------------------------------------
| Global Round : 17 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.020701
| Global Round : 17 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000043
| Global Round : 17 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000178
| Global Round : 17 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000120
| Global Round : 17 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000651
| Global Round : 17 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.002149
| Global Round : 17 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000166
| Global Round : 17 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000551
| Global Round : 17 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000102
| Global Round : 17 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000504
| Global Round : 17 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.021192
| Global Round : 17 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000053
| Global Round : 17 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.002425
| Global Round : 17 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000050
| Global Round : 17 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.001733
| Global Round : 17 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000745
| Global Round : 17 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000279
| Global Round : 17 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.007420
| Global Round : 17 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000238
| Global Round : 17 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000746
| Global Round : 17 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000964
| Global Round : 17 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000430
| Global Round : 17 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000286
| Global Round : 17 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000165
| Global Round : 17 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000173
| Global Round : 17 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000623
| Global Round : 17 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000067
| Global Round : 17 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000025
| Global Round : 17 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000449
| Global Round : 17 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.006300
| Global Round : 17 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.005223
| Global Round : 17 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.001089
| Global Round : 17 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000386
| Global Round : 17 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000330
| Global Round : 17 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.002026
| Global Round : 17 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000126
| Global Round : 17 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000209
| Global Round : 17 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000186
| Global Round : 17 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000052
| Global Round : 17 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.012742
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
 
Avg Training Stats after 18 global rounds:
Training Loss : nan
Train Accuracy: 9968.78% 

Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 51.10 %
----Attack Model Testing----
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
 60%|    | 18/30 [1:06:27<44:05, 220.43s/it]Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

Selected users for the training: [7 2 8 4 9 6 0 5 1 3]
------------------------------------------
------User: 7, Epoch: 18--------
------------------------------------------
| Global Round : 18 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000720
| Global Round : 18 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000024
| Global Round : 18 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000158
| Global Round : 18 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.119643
| Global Round : 18 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000033
| Global Round : 18 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000180
| Global Round : 18 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000045
| Global Round : 18 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000018
| Global Round : 18 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.003814
| Global Round : 18 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000089
| Global Round : 18 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000129
| Global Round : 18 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000661
| Global Round : 18 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000093
| Global Round : 18 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000356
| Global Round : 18 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000710
| Global Round : 18 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000269
| Global Round : 18 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.001486
| Global Round : 18 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000066
| Global Round : 18 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000078
| Global Round : 18 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000214
| Global Round : 18 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.005167
| Global Round : 18 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.001800
| Global Round : 18 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.002652
| Global Round : 18 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000022
| Global Round : 18 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.002405
| Global Round : 18 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000084
| Global Round : 18 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.005701
| Global Round : 18 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000294
| Global Round : 18 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000191
| Global Round : 18 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.002157
| Global Round : 18 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000146
| Global Round : 18 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.001398
| Global Round : 18 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000166
| Global Round : 18 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000346
| Global Round : 18 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000132
| Global Round : 18 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.003757
| Global Round : 18 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000025
| Global Round : 18 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.001822
| Global Round : 18 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000801
| Global Round : 18 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.007532
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 2, Epoch: 18--------
------------------------------------------
| Global Round : 18 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000122
| Global Round : 18 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000563
| Global Round : 18 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000117
| Global Round : 18 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000153
| Global Round : 18 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000168
| Global Round : 18 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.002383
| Global Round : 18 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000433
| Global Round : 18 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000362
| Global Round : 18 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.001870
| Global Round : 18 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000338
| Global Round : 18 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000363
| Global Round : 18 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.001527
| Global Round : 18 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000508
| Global Round : 18 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000041
| Global Round : 18 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.020780
| Global Round : 18 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000168
| Global Round : 18 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.003279
| Global Round : 18 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.007617
| Global Round : 18 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000239
| Global Round : 18 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000066
| Global Round : 18 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000712
| Global Round : 18 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000182
| Global Round : 18 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.007099
| Global Round : 18 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000090
| Global Round : 18 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000027
| Global Round : 18 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000141
| Global Round : 18 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000133
| Global Round : 18 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000362
| Global Round : 18 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.021174
| Global Round : 18 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.017010
| Global Round : 18 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.149976
| Global Round : 18 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000507
| Global Round : 18 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.001260
| Global Round : 18 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000413
| Global Round : 18 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000066
| Global Round : 18 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000115
| Global Round : 18 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000388
| Global Round : 18 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000295
| Global Round : 18 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000272
| Global Round : 18 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.006786
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 8, Epoch: 18--------
------------------------------------------
| Global Round : 18 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.002469
| Global Round : 18 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.001784
| Global Round : 18 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000276
| Global Round : 18 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000647
| Global Round : 18 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000832
| Global Round : 18 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000043
| Global Round : 18 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.001706
| Global Round : 18 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000274
| Global Round : 18 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.001071
| Global Round : 18 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000477
| Global Round : 18 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000231
| Global Round : 18 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.001619
| Global Round : 18 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000328
| Global Round : 18 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000043
| Global Round : 18 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.105464
| Global Round : 18 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000388
| Global Round : 18 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.015126
| Global Round : 18 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000075
| Global Round : 18 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.002089
| Global Round : 18 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.001684
| Global Round : 18 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000166
| Global Round : 18 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000941
| Global Round : 18 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000115
| Global Round : 18 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000021
| Global Round : 18 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000936
| Global Round : 18 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000070
| Global Round : 18 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.001191
| Global Round : 18 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000062
| Global Round : 18 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000922
| Global Round : 18 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000615
| Global Round : 18 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000471
| Global Round : 18 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000173
| Global Round : 18 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000196
| Global Round : 18 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000308
| Global Round : 18 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000181
| Global Round : 18 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000236
| Global Round : 18 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000298
| Global Round : 18 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000047
| Global Round : 18 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000026
| Global Round : 18 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000084
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 4, Epoch: 18--------
------------------------------------------
| Global Round : 18 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000030
| Global Round : 18 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.004058
| Global Round : 18 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000097
| Global Round : 18 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.001266
| Global Round : 18 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000442
| Global Round : 18 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000067
| Global Round : 18 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000435
| Global Round : 18 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000492
| Global Round : 18 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000363
| Global Round : 18 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000149
| Global Round : 18 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000231
| Global Round : 18 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.001091
| Global Round : 18 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.015610
| Global Round : 18 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.004021
| Global Round : 18 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000618
| Global Round : 18 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000057
| Global Round : 18 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000417
| Global Round : 18 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000120
| Global Round : 18 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000117
| Global Round : 18 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000210
| Global Round : 18 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000150
| Global Round : 18 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.001044
| Global Round : 18 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.005271
| Global Round : 18 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.001429
| Global Round : 18 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000124
| Global Round : 18 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000113
| Global Round : 18 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000394
| Global Round : 18 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000164
| Global Round : 18 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000070
| Global Round : 18 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.003462
| Global Round : 18 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000105
| Global Round : 18 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.003121
| Global Round : 18 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000262
| Global Round : 18 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.006058
| Global Round : 18 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000581
| Global Round : 18 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.005886
| Global Round : 18 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000161
| Global Round : 18 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.015900
| Global Round : 18 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000720
| Global Round : 18 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000375
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 9, Epoch: 18--------
------------------------------------------
| Global Round : 18 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000190
| Global Round : 18 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.002093
| Global Round : 18 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000347
| Global Round : 18 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000073
| Global Round : 18 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000595
| Global Round : 18 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.001142
| Global Round : 18 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.002297
| Global Round : 18 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000018
| Global Round : 18 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.001588
| Global Round : 18 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000300
| Global Round : 18 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000841
| Global Round : 18 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000050
| Global Round : 18 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000341
| Global Round : 18 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000896
| Global Round : 18 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000136
| Global Round : 18 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.001574
| Global Round : 18 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000085
| Global Round : 18 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.001651
| Global Round : 18 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000032
| Global Round : 18 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.002350
| Global Round : 18 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000078
| Global Round : 18 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000056
| Global Round : 18 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.023993
| Global Round : 18 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.002969
| Global Round : 18 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000796
| Global Round : 18 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000052
| Global Round : 18 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000229
| Global Round : 18 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.002442
| Global Round : 18 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000580
| Global Round : 18 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000128
| Global Round : 18 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000448
| Global Round : 18 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000645
| Global Round : 18 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000771
| Global Round : 18 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000009
| Global Round : 18 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000029
| Global Round : 18 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000448
| Global Round : 18 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000158
| Global Round : 18 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000049
| Global Round : 18 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000425
| Global Round : 18 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000698
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 6, Epoch: 18--------
------------------------------------------
| Global Round : 18 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000127
| Global Round : 18 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000066
| Global Round : 18 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000215
| Global Round : 18 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000228
| Global Round : 18 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000396
| Global Round : 18 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000838
| Global Round : 18 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.006735
| Global Round : 18 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000438
| Global Round : 18 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.001400
| Global Round : 18 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.002322
| Global Round : 18 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000563
| Global Round : 18 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.002775
| Global Round : 18 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000397
| Global Round : 18 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000052
| Global Round : 18 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000435
| Global Round : 18 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.010442
| Global Round : 18 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.002702
| Global Round : 18 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000266
| Global Round : 18 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000042
| Global Round : 18 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.002780
| Global Round : 18 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000257
| Global Round : 18 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.001067
| Global Round : 18 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.005090
| Global Round : 18 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000246
| Global Round : 18 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.002694
| Global Round : 18 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000014
| Global Round : 18 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.004699
| Global Round : 18 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.015340
| Global Round : 18 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000263
| Global Round : 18 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000033
| Global Round : 18 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000120
| Global Round : 18 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000267
| Global Round : 18 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000134
| Global Round : 18 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.002021
| Global Round : 18 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000324
| Global Round : 18 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000152
| Global Round : 18 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.005495
| Global Round : 18 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000170
| Global Round : 18 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000985
| Global Round : 18 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.007558
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 0, Epoch: 18--------
------------------------------------------
| Global Round : 18 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000503
| Global Round : 18 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.003134
| Global Round : 18 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000138
| Global Round : 18 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.002284
| Global Round : 18 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000056
| Global Round : 18 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000288
| Global Round : 18 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.002938
| Global Round : 18 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.001074
| Global Round : 18 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000013
| Global Round : 18 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000179
| Global Round : 18 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.003089
| Global Round : 18 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000639
| Global Round : 18 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000743
| Global Round : 18 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000614
| Global Round : 18 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.005303
| Global Round : 18 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000497
| Global Round : 18 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000740
| Global Round : 18 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000298
| Global Round : 18 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.001531
| Global Round : 18 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000123
| Global Round : 18 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000030
| Global Round : 18 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000112
| Global Round : 18 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000482
| Global Round : 18 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000024
| Global Round : 18 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000967
| Global Round : 18 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.079209
| Global Round : 18 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000059
| Global Round : 18 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.002319
| Global Round : 18 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.003598
| Global Round : 18 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000071
| Global Round : 18 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000419
| Global Round : 18 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000680
| Global Round : 18 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000692
| Global Round : 18 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000069
| Global Round : 18 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000045
| Global Round : 18 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000129
| Global Round : 18 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000008
| Global Round : 18 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000364
| Global Round : 18 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000490
| Global Round : 18 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.003755
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.40%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.40%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.40%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.40%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.40%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.40%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.40%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.40%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.40%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.40%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.40%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.40%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.40%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.40%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.40%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.40%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.40%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.40%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.40%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.40%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.40%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.40%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.40%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.40%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.40%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.40%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.40%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.40%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.40%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.40%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.40%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.40%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.40%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.40%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.40%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.40%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.40%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.40%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.40%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.40%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.40%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.40%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.40%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.40%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.40%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.40%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.40%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.40%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.40%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.40%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 51.40 %
----Attack Model Testing----
Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

------------------------------------------
------User: 5, Epoch: 18--------
------------------------------------------
| Global Round : 18 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000513
| Global Round : 18 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000065
| Global Round : 18 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000601
| Global Round : 18 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000238
| Global Round : 18 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000127
| Global Round : 18 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.002445
| Global Round : 18 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000160
| Global Round : 18 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000061
| Global Round : 18 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000013
| Global Round : 18 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000072
| Global Round : 18 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000039
| Global Round : 18 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.001148
| Global Round : 18 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000901
| Global Round : 18 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000348
| Global Round : 18 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000029
| Global Round : 18 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.001943
| Global Round : 18 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000918
| Global Round : 18 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.004759
| Global Round : 18 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000340
| Global Round : 18 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000054
| Global Round : 18 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.001876
| Global Round : 18 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000339
| Global Round : 18 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.001041
| Global Round : 18 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000530
| Global Round : 18 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000159
| Global Round : 18 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000176
| Global Round : 18 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000687
| Global Round : 18 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.003523
| Global Round : 18 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000218
| Global Round : 18 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000129
| Global Round : 18 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.001574
| Global Round : 18 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000068
| Global Round : 18 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000315
| Global Round : 18 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000486
| Global Round : 18 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.002905
| Global Round : 18 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000009
| Global Round : 18 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000745
| Global Round : 18 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.007343
| Global Round : 18 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000453
| Global Round : 18 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.001335
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 1, Epoch: 18--------
------------------------------------------
| Global Round : 18 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000054
| Global Round : 18 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.019810
| Global Round : 18 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000107
| Global Round : 18 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000046
| Global Round : 18 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000025
| Global Round : 18 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.002743
| Global Round : 18 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000437
| Global Round : 18 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000233
| Global Round : 18 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.001565
| Global Round : 18 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000207
| Global Round : 18 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000212
| Global Round : 18 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.054416
| Global Round : 18 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000104
| Global Round : 18 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000589
| Global Round : 18 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.008535
| Global Round : 18 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.001153
| Global Round : 18 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.004702
| Global Round : 18 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.003231
| Global Round : 18 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000369
| Global Round : 18 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.004268
| Global Round : 18 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000587
| Global Round : 18 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000638
| Global Round : 18 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000087
| Global Round : 18 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000062
| Global Round : 18 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000015
| Global Round : 18 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.002702
| Global Round : 18 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000062
| Global Round : 18 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000224
| Global Round : 18 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000031
| Global Round : 18 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000334
| Global Round : 18 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000458
| Global Round : 18 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000279
| Global Round : 18 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000110
| Global Round : 18 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.001060
| Global Round : 18 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000112
| Global Round : 18 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000029
| Global Round : 18 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000313
| Global Round : 18 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.006466
| Global Round : 18 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.003704
| Global Round : 18 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000325
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 3, Epoch: 18--------
------------------------------------------
| Global Round : 18 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.001726
| Global Round : 18 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000233
| Global Round : 18 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000655
| Global Round : 18 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000101
| Global Round : 18 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000404
| Global Round : 18 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000270
| Global Round : 18 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000093
| Global Round : 18 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000014
| Global Round : 18 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000319
| Global Round : 18 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000091
| Global Round : 18 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000140
| Global Round : 18 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000023
| Global Round : 18 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.005791
| Global Round : 18 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000008
| Global Round : 18 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000170
| Global Round : 18 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000026
| Global Round : 18 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.001109
| Global Round : 18 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000116
| Global Round : 18 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.002406
| Global Round : 18 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.001577
| Global Round : 18 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000227
| Global Round : 18 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000038
| Global Round : 18 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000049
| Global Round : 18 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000267
| Global Round : 18 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.001537
| Global Round : 18 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000078
| Global Round : 18 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000677
| Global Round : 18 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000065
| Global Round : 18 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000371
| Global Round : 18 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.013971
| Global Round : 18 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000348
| Global Round : 18 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000050
| Global Round : 18 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000189
| Global Round : 18 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000150
| Global Round : 18 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000005
| Global Round : 18 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000149
| Global Round : 18 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000066
| Global Round : 18 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000163
| Global Round : 18 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000050
| Global Round : 18 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.020161
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
 
Avg Training Stats after 19 global rounds:
Training Loss : nan
Train Accuracy: 9970.32% 

Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 50.80 %
----Attack Model Testing----
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
 63%|   | 19/30 [1:10:17<40:56, 223.29s/it]Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

Selected users for the training: [7 9 1 5 4 2 0 8 6 3]
------------------------------------------
------User: 7, Epoch: 19--------
------------------------------------------
| Global Round : 19 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000277
| Global Round : 19 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.001653
| Global Round : 19 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000028
| Global Round : 19 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.010451
| Global Round : 19 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000765
| Global Round : 19 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.003359
| Global Round : 19 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.009622
| Global Round : 19 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.002314
| Global Round : 19 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000229
| Global Round : 19 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000736
| Global Round : 19 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000357
| Global Round : 19 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000097
| Global Round : 19 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000859
| Global Round : 19 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000683
| Global Round : 19 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000014
| Global Round : 19 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000057
| Global Round : 19 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.001202
| Global Round : 19 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000063
| Global Round : 19 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000264
| Global Round : 19 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000009
| Global Round : 19 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000051
| Global Round : 19 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.011762
| Global Round : 19 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000084
| Global Round : 19 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000336
| Global Round : 19 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000034
| Global Round : 19 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000230
| Global Round : 19 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000466
| Global Round : 19 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.006342
| Global Round : 19 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000783
| Global Round : 19 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000088
| Global Round : 19 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.002565
| Global Round : 19 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.002342
| Global Round : 19 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.002513
| Global Round : 19 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.007382
| Global Round : 19 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.002097
| Global Round : 19 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.010333
| Global Round : 19 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000137
| Global Round : 19 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.048713
| Global Round : 19 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000052
| Global Round : 19 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000949
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 9, Epoch: 19--------
------------------------------------------
| Global Round : 19 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000010
| Global Round : 19 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000246
| Global Round : 19 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000371
| Global Round : 19 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000939
| Global Round : 19 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000043
| Global Round : 19 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000413
| Global Round : 19 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.001606
| Global Round : 19 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000003
| Global Round : 19 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000473
| Global Round : 19 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000243
| Global Round : 19 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000145
| Global Round : 19 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000155
| Global Round : 19 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000826
| Global Round : 19 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.019057
| Global Round : 19 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.001722
| Global Round : 19 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000489
| Global Round : 19 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000024
| Global Round : 19 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000078
| Global Round : 19 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000009
| Global Round : 19 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000681
| Global Round : 19 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000136
| Global Round : 19 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000173
| Global Round : 19 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000075
| Global Round : 19 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000576
| Global Round : 19 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.003830
| Global Round : 19 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000959
| Global Round : 19 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000081
| Global Round : 19 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000123
| Global Round : 19 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000776
| Global Round : 19 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.005029
| Global Round : 19 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.004037
| Global Round : 19 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000033
| Global Round : 19 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000030
| Global Round : 19 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.015208
| Global Round : 19 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.004928
| Global Round : 19 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.002923
| Global Round : 19 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000108
| Global Round : 19 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000009
| Global Round : 19 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000111
| Global Round : 19 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000008
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 1, Epoch: 19--------
------------------------------------------
| Global Round : 19 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.003369
| Global Round : 19 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000089
| Global Round : 19 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000381
| Global Round : 19 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000111
| Global Round : 19 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.001612
| Global Round : 19 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000116
| Global Round : 19 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000057
| Global Round : 19 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000257
| Global Round : 19 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.001026
| Global Round : 19 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000441
| Global Round : 19 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000100
| Global Round : 19 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000047
| Global Round : 19 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.001433
| Global Round : 19 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.006513
| Global Round : 19 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000054
| Global Round : 19 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.005929
| Global Round : 19 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000205
| Global Round : 19 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000077
| Global Round : 19 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000108
| Global Round : 19 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.001421
| Global Round : 19 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000022
| Global Round : 19 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000018
| Global Round : 19 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000089
| Global Round : 19 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000275
| Global Round : 19 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000164
| Global Round : 19 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.001905
| Global Round : 19 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000019
| Global Round : 19 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.002136
| Global Round : 19 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000249
| Global Round : 19 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.002223
| Global Round : 19 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000369
| Global Round : 19 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000297
| Global Round : 19 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000111
| Global Round : 19 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000780
| Global Round : 19 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000121
| Global Round : 19 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000036
| Global Round : 19 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000271
| Global Round : 19 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000135
| Global Round : 19 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000023
| Global Round : 19 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000105
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 5, Epoch: 19--------
------------------------------------------
| Global Round : 19 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000053
| Global Round : 19 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000195
| Global Round : 19 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000028
| Global Round : 19 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000171
| Global Round : 19 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000211
| Global Round : 19 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000020
| Global Round : 19 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000441
| Global Round : 19 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.011563
| Global Round : 19 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.003113
| Global Round : 19 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000050
| Global Round : 19 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000062
| Global Round : 19 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000041
| Global Round : 19 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000177
| Global Round : 19 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000148
| Global Round : 19 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000003
| Global Round : 19 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.001512
| Global Round : 19 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.001620
| Global Round : 19 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000452
| Global Round : 19 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000327
| Global Round : 19 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000015
| Global Round : 19 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.053204
| Global Round : 19 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000441
| Global Round : 19 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000407
| Global Round : 19 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000073
| Global Round : 19 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000069
| Global Round : 19 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000066
| Global Round : 19 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000125
| Global Round : 19 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.004665
| Global Round : 19 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000818
| Global Round : 19 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000358
| Global Round : 19 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000982
| Global Round : 19 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.007747
| Global Round : 19 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.025052
| Global Round : 19 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000245
| Global Round : 19 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000014
| Global Round : 19 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000081
| Global Round : 19 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000184
| Global Round : 19 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.003911
| Global Round : 19 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000057
| Global Round : 19 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000776
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 4, Epoch: 19--------
------------------------------------------
| Global Round : 19 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.002531
| Global Round : 19 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000767
| Global Round : 19 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000012
| Global Round : 19 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000198
| Global Round : 19 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000023
| Global Round : 19 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000468
| Global Round : 19 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000066
| Global Round : 19 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000223
| Global Round : 19 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.001144
| Global Round : 19 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000025
| Global Round : 19 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000278
| Global Round : 19 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000211
| Global Round : 19 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.012935
| Global Round : 19 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000010
| Global Round : 19 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000451
| Global Round : 19 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.001222
| Global Round : 19 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000172
| Global Round : 19 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.003275
| Global Round : 19 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000077
| Global Round : 19 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000032
| Global Round : 19 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000082
| Global Round : 19 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000469
| Global Round : 19 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000238
| Global Round : 19 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000151
| Global Round : 19 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000500
| Global Round : 19 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000676
| Global Round : 19 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000635
| Global Round : 19 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000236
| Global Round : 19 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000158
| Global Round : 19 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000924
| Global Round : 19 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000228
| Global Round : 19 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.004389
| Global Round : 19 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000202
| Global Round : 19 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000305
| Global Round : 19 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000408
| Global Round : 19 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000049
| Global Round : 19 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.013738
| Global Round : 19 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.001271
| Global Round : 19 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.002609
| Global Round : 19 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000625
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 2, Epoch: 19--------
------------------------------------------
| Global Round : 19 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000309
| Global Round : 19 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000445
| Global Round : 19 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000600
| Global Round : 19 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000185
| Global Round : 19 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000065
| Global Round : 19 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000137
| Global Round : 19 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000945
| Global Round : 19 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000205
| Global Round : 19 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000700
| Global Round : 19 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000807
| Global Round : 19 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000259
| Global Round : 19 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.005492
| Global Round : 19 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000257
| Global Round : 19 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000129
| Global Round : 19 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000041
| Global Round : 19 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000090
| Global Round : 19 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000510
| Global Round : 19 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000353
| Global Round : 19 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.001213
| Global Round : 19 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000084
| Global Round : 19 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000638
| Global Round : 19 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.004442
| Global Round : 19 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000025
| Global Round : 19 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000760
| Global Round : 19 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000891
| Global Round : 19 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000099
| Global Round : 19 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000048
| Global Round : 19 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000139
| Global Round : 19 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.003742
| Global Round : 19 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000234
| Global Round : 19 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000188
| Global Round : 19 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000595
| Global Round : 19 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000035
| Global Round : 19 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000012
| Global Round : 19 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000032
| Global Round : 19 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.001407
| Global Round : 19 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000309
| Global Round : 19 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000081
| Global Round : 19 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000729
| Global Round : 19 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.001166
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 0, Epoch: 19--------
------------------------------------------
| Global Round : 19 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000571
| Global Round : 19 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000334
| Global Round : 19 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000081
| Global Round : 19 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000443
| Global Round : 19 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.001003
| Global Round : 19 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000234
| Global Round : 19 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.005948
| Global Round : 19 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000021
| Global Round : 19 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.003967
| Global Round : 19 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.002323
| Global Round : 19 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000322
| Global Round : 19 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.003084
| Global Round : 19 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000035
| Global Round : 19 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.001447
| Global Round : 19 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.001156
| Global Round : 19 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.006644
| Global Round : 19 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.001728
| Global Round : 19 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.001198
| Global Round : 19 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000777
| Global Round : 19 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000043
| Global Round : 19 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000252
| Global Round : 19 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000525
| Global Round : 19 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000813
| Global Round : 19 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000615
| Global Round : 19 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000410
| Global Round : 19 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.001860
| Global Round : 19 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000137
| Global Round : 19 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.002109
| Global Round : 19 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000068
| Global Round : 19 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000686
| Global Round : 19 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.005109
| Global Round : 19 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.001680
| Global Round : 19 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000074
| Global Round : 19 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000437
| Global Round : 19 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000405
| Global Round : 19 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.005439
| Global Round : 19 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000217
| Global Round : 19 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000446
| Global Round : 19 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.001676
| Global Round : 19 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000006
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 49.76% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 48.90 %
----Attack Model Testing----
Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

------------------------------------------
------User: 8, Epoch: 19--------
------------------------------------------
| Global Round : 19 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.002640
| Global Round : 19 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000237
| Global Round : 19 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000103
| Global Round : 19 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.001826
| Global Round : 19 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.003881
| Global Round : 19 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000074
| Global Round : 19 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000094
| Global Round : 19 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000095
| Global Round : 19 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.003287
| Global Round : 19 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.002895
| Global Round : 19 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000219
| Global Round : 19 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000290
| Global Round : 19 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000164
| Global Round : 19 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000184
| Global Round : 19 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000374
| Global Round : 19 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000280
| Global Round : 19 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000027
| Global Round : 19 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000222
| Global Round : 19 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000016
| Global Round : 19 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000058
| Global Round : 19 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000023
| Global Round : 19 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000054
| Global Round : 19 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000349
| Global Round : 19 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000312
| Global Round : 19 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.001911
| Global Round : 19 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.002837
| Global Round : 19 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000357
| Global Round : 19 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.005399
| Global Round : 19 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000092
| Global Round : 19 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.003185
| Global Round : 19 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.001375
| Global Round : 19 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000466
| Global Round : 19 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000192
| Global Round : 19 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000198
| Global Round : 19 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000076
| Global Round : 19 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.013958
| Global Round : 19 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000373
| Global Round : 19 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000019
| Global Round : 19 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.001795
| Global Round : 19 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000230
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 6, Epoch: 19--------
------------------------------------------
| Global Round : 19 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000968
| Global Round : 19 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000215
| Global Round : 19 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000176
| Global Round : 19 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000183
| Global Round : 19 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000706
| Global Round : 19 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000902
| Global Round : 19 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000275
| Global Round : 19 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000107
| Global Round : 19 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000841
| Global Round : 19 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.001904
| Global Round : 19 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000025
| Global Round : 19 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000028
| Global Round : 19 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000130
| Global Round : 19 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.001670
| Global Round : 19 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000475
| Global Round : 19 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000511
| Global Round : 19 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000075
| Global Round : 19 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000019
| Global Round : 19 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000029
| Global Round : 19 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000734
| Global Round : 19 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000077
| Global Round : 19 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000030
| Global Round : 19 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000421
| Global Round : 19 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000024
| Global Round : 19 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000155
| Global Round : 19 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.001831
| Global Round : 19 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000665
| Global Round : 19 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000922
| Global Round : 19 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.001128
| Global Round : 19 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000096
| Global Round : 19 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.001109
| Global Round : 19 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000033
| Global Round : 19 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000323
| Global Round : 19 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000133
| Global Round : 19 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000026
| Global Round : 19 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000011
| Global Round : 19 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.001222
| Global Round : 19 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.003392
| Global Round : 19 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000373
| Global Round : 19 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000023
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 3, Epoch: 19--------
------------------------------------------
| Global Round : 19 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.001247
| Global Round : 19 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.001711
| Global Round : 19 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.001628
| Global Round : 19 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.001565
| Global Round : 19 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000344
| Global Round : 19 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000118
| Global Round : 19 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.003122
| Global Round : 19 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000856
| Global Round : 19 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000030
| Global Round : 19 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000227
| Global Round : 19 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000093
| Global Round : 19 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000024
| Global Round : 19 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000239
| Global Round : 19 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000046
| Global Round : 19 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000105
| Global Round : 19 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.005857
| Global Round : 19 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.001530
| Global Round : 19 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000315
| Global Round : 19 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000042
| Global Round : 19 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000071
| Global Round : 19 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000270
| Global Round : 19 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000121
| Global Round : 19 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000073
| Global Round : 19 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000032
| Global Round : 19 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000503
| Global Round : 19 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.001053
| Global Round : 19 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000093
| Global Round : 19 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000451
| Global Round : 19 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.006329
| Global Round : 19 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000189
| Global Round : 19 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.001366
| Global Round : 19 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000154
| Global Round : 19 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000080
| Global Round : 19 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.004299
| Global Round : 19 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.002323
| Global Round : 19 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000194
| Global Round : 19 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000426
| Global Round : 19 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.001037
| Global Round : 19 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000132
| Global Round : 19 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000972
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
 
Avg Training Stats after 20 global rounds:
Training Loss : nan
Train Accuracy: 9971.80% 

Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 49.71% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.80%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 50.80 %
----Attack Model Testing----
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
 67%|   | 20/30 [1:14:20<38:11, 229.16s/it]Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

Selected users for the training: [3 0 4 7 9 5 1 8 6 2]
------------------------------------------
------User: 3, Epoch: 20--------
------------------------------------------
| Global Round : 20 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000063
| Global Round : 20 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000746
| Global Round : 20 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000351
| Global Round : 20 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000123
| Global Round : 20 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000053
| Global Round : 20 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000423
| Global Round : 20 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000704
| Global Round : 20 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000025
| Global Round : 20 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.003658
| Global Round : 20 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000005
| Global Round : 20 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000028
| Global Round : 20 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000525
| Global Round : 20 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000159
| Global Round : 20 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.003534
| Global Round : 20 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.001808
| Global Round : 20 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000038
| Global Round : 20 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000246
| Global Round : 20 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.002618
| Global Round : 20 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.005476
| Global Round : 20 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000110
| Global Round : 20 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.001242
| Global Round : 20 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.004729
| Global Round : 20 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000016
| Global Round : 20 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000788
| Global Round : 20 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000246
| Global Round : 20 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.004148
| Global Round : 20 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000359
| Global Round : 20 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000164
| Global Round : 20 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.001895
| Global Round : 20 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000046
| Global Round : 20 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000010
| Global Round : 20 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000026
| Global Round : 20 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000537
| Global Round : 20 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000321
| Global Round : 20 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000873
| Global Round : 20 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000079
| Global Round : 20 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000112
| Global Round : 20 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000321
| Global Round : 20 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000182
| Global Round : 20 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000169
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 0, Epoch: 20--------
------------------------------------------
| Global Round : 20 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000098
| Global Round : 20 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000031
| Global Round : 20 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.003041
| Global Round : 20 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000357
| Global Round : 20 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000062
| Global Round : 20 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000047
| Global Round : 20 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000360
| Global Round : 20 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000184
| Global Round : 20 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000033
| Global Round : 20 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.001891
| Global Round : 20 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000584
| Global Round : 20 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000290
| Global Round : 20 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.002508
| Global Round : 20 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000076
| Global Round : 20 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000330
| Global Round : 20 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000512
| Global Round : 20 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000194
| Global Round : 20 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000033
| Global Round : 20 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.001793
| Global Round : 20 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000122
| Global Round : 20 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.002220
| Global Round : 20 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000044
| Global Round : 20 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000018
| Global Round : 20 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000096
| Global Round : 20 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000711
| Global Round : 20 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000277
| Global Round : 20 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000729
| Global Round : 20 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.003990
| Global Round : 20 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000102
| Global Round : 20 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000919
| Global Round : 20 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000335
| Global Round : 20 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000376
| Global Round : 20 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000070
| Global Round : 20 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000073
| Global Round : 20 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000546
| Global Round : 20 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000266
| Global Round : 20 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000045
| Global Round : 20 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.004058
| Global Round : 20 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.003449
| Global Round : 20 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.001200
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 49.75% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 49.80 %
----Attack Model Testing----
Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

------------------------------------------
------User: 4, Epoch: 20--------
------------------------------------------
| Global Round : 20 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000017
| Global Round : 20 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000060
| Global Round : 20 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000668
| Global Round : 20 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000369
| Global Round : 20 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000491
| Global Round : 20 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000788
| Global Round : 20 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000006
| Global Round : 20 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.002635
| Global Round : 20 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000138
| Global Round : 20 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000032
| Global Round : 20 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000137
| Global Round : 20 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000523
| Global Round : 20 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000573
| Global Round : 20 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000377
| Global Round : 20 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000582
| Global Round : 20 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000005
| Global Round : 20 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.005226
| Global Round : 20 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.001847
| Global Round : 20 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000284
| Global Round : 20 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000118
| Global Round : 20 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000108
| Global Round : 20 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.001913
| Global Round : 20 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.022527
| Global Round : 20 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000797
| Global Round : 20 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.002736
| Global Round : 20 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000054
| Global Round : 20 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000393
| Global Round : 20 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000641
| Global Round : 20 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000387
| Global Round : 20 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000637
| Global Round : 20 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000264
| Global Round : 20 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.006851
| Global Round : 20 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000075
| Global Round : 20 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000314
| Global Round : 20 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000264
| Global Round : 20 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000653
| Global Round : 20 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000158
| Global Round : 20 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000046
| Global Round : 20 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000101
| Global Round : 20 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000400
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 7, Epoch: 20--------
------------------------------------------
| Global Round : 20 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000376
| Global Round : 20 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000397
| Global Round : 20 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000146
| Global Round : 20 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000017
| Global Round : 20 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.004394
| Global Round : 20 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000020
| Global Round : 20 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000137
| Global Round : 20 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000419
| Global Round : 20 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000149
| Global Round : 20 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000398
| Global Round : 20 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000083
| Global Round : 20 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000049
| Global Round : 20 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000894
| Global Round : 20 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000466
| Global Round : 20 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000029
| Global Round : 20 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000406
| Global Round : 20 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000038
| Global Round : 20 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000214
| Global Round : 20 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000310
| Global Round : 20 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.006556
| Global Round : 20 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.004357
| Global Round : 20 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000009
| Global Round : 20 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000028
| Global Round : 20 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000358
| Global Round : 20 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.002848
| Global Round : 20 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000449
| Global Round : 20 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000027
| Global Round : 20 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000079
| Global Round : 20 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.001829
| Global Round : 20 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000147
| Global Round : 20 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.001423
| Global Round : 20 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000146
| Global Round : 20 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000005
| Global Round : 20 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000271
| Global Round : 20 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.001156
| Global Round : 20 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000689
| Global Round : 20 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000482
| Global Round : 20 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000010
| Global Round : 20 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000207
| Global Round : 20 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.005529
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 9, Epoch: 20--------
------------------------------------------
| Global Round : 20 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000037
| Global Round : 20 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000619
| Global Round : 20 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.003129
| Global Round : 20 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000114
| Global Round : 20 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.002598
| Global Round : 20 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000013
| Global Round : 20 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000959
| Global Round : 20 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.002575
| Global Round : 20 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000229
| Global Round : 20 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.007021
| Global Round : 20 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000293
| Global Round : 20 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.002200
| Global Round : 20 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000498
| Global Round : 20 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000289
| Global Round : 20 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000321
| Global Round : 20 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000050
| Global Round : 20 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.006336
| Global Round : 20 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.001579
| Global Round : 20 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.002374
| Global Round : 20 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000109
| Global Round : 20 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000271
| Global Round : 20 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000174
| Global Round : 20 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000675
| Global Round : 20 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000094
| Global Round : 20 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.001241
| Global Round : 20 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000013
| Global Round : 20 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000032
| Global Round : 20 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.002988
| Global Round : 20 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.002967
| Global Round : 20 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000061
| Global Round : 20 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000873
| Global Round : 20 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000560
| Global Round : 20 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000629
| Global Round : 20 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000004
| Global Round : 20 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000863
| Global Round : 20 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000564
| Global Round : 20 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.001795
| Global Round : 20 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.001752
| Global Round : 20 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000006
| Global Round : 20 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.001231
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 5, Epoch: 20--------
------------------------------------------
| Global Round : 20 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.003543
| Global Round : 20 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000070
| Global Round : 20 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000086
| Global Round : 20 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000672
| Global Round : 20 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000638
| Global Round : 20 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000051
| Global Round : 20 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000648
| Global Round : 20 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.001164
| Global Round : 20 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000041
| Global Round : 20 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000007
| Global Round : 20 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000073
| Global Round : 20 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000239
| Global Round : 20 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000046
| Global Round : 20 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.002282
| Global Round : 20 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000261
| Global Round : 20 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.001078
| Global Round : 20 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000437
| Global Round : 20 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000276
| Global Round : 20 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000319
| Global Round : 20 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.001103
| Global Round : 20 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.001896
| Global Round : 20 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.001815
| Global Round : 20 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000484
| Global Round : 20 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000006
| Global Round : 20 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000172
| Global Round : 20 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000935
| Global Round : 20 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000816
| Global Round : 20 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.003043
| Global Round : 20 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000226
| Global Round : 20 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000143
| Global Round : 20 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000605
| Global Round : 20 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.002536
| Global Round : 20 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000625
| Global Round : 20 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000466
| Global Round : 20 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000318
| Global Round : 20 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.002573
| Global Round : 20 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000135
| Global Round : 20 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000103
| Global Round : 20 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000091
| Global Round : 20 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000018
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 1, Epoch: 20--------
------------------------------------------
| Global Round : 20 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.008443
| Global Round : 20 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000232
| Global Round : 20 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000019
| Global Round : 20 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000030
| Global Round : 20 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.001293
| Global Round : 20 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000202
| Global Round : 20 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000492
| Global Round : 20 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000234
| Global Round : 20 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.005217
| Global Round : 20 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000122
| Global Round : 20 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000106
| Global Round : 20 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.001311
| Global Round : 20 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000594
| Global Round : 20 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000609
| Global Round : 20 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000716
| Global Round : 20 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000059
| Global Round : 20 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000265
| Global Round : 20 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000065
| Global Round : 20 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000476
| Global Round : 20 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000341
| Global Round : 20 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000231
| Global Round : 20 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.015216
| Global Round : 20 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000228
| Global Round : 20 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000034
| Global Round : 20 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000100
| Global Round : 20 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000542
| Global Round : 20 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000663
| Global Round : 20 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000074
| Global Round : 20 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000013
| Global Round : 20 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.001461
| Global Round : 20 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000128
| Global Round : 20 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000196
| Global Round : 20 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000077
| Global Round : 20 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000529
| Global Round : 20 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000837
| Global Round : 20 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.003909
| Global Round : 20 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000022
| Global Round : 20 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000187
| Global Round : 20 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000032
| Global Round : 20 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000349
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 8, Epoch: 20--------
------------------------------------------
| Global Round : 20 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000356
| Global Round : 20 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.002241
| Global Round : 20 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000097
| Global Round : 20 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000253
| Global Round : 20 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000552
| Global Round : 20 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.008665
| Global Round : 20 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000869
| Global Round : 20 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.003180
| Global Round : 20 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000110
| Global Round : 20 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000033
| Global Round : 20 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.005751
| Global Round : 20 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.004685
| Global Round : 20 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000098
| Global Round : 20 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000568
| Global Round : 20 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000391
| Global Round : 20 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000651
| Global Round : 20 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000016
| Global Round : 20 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.001737
| Global Round : 20 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000309
| Global Round : 20 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.001263
| Global Round : 20 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.006691
| Global Round : 20 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.001418
| Global Round : 20 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.001296
| Global Round : 20 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.004263
| Global Round : 20 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.001570
| Global Round : 20 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000021
| Global Round : 20 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000014
| Global Round : 20 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000130
| Global Round : 20 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000535
| Global Round : 20 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000491
| Global Round : 20 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000167
| Global Round : 20 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000430
| Global Round : 20 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.007827
| Global Round : 20 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000081
| Global Round : 20 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000319
| Global Round : 20 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.001506
| Global Round : 20 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000154
| Global Round : 20 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000151
| Global Round : 20 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000803
| Global Round : 20 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000332
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 6, Epoch: 20--------
------------------------------------------
| Global Round : 20 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.001131
| Global Round : 20 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000317
| Global Round : 20 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000317
| Global Round : 20 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.001138
| Global Round : 20 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.001037
| Global Round : 20 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.001729
| Global Round : 20 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000217
| Global Round : 20 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.001104
| Global Round : 20 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000240
| Global Round : 20 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000165
| Global Round : 20 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000142
| Global Round : 20 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.004102
| Global Round : 20 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000024
| Global Round : 20 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000042
| Global Round : 20 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000091
| Global Round : 20 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000681
| Global Round : 20 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000240
| Global Round : 20 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.001445
| Global Round : 20 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000018
| Global Round : 20 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000096
| Global Round : 20 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.001424
| Global Round : 20 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000074
| Global Round : 20 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.007584
| Global Round : 20 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000032
| Global Round : 20 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000212
| Global Round : 20 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.005460
| Global Round : 20 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000247
| Global Round : 20 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000483
| Global Round : 20 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.002273
| Global Round : 20 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.001707
| Global Round : 20 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000255
| Global Round : 20 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.001367
| Global Round : 20 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.002552
| Global Round : 20 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000356
| Global Round : 20 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000063
| Global Round : 20 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000211
| Global Round : 20 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.002564
| Global Round : 20 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000825
| Global Round : 20 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000044
| Global Round : 20 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.002051
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 2, Epoch: 20--------
------------------------------------------
| Global Round : 20 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000179
| Global Round : 20 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000702
| Global Round : 20 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000117
| Global Round : 20 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000044
| Global Round : 20 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000420
| Global Round : 20 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.010036
| Global Round : 20 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000316
| Global Round : 20 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000033
| Global Round : 20 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.001809
| Global Round : 20 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000506
| Global Round : 20 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000011
| Global Round : 20 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000144
| Global Round : 20 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.001494
| Global Round : 20 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000018
| Global Round : 20 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000299
| Global Round : 20 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000521
| Global Round : 20 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000557
| Global Round : 20 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.006044
| Global Round : 20 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000755
| Global Round : 20 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000493
| Global Round : 20 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000449
| Global Round : 20 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.003706
| Global Round : 20 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000612
| Global Round : 20 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000066
| Global Round : 20 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000811
| Global Round : 20 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.198937
| Global Round : 20 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000429
| Global Round : 20 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.060968
| Global Round : 20 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.011211
| Global Round : 20 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000216
| Global Round : 20 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.005201
| Global Round : 20 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.001415
| Global Round : 20 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000184
| Global Round : 20 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000077
| Global Round : 20 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000102
| Global Round : 20 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000765
| Global Round : 20 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000788
| Global Round : 20 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000098
| Global Round : 20 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000423
| Global Round : 20 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.002827
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
 
Avg Training Stats after 21 global rounds:
Training Loss : nan
Train Accuracy: 9973.14% 

Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 49.61% | Val Loss: nan | Val Acc: 51.70%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.70%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.70%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.70%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.70%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.70%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.70%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.70%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.70%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.70%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.70%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.70%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.70%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.70%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.70%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.70%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.70%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.70%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.70%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.70%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.70%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.70%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.70%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.70%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.70%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.70%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.70%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.70%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.70%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.70%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.70%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.70%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.70%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.70%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.70%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.70%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.70%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.70%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.70%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.70%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.70%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.70%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.70%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.70%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.70%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.70%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.70%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.70%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.70%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 49.93% | Val Loss: nan | Val Acc: 51.70%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 51.70 %
----Attack Model Testing----
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
 70%|   | 21/30 [1:18:12<34:31, 230.12s/it]Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

Selected users for the training: [6 8 2 0 5 9 1 3 7 4]
------------------------------------------
------User: 6, Epoch: 21--------
------------------------------------------
| Global Round : 21 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000023
| Global Round : 21 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000102
| Global Round : 21 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000194
| Global Round : 21 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000265
| Global Round : 21 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.006420
| Global Round : 21 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000254
| Global Round : 21 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.003027
| Global Round : 21 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000056
| Global Round : 21 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000410
| Global Round : 21 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000004
| Global Round : 21 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000438
| Global Round : 21 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000104
| Global Round : 21 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000033
| Global Round : 21 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.001379
| Global Round : 21 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.001561
| Global Round : 21 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000053
| Global Round : 21 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000813
| Global Round : 21 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000267
| Global Round : 21 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000759
| Global Round : 21 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000373
| Global Round : 21 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000020
| Global Round : 21 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.003564
| Global Round : 21 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000023
| Global Round : 21 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000010
| Global Round : 21 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000134
| Global Round : 21 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000105
| Global Round : 21 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.001447
| Global Round : 21 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000696
| Global Round : 21 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000427
| Global Round : 21 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000524
| Global Round : 21 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000251
| Global Round : 21 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000655
| Global Round : 21 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.004185
| Global Round : 21 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.003205
| Global Round : 21 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.014791
| Global Round : 21 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000020
| Global Round : 21 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.001014
| Global Round : 21 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000030
| Global Round : 21 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000146
| Global Round : 21 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000543
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 8, Epoch: 21--------
------------------------------------------
| Global Round : 21 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000082
| Global Round : 21 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.006171
| Global Round : 21 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000023
| Global Round : 21 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.014641
| Global Round : 21 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000037
| Global Round : 21 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000070
| Global Round : 21 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000039
| Global Round : 21 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000153
| Global Round : 21 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000336
| Global Round : 21 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000506
| Global Round : 21 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000575
| Global Round : 21 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000032
| Global Round : 21 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000161
| Global Round : 21 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000129
| Global Round : 21 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000016
| Global Round : 21 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000174
| Global Round : 21 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000415
| Global Round : 21 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000017
| Global Round : 21 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000411
| Global Round : 21 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000076
| Global Round : 21 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000184
| Global Round : 21 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.004797
| Global Round : 21 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000341
| Global Round : 21 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000651
| Global Round : 21 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000079
| Global Round : 21 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000123
| Global Round : 21 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000399
| Global Round : 21 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000198
| Global Round : 21 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.004728
| Global Round : 21 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000396
| Global Round : 21 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000250
| Global Round : 21 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000092
| Global Round : 21 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000066
| Global Round : 21 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000054
| Global Round : 21 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000038
| Global Round : 21 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000007
| Global Round : 21 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000220
| Global Round : 21 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.001450
| Global Round : 21 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000443
| Global Round : 21 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000028
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 2, Epoch: 21--------
------------------------------------------
| Global Round : 21 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.010764
| Global Round : 21 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000046
| Global Round : 21 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000530
| Global Round : 21 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.012275
| Global Round : 21 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000102
| Global Round : 21 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000325
| Global Round : 21 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000687
| Global Round : 21 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000792
| Global Round : 21 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.003780
| Global Round : 21 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000034
| Global Round : 21 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000027
| Global Round : 21 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000051
| Global Round : 21 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000132
| Global Round : 21 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000699
| Global Round : 21 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000045
| Global Round : 21 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000277
| Global Round : 21 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000126
| Global Round : 21 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000494
| Global Round : 21 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000040
| Global Round : 21 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000073
| Global Round : 21 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000162
| Global Round : 21 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000386
| Global Round : 21 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.002866
| Global Round : 21 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000840
| Global Round : 21 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.003911
| Global Round : 21 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.003499
| Global Round : 21 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.003635
| Global Round : 21 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000387
| Global Round : 21 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000013
| Global Round : 21 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000377
| Global Round : 21 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000152
| Global Round : 21 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000021
| Global Round : 21 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000070
| Global Round : 21 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.014910
| Global Round : 21 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.001205
| Global Round : 21 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.001030
| Global Round : 21 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000616
| Global Round : 21 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000042
| Global Round : 21 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000036
| Global Round : 21 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.001925
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 0, Epoch: 21--------
------------------------------------------
| Global Round : 21 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000320
| Global Round : 21 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000510
| Global Round : 21 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.003041
| Global Round : 21 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000092
| Global Round : 21 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000942
| Global Round : 21 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.004641
| Global Round : 21 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000012
| Global Round : 21 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000131
| Global Round : 21 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000180
| Global Round : 21 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.002437
| Global Round : 21 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000008
| Global Round : 21 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000016
| Global Round : 21 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000663
| Global Round : 21 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000153
| Global Round : 21 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000117
| Global Round : 21 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000171
| Global Round : 21 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.026556
| Global Round : 21 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.001224
| Global Round : 21 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.002461
| Global Round : 21 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000085
| Global Round : 21 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000372
| Global Round : 21 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000010
| Global Round : 21 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.003009
| Global Round : 21 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000171
| Global Round : 21 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000076
| Global Round : 21 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000005
| Global Round : 21 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000648
| Global Round : 21 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.002749
| Global Round : 21 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000512
| Global Round : 21 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000134
| Global Round : 21 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.002971
| Global Round : 21 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.004725
| Global Round : 21 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000311
| Global Round : 21 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000072
| Global Round : 21 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000127
| Global Round : 21 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000109
| Global Round : 21 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.031630
| Global Round : 21 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000839
| Global Round : 21 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000010
| Global Round : 21 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.001291
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 49.80% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 48.90 %
----Attack Model Testing----
Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

------------------------------------------
------User: 5, Epoch: 21--------
------------------------------------------
| Global Round : 21 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000674
| Global Round : 21 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000122
| Global Round : 21 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000911
| Global Round : 21 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000946
| Global Round : 21 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.001626
| Global Round : 21 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000172
| Global Round : 21 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.001374
| Global Round : 21 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000189
| Global Round : 21 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000088
| Global Round : 21 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000093
| Global Round : 21 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000134
| Global Round : 21 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.002483
| Global Round : 21 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.001206
| Global Round : 21 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.010562
| Global Round : 21 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000066
| Global Round : 21 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000645
| Global Round : 21 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.001387
| Global Round : 21 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000424
| Global Round : 21 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000009
| Global Round : 21 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000330
| Global Round : 21 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000328
| Global Round : 21 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000203
| Global Round : 21 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000915
| Global Round : 21 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000417
| Global Round : 21 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.002973
| Global Round : 21 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000210
| Global Round : 21 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000070
| Global Round : 21 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.002598
| Global Round : 21 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.015170
| Global Round : 21 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.001015
| Global Round : 21 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000083
| Global Round : 21 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000052
| Global Round : 21 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000135
| Global Round : 21 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000268
| Global Round : 21 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.001359
| Global Round : 21 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000042
| Global Round : 21 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000318
| Global Round : 21 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000064
| Global Round : 21 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000106
| Global Round : 21 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.002044
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 9, Epoch: 21--------
------------------------------------------
| Global Round : 21 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000264
| Global Round : 21 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000032
| Global Round : 21 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000556
| Global Round : 21 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000038
| Global Round : 21 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000156
| Global Round : 21 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000186
| Global Round : 21 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000559
| Global Round : 21 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.001121
| Global Round : 21 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.002801
| Global Round : 21 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000168
| Global Round : 21 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000067
| Global Round : 21 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.001423
| Global Round : 21 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000299
| Global Round : 21 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000061
| Global Round : 21 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000250
| Global Round : 21 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000122
| Global Round : 21 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000642
| Global Round : 21 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.007591
| Global Round : 21 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000034
| Global Round : 21 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.001663
| Global Round : 21 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000090
| Global Round : 21 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.001316
| Global Round : 21 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000056
| Global Round : 21 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000222
| Global Round : 21 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000195
| Global Round : 21 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.003757
| Global Round : 21 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000022
| Global Round : 21 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000127
| Global Round : 21 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.001113
| Global Round : 21 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000377
| Global Round : 21 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.002934
| Global Round : 21 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000052
| Global Round : 21 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000197
| Global Round : 21 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000355
| Global Round : 21 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.001391
| Global Round : 21 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000215
| Global Round : 21 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000851
| Global Round : 21 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.002678
| Global Round : 21 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000049
| Global Round : 21 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000664
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 1, Epoch: 21--------
------------------------------------------
| Global Round : 21 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000099
| Global Round : 21 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000367
| Global Round : 21 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000999
| Global Round : 21 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.001529
| Global Round : 21 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000015
| Global Round : 21 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.003165
| Global Round : 21 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000758
| Global Round : 21 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000040
| Global Round : 21 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.001079
| Global Round : 21 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000834
| Global Round : 21 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000034
| Global Round : 21 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.008972
| Global Round : 21 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.002391
| Global Round : 21 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000100
| Global Round : 21 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.022799
| Global Round : 21 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000537
| Global Round : 21 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.006645
| Global Round : 21 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.009587
| Global Round : 21 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000083
| Global Round : 21 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000126
| Global Round : 21 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.002504
| Global Round : 21 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000045
| Global Round : 21 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000404
| Global Round : 21 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000311
| Global Round : 21 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000028
| Global Round : 21 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000338
| Global Round : 21 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000004
| Global Round : 21 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000066
| Global Round : 21 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000253
| Global Round : 21 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000013
| Global Round : 21 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.003415
| Global Round : 21 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000185
| Global Round : 21 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000180
| Global Round : 21 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000434
| Global Round : 21 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000067
| Global Round : 21 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000030
| Global Round : 21 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000646
| Global Round : 21 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000041
| Global Round : 21 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.001250
| Global Round : 21 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000340
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 3, Epoch: 21--------
------------------------------------------
| Global Round : 21 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000101
| Global Round : 21 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000026
| Global Round : 21 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000047
| Global Round : 21 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000085
| Global Round : 21 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000329
| Global Round : 21 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000490
| Global Round : 21 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000310
| Global Round : 21 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000348
| Global Round : 21 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000018
| Global Round : 21 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000463
| Global Round : 21 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000057
| Global Round : 21 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000014
| Global Round : 21 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000017
| Global Round : 21 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000920
| Global Round : 21 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000061
| Global Round : 21 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000130
| Global Round : 21 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.003274
| Global Round : 21 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000852
| Global Round : 21 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000590
| Global Round : 21 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.005552
| Global Round : 21 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.045192
| Global Round : 21 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000420
| Global Round : 21 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000037
| Global Round : 21 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000200
| Global Round : 21 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000033
| Global Round : 21 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000181
| Global Round : 21 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000089
| Global Round : 21 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000392
| Global Round : 21 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000484
| Global Round : 21 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.006614
| Global Round : 21 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.008752
| Global Round : 21 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.002428
| Global Round : 21 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000472
| Global Round : 21 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000809
| Global Round : 21 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.002217
| Global Round : 21 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000491
| Global Round : 21 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000034
| Global Round : 21 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.038948
| Global Round : 21 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000437
| Global Round : 21 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000306
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 7, Epoch: 21--------
------------------------------------------
| Global Round : 21 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.002564
| Global Round : 21 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000035
| Global Round : 21 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.014086
| Global Round : 21 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000452
| Global Round : 21 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000222
| Global Round : 21 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000030
| Global Round : 21 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000084
| Global Round : 21 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000134
| Global Round : 21 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000087
| Global Round : 21 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000898
| Global Round : 21 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000096
| Global Round : 21 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000646
| Global Round : 21 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000650
| Global Round : 21 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000051
| Global Round : 21 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000105
| Global Round : 21 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.004115
| Global Round : 21 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000427
| Global Round : 21 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000010
| Global Round : 21 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000921
| Global Round : 21 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000013
| Global Round : 21 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000421
| Global Round : 21 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000524
| Global Round : 21 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.001302
| Global Round : 21 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000135
| Global Round : 21 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000099
| Global Round : 21 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.001546
| Global Round : 21 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.011010
| Global Round : 21 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000154
| Global Round : 21 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000326
| Global Round : 21 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000155
| Global Round : 21 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000088
| Global Round : 21 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.003697
| Global Round : 21 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.003197
| Global Round : 21 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.003199
| Global Round : 21 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000015
| Global Round : 21 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000581
| Global Round : 21 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000081
| Global Round : 21 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.013847
| Global Round : 21 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000299
| Global Round : 21 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000052
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 4, Epoch: 21--------
------------------------------------------
| Global Round : 21 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.008026
| Global Round : 21 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000222
| Global Round : 21 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.001772
| Global Round : 21 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000224
| Global Round : 21 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000465
| Global Round : 21 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000037
| Global Round : 21 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000459
| Global Round : 21 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000081
| Global Round : 21 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.003359
| Global Round : 21 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.001961
| Global Round : 21 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000183
| Global Round : 21 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000006
| Global Round : 21 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.004795
| Global Round : 21 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000382
| Global Round : 21 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000494
| Global Round : 21 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000097
| Global Round : 21 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.030399
| Global Round : 21 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000100
| Global Round : 21 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.001282
| Global Round : 21 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000066
| Global Round : 21 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000231
| Global Round : 21 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000867
| Global Round : 21 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.001362
| Global Round : 21 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.003299
| Global Round : 21 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.001393
| Global Round : 21 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000043
| Global Round : 21 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000343
| Global Round : 21 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000172
| Global Round : 21 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.002729
| Global Round : 21 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000087
| Global Round : 21 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000097
| Global Round : 21 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000620
| Global Round : 21 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.018150
| Global Round : 21 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000728
| Global Round : 21 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.002720
| Global Round : 21 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000736
| Global Round : 21 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000039
| Global Round : 21 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000176
| Global Round : 21 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000446
| Global Round : 21 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000938
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
 
Avg Training Stats after 22 global rounds:
Training Loss : nan
Train Accuracy: 9974.36% 

Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 49.87% | Val Loss: nan | Val Acc: 52.90%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.90%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.90%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.90%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.90%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.90%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.90%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.90%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.90%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.90%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.90%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.90%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.90%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.90%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.90%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.90%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.90%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.90%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.90%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.90%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.90%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.90%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.90%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.90%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.90%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.90%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.90%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.90%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.90%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.90%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.90%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.90%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.90%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.90%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.90%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.90%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.90%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.90%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.90%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.90%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.90%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.90%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.90%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.90%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.90%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.90%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.90%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.90%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.90%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.90%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 52.90 %
----Attack Model Testing----
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
 73%|  | 22/30 [1:22:03<30:42, 230.32s/it]Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

Selected users for the training: [9 4 6 0 8 2 1 7 3 5]
------------------------------------------
------User: 9, Epoch: 22--------
------------------------------------------
| Global Round : 22 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000243
| Global Round : 22 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000262
| Global Round : 22 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000027
| Global Round : 22 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000030
| Global Round : 22 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000312
| Global Round : 22 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000095
| Global Round : 22 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000049
| Global Round : 22 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000258
| Global Round : 22 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000056
| Global Round : 22 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000852
| Global Round : 22 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000062
| Global Round : 22 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000130
| Global Round : 22 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000301
| Global Round : 22 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000218
| Global Round : 22 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000055
| Global Round : 22 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.001552
| Global Round : 22 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.003283
| Global Round : 22 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000180
| Global Round : 22 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000523
| Global Round : 22 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000021
| Global Round : 22 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000108
| Global Round : 22 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000128
| Global Round : 22 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000127
| Global Round : 22 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000035
| Global Round : 22 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.001446
| Global Round : 22 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.001334
| Global Round : 22 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000009
| Global Round : 22 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000131
| Global Round : 22 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000032
| Global Round : 22 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000083
| Global Round : 22 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000164
| Global Round : 22 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000032
| Global Round : 22 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000071
| Global Round : 22 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000199
| Global Round : 22 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.005325
| Global Round : 22 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000125
| Global Round : 22 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000008
| Global Round : 22 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000913
| Global Round : 22 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000184
| Global Round : 22 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000039
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 4, Epoch: 22--------
------------------------------------------
| Global Round : 22 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.001789
| Global Round : 22 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000175
| Global Round : 22 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000653
| Global Round : 22 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.001425
| Global Round : 22 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.002071
| Global Round : 22 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000154
| Global Round : 22 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000100
| Global Round : 22 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000187
| Global Round : 22 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000023
| Global Round : 22 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000092
| Global Round : 22 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000787
| Global Round : 22 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000054
| Global Round : 22 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000151
| Global Round : 22 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000085
| Global Round : 22 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000982
| Global Round : 22 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000205
| Global Round : 22 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.002761
| Global Round : 22 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000218
| Global Round : 22 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000083
| Global Round : 22 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000129
| Global Round : 22 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000064
| Global Round : 22 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.002150
| Global Round : 22 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000003
| Global Round : 22 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.001118
| Global Round : 22 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000009
| Global Round : 22 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000334
| Global Round : 22 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000426
| Global Round : 22 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000796
| Global Round : 22 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000237
| Global Round : 22 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000007
| Global Round : 22 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.008944
| Global Round : 22 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000109
| Global Round : 22 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000242
| Global Round : 22 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000030
| Global Round : 22 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000100
| Global Round : 22 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000542
| Global Round : 22 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000023
| Global Round : 22 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000204
| Global Round : 22 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.003161
| Global Round : 22 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.002893
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 6, Epoch: 22--------
------------------------------------------
| Global Round : 22 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.001851
| Global Round : 22 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000121
| Global Round : 22 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000023
| Global Round : 22 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.001510
| Global Round : 22 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000097
| Global Round : 22 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000533
| Global Round : 22 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000012
| Global Round : 22 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000057
| Global Round : 22 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000682
| Global Round : 22 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.003288
| Global Round : 22 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000030
| Global Round : 22 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000685
| Global Round : 22 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.024822
| Global Round : 22 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000295
| Global Round : 22 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000164
| Global Round : 22 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.003139
| Global Round : 22 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000007
| Global Round : 22 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000212
| Global Round : 22 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.007819
| Global Round : 22 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.005325
| Global Round : 22 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.026676
| Global Round : 22 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.002484
| Global Round : 22 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000023
| Global Round : 22 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.030271
| Global Round : 22 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000025
| Global Round : 22 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.001130
| Global Round : 22 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000077
| Global Round : 22 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.001179
| Global Round : 22 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000143
| Global Round : 22 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.001591
| Global Round : 22 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000023
| Global Round : 22 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000379
| Global Round : 22 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000032
| Global Round : 22 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000269
| Global Round : 22 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.004054
| Global Round : 22 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000271
| Global Round : 22 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000030
| Global Round : 22 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000486
| Global Round : 22 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000070
| Global Round : 22 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000087
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 0, Epoch: 22--------
------------------------------------------
| Global Round : 22 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000144
| Global Round : 22 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.009780
| Global Round : 22 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000192
| Global Round : 22 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000313
| Global Round : 22 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000930
| Global Round : 22 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000015
| Global Round : 22 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.001094
| Global Round : 22 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.001355
| Global Round : 22 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.005874
| Global Round : 22 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.005367
| Global Round : 22 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000043
| Global Round : 22 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000577
| Global Round : 22 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.004934
| Global Round : 22 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000276
| Global Round : 22 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.003607
| Global Round : 22 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.008987
| Global Round : 22 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000109
| Global Round : 22 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.001993
| Global Round : 22 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.007192
| Global Round : 22 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.001232
| Global Round : 22 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000488
| Global Round : 22 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000322
| Global Round : 22 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000805
| Global Round : 22 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000164
| Global Round : 22 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000236
| Global Round : 22 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.001410
| Global Round : 22 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000449
| Global Round : 22 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000585
| Global Round : 22 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000325
| Global Round : 22 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000270
| Global Round : 22 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000180
| Global Round : 22 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000933
| Global Round : 22 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000883
| Global Round : 22 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000057
| Global Round : 22 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.001121
| Global Round : 22 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.011880
| Global Round : 22 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.001302
| Global Round : 22 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000208
| Global Round : 22 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000388
| Global Round : 22 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000583
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 50.01% | Val Loss: nan | Val Acc: 49.80%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 49.80 %
----Attack Model Testing----
Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

------------------------------------------
------User: 8, Epoch: 22--------
------------------------------------------
| Global Round : 22 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000015
| Global Round : 22 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000223
| Global Round : 22 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000042
| Global Round : 22 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000860
| Global Round : 22 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000302
| Global Round : 22 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000216
| Global Round : 22 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.001983
| Global Round : 22 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000157
| Global Round : 22 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000286
| Global Round : 22 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000259
| Global Round : 22 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000129
| Global Round : 22 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000017
| Global Round : 22 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000296
| Global Round : 22 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.001506
| Global Round : 22 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.001163
| Global Round : 22 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000837
| Global Round : 22 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000105
| Global Round : 22 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000004
| Global Round : 22 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000359
| Global Round : 22 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000042
| Global Round : 22 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000120
| Global Round : 22 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000087
| Global Round : 22 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000602
| Global Round : 22 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000498
| Global Round : 22 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.001021
| Global Round : 22 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000213
| Global Round : 22 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.005415
| Global Round : 22 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000113
| Global Round : 22 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.019437
| Global Round : 22 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000808
| Global Round : 22 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000849
| Global Round : 22 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.001275
| Global Round : 22 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000388
| Global Round : 22 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000407
| Global Round : 22 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000193
| Global Round : 22 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.001813
| Global Round : 22 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000672
| Global Round : 22 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000218
| Global Round : 22 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.001443
| Global Round : 22 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000202
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 2, Epoch: 22--------
------------------------------------------
| Global Round : 22 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.001921
| Global Round : 22 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000907
| Global Round : 22 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000213
| Global Round : 22 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000095
| Global Round : 22 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000012
| Global Round : 22 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000680
| Global Round : 22 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.006281
| Global Round : 22 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000205
| Global Round : 22 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000188
| Global Round : 22 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000122
| Global Round : 22 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000555
| Global Round : 22 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.001696
| Global Round : 22 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000023
| Global Round : 22 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.001301
| Global Round : 22 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000966
| Global Round : 22 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000080
| Global Round : 22 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000047
| Global Round : 22 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.004207
| Global Round : 22 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000800
| Global Round : 22 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.001306
| Global Round : 22 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000036
| Global Round : 22 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.003279
| Global Round : 22 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000158
| Global Round : 22 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000548
| Global Round : 22 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000292
| Global Round : 22 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.001385
| Global Round : 22 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000210
| Global Round : 22 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.002104
| Global Round : 22 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.002354
| Global Round : 22 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000391
| Global Round : 22 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.001249
| Global Round : 22 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.002995
| Global Round : 22 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000023
| Global Round : 22 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000955
| Global Round : 22 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000140
| Global Round : 22 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000016
| Global Round : 22 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000028
| Global Round : 22 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000291
| Global Round : 22 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000184
| Global Round : 22 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000547
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 1, Epoch: 22--------
------------------------------------------
| Global Round : 22 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000135
| Global Round : 22 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000067
| Global Round : 22 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000173
| Global Round : 22 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.002132
| Global Round : 22 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.005571
| Global Round : 22 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000633
| Global Round : 22 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.010382
| Global Round : 22 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000211
| Global Round : 22 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000214
| Global Round : 22 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000536
| Global Round : 22 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000618
| Global Round : 22 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000064
| Global Round : 22 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.001683
| Global Round : 22 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.001493
| Global Round : 22 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000071
| Global Round : 22 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000273
| Global Round : 22 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000317
| Global Round : 22 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000022
| Global Round : 22 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000037
| Global Round : 22 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000029
| Global Round : 22 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000063
| Global Round : 22 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000135
| Global Round : 22 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000192
| Global Round : 22 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000073
| Global Round : 22 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.002206
| Global Round : 22 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.003401
| Global Round : 22 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.010456
| Global Round : 22 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.003957
| Global Round : 22 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000490
| Global Round : 22 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000124
| Global Round : 22 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000023
| Global Round : 22 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000069
| Global Round : 22 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.001788
| Global Round : 22 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.001171
| Global Round : 22 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.001135
| Global Round : 22 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000392
| Global Round : 22 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.001613
| Global Round : 22 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000804
| Global Round : 22 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000666
| Global Round : 22 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000247
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 7, Epoch: 22--------
------------------------------------------
| Global Round : 22 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000071
| Global Round : 22 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000518
| Global Round : 22 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000083
| Global Round : 22 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000121
| Global Round : 22 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000552
| Global Round : 22 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000324
| Global Round : 22 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000018
| Global Round : 22 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.001660
| Global Round : 22 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000068
| Global Round : 22 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000088
| Global Round : 22 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.001030
| Global Round : 22 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000862
| Global Round : 22 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.001133
| Global Round : 22 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000782
| Global Round : 22 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000870
| Global Round : 22 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000173
| Global Round : 22 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000054
| Global Round : 22 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.002188
| Global Round : 22 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000430
| Global Round : 22 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000214
| Global Round : 22 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000170
| Global Round : 22 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.002092
| Global Round : 22 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000087
| Global Round : 22 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000128
| Global Round : 22 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000067
| Global Round : 22 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000007
| Global Round : 22 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.003151
| Global Round : 22 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000296
| Global Round : 22 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.005024
| Global Round : 22 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000122
| Global Round : 22 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.002134
| Global Round : 22 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.001541
| Global Round : 22 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000132
| Global Round : 22 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.003383
| Global Round : 22 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000023
| Global Round : 22 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000884
| Global Round : 22 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.004931
| Global Round : 22 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000256
| Global Round : 22 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.007593
| Global Round : 22 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.004827
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 3, Epoch: 22--------
------------------------------------------
| Global Round : 22 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000078
| Global Round : 22 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000619
| Global Round : 22 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.001542
| Global Round : 22 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000029
| Global Round : 22 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000217
| Global Round : 22 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.012224
| Global Round : 22 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000110
| Global Round : 22 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.002352
| Global Round : 22 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000129
| Global Round : 22 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000082
| Global Round : 22 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.001418
| Global Round : 22 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.008502
| Global Round : 22 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000210
| Global Round : 22 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000120
| Global Round : 22 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000180
| Global Round : 22 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000126
| Global Round : 22 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.002581
| Global Round : 22 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.003039
| Global Round : 22 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000918
| Global Round : 22 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.001082
| Global Round : 22 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000946
| Global Round : 22 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000157
| Global Round : 22 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.002292
| Global Round : 22 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000217
| Global Round : 22 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.003750
| Global Round : 22 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000153
| Global Round : 22 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000827
| Global Round : 22 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000563
| Global Round : 22 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000214
| Global Round : 22 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000135
| Global Round : 22 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000257
| Global Round : 22 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000190
| Global Round : 22 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000213
| Global Round : 22 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000099
| Global Round : 22 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.002367
| Global Round : 22 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.001896
| Global Round : 22 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000016
| Global Round : 22 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.001231
| Global Round : 22 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000064
| Global Round : 22 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000734
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 5, Epoch: 22--------
------------------------------------------
| Global Round : 22 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000205
| Global Round : 22 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.001304
| Global Round : 22 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000196
| Global Round : 22 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000877
| Global Round : 22 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000019
| Global Round : 22 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.024457
| Global Round : 22 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000187
| Global Round : 22 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.001402
| Global Round : 22 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000569
| Global Round : 22 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.009196
| Global Round : 22 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000207
| Global Round : 22 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000193
| Global Round : 22 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000331
| Global Round : 22 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000137
| Global Round : 22 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000181
| Global Round : 22 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000414
| Global Round : 22 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000361
| Global Round : 22 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000249
| Global Round : 22 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000016
| Global Round : 22 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000017
| Global Round : 22 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000023
| Global Round : 22 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000053
| Global Round : 22 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000359
| Global Round : 22 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000566
| Global Round : 22 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000022
| Global Round : 22 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000037
| Global Round : 22 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000281
| Global Round : 22 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000047
| Global Round : 22 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000907
| Global Round : 22 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000045
| Global Round : 22 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000140
| Global Round : 22 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000992
| Global Round : 22 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000063
| Global Round : 22 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.001942
| Global Round : 22 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000417
| Global Round : 22 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.001548
| Global Round : 22 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000151
| Global Round : 22 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000593
| Global Round : 22 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000589
| Global Round : 22 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.007473
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
 
Avg Training Stats after 23 global rounds:
Training Loss : nan
Train Accuracy: 9975.48% 

Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 49.40 %
----Attack Model Testing----
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
 77%|  | 23/30 [1:25:47<26:38, 228.38s/it]Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

Selected users for the training: [6 5 4 2 7 9 0 1 8 3]
------------------------------------------
------User: 6, Epoch: 23--------
------------------------------------------
| Global Round : 23 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000185
| Global Round : 23 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000432
| Global Round : 23 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000316
| Global Round : 23 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000383
| Global Round : 23 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.002846
| Global Round : 23 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.001343
| Global Round : 23 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000094
| Global Round : 23 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000187
| Global Round : 23 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000526
| Global Round : 23 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000461
| Global Round : 23 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000034
| Global Round : 23 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000132
| Global Round : 23 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000155
| Global Round : 23 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000196
| Global Round : 23 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000039
| Global Round : 23 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000026
| Global Round : 23 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000142
| Global Round : 23 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000144
| Global Round : 23 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000042
| Global Round : 23 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000227
| Global Round : 23 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.001357
| Global Round : 23 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000704
| Global Round : 23 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000608
| Global Round : 23 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000230
| Global Round : 23 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000092
| Global Round : 23 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.001530
| Global Round : 23 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000074
| Global Round : 23 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000015
| Global Round : 23 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000613
| Global Round : 23 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.001123
| Global Round : 23 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000366
| Global Round : 23 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000834
| Global Round : 23 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.001186
| Global Round : 23 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000350
| Global Round : 23 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000146
| Global Round : 23 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.001376
| Global Round : 23 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000358
| Global Round : 23 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000892
| Global Round : 23 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.003994
| Global Round : 23 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.001013
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 5, Epoch: 23--------
------------------------------------------
| Global Round : 23 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000007
| Global Round : 23 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000727
| Global Round : 23 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000057
| Global Round : 23 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.001356
| Global Round : 23 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000752
| Global Round : 23 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000411
| Global Round : 23 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000411
| Global Round : 23 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.002320
| Global Round : 23 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.001493
| Global Round : 23 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000423
| Global Round : 23 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000111
| Global Round : 23 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000553
| Global Round : 23 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000539
| Global Round : 23 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000194
| Global Round : 23 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000090
| Global Round : 23 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000022
| Global Round : 23 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000191
| Global Round : 23 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.001449
| Global Round : 23 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000027
| Global Round : 23 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000020
| Global Round : 23 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000263
| Global Round : 23 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000196
| Global Round : 23 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000299
| Global Round : 23 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000264
| Global Round : 23 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000050
| Global Round : 23 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000188
| Global Round : 23 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000061
| Global Round : 23 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000168
| Global Round : 23 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000983
| Global Round : 23 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.001192
| Global Round : 23 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000009
| Global Round : 23 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000357
| Global Round : 23 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.001250
| Global Round : 23 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000767
| Global Round : 23 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.001966
| Global Round : 23 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000466
| Global Round : 23 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000961
| Global Round : 23 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000686
| Global Round : 23 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000053
| Global Round : 23 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.002390
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 4, Epoch: 23--------
------------------------------------------
| Global Round : 23 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.003387
| Global Round : 23 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000016
| Global Round : 23 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.001450
| Global Round : 23 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000158
| Global Round : 23 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.002265
| Global Round : 23 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000112
| Global Round : 23 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000010
| Global Round : 23 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000418
| Global Round : 23 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000159
| Global Round : 23 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000375
| Global Round : 23 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000028
| Global Round : 23 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000221
| Global Round : 23 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.002019
| Global Round : 23 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000024
| Global Round : 23 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000087
| Global Round : 23 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000011
| Global Round : 23 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000054
| Global Round : 23 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000311
| Global Round : 23 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000573
| Global Round : 23 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000593
| Global Round : 23 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000039
| Global Round : 23 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000072
| Global Round : 23 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000690
| Global Round : 23 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000022
| Global Round : 23 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000212
| Global Round : 23 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000037
| Global Round : 23 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000021
| Global Round : 23 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000192
| Global Round : 23 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000431
| Global Round : 23 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000463
| Global Round : 23 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000010
| Global Round : 23 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000815
| Global Round : 23 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000115
| Global Round : 23 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000181
| Global Round : 23 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000192
| Global Round : 23 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000106
| Global Round : 23 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000377
| Global Round : 23 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.001591
| Global Round : 23 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000036
| Global Round : 23 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000162
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 2, Epoch: 23--------
------------------------------------------
| Global Round : 23 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000219
| Global Round : 23 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.001005
| Global Round : 23 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000113
| Global Round : 23 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000033
| Global Round : 23 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000093
| Global Round : 23 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.002636
| Global Round : 23 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000281
| Global Round : 23 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000177
| Global Round : 23 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000202
| Global Round : 23 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000280
| Global Round : 23 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000114
| Global Round : 23 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.018600
| Global Round : 23 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.004425
| Global Round : 23 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.002630
| Global Round : 23 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000242
| Global Round : 23 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000105
| Global Round : 23 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.009991
| Global Round : 23 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000746
| Global Round : 23 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.001339
| Global Round : 23 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.001167
| Global Round : 23 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000174
| Global Round : 23 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.002556
| Global Round : 23 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.022139
| Global Round : 23 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000693
| Global Round : 23 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.001463
| Global Round : 23 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000273
| Global Round : 23 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000481
| Global Round : 23 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000872
| Global Round : 23 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000028
| Global Round : 23 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000800
| Global Round : 23 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000198
| Global Round : 23 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000046
| Global Round : 23 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000104
| Global Round : 23 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000105
| Global Round : 23 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000169
| Global Round : 23 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000156
| Global Round : 23 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000009
| Global Round : 23 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000416
| Global Round : 23 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.002445
| Global Round : 23 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.021407
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 7, Epoch: 23--------
------------------------------------------
| Global Round : 23 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000052
| Global Round : 23 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000337
| Global Round : 23 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000075
| Global Round : 23 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000033
| Global Round : 23 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000428
| Global Round : 23 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000054
| Global Round : 23 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000113
| Global Round : 23 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000756
| Global Round : 23 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.001266
| Global Round : 23 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.002456
| Global Round : 23 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000123
| Global Round : 23 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000033
| Global Round : 23 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000207
| Global Round : 23 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000028
| Global Round : 23 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000495
| Global Round : 23 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000070
| Global Round : 23 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000057
| Global Round : 23 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000167
| Global Round : 23 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.009470
| Global Round : 23 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000200
| Global Round : 23 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000031
| Global Round : 23 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.001060
| Global Round : 23 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000006
| Global Round : 23 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.001183
| Global Round : 23 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000157
| Global Round : 23 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000031
| Global Round : 23 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000077
| Global Round : 23 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000992
| Global Round : 23 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000057
| Global Round : 23 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000021
| Global Round : 23 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.001425
| Global Round : 23 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000652
| Global Round : 23 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.002780
| Global Round : 23 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000275
| Global Round : 23 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.003191
| Global Round : 23 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.004418
| Global Round : 23 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.001405
| Global Round : 23 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000159
| Global Round : 23 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000204
| Global Round : 23 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000048
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 9, Epoch: 23--------
------------------------------------------
| Global Round : 23 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000347
| Global Round : 23 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000022
| Global Round : 23 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.003434
| Global Round : 23 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.002206
| Global Round : 23 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000126
| Global Round : 23 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000073
| Global Round : 23 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000484
| Global Round : 23 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000174
| Global Round : 23 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000074
| Global Round : 23 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000737
| Global Round : 23 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.004070
| Global Round : 23 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.002166
| Global Round : 23 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000893
| Global Round : 23 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000158
| Global Round : 23 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000619
| Global Round : 23 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000034
| Global Round : 23 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000960
| Global Round : 23 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.002912
| Global Round : 23 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.030658
| Global Round : 23 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000006
| Global Round : 23 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.006181
| Global Round : 23 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.003260
| Global Round : 23 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000627
| Global Round : 23 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.002333
| Global Round : 23 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.001064
| Global Round : 23 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000458
| Global Round : 23 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.003509
| Global Round : 23 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.003072
| Global Round : 23 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000068
| Global Round : 23 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.034220
| Global Round : 23 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000360
| Global Round : 23 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000618
| Global Round : 23 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.004975
| Global Round : 23 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000010
| Global Round : 23 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000131
| Global Round : 23 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000061
| Global Round : 23 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000163
| Global Round : 23 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000004
| Global Round : 23 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000390
| Global Round : 23 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000330
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 0, Epoch: 23--------
------------------------------------------
| Global Round : 23 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000121
| Global Round : 23 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.001189
| Global Round : 23 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000320
| Global Round : 23 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000123
| Global Round : 23 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000237
| Global Round : 23 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.002673
| Global Round : 23 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000033
| Global Round : 23 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000498
| Global Round : 23 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.004723
| Global Round : 23 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.009723
| Global Round : 23 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.004475
| Global Round : 23 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.005431
| Global Round : 23 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000991
| Global Round : 23 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000892
| Global Round : 23 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.002824
| Global Round : 23 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000081
| Global Round : 23 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.005002
| Global Round : 23 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000923
| Global Round : 23 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000298
| Global Round : 23 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.001515
| Global Round : 23 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000090
| Global Round : 23 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000479
| Global Round : 23 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000105
| Global Round : 23 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.002911
| Global Round : 23 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.002164
| Global Round : 23 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000232
| Global Round : 23 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000234
| Global Round : 23 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.001410
| Global Round : 23 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.003671
| Global Round : 23 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000097
| Global Round : 23 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000040
| Global Round : 23 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000704
| Global Round : 23 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000238
| Global Round : 23 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000119
| Global Round : 23 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.013553
| Global Round : 23 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.004348
| Global Round : 23 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000167
| Global Round : 23 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000027
| Global Round : 23 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000358
| Global Round : 23 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000973
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 49.82% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 49.00 %
----Attack Model Testing----
Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

------------------------------------------
------User: 1, Epoch: 23--------
------------------------------------------
| Global Round : 23 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000162
| Global Round : 23 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000076
| Global Round : 23 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000108
| Global Round : 23 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.002171
| Global Round : 23 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000876
| Global Round : 23 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000025
| Global Round : 23 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.001363
| Global Round : 23 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000074
| Global Round : 23 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.001083
| Global Round : 23 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000637
| Global Round : 23 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000512
| Global Round : 23 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000386
| Global Round : 23 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000651
| Global Round : 23 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000117
| Global Round : 23 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000189
| Global Round : 23 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.010364
| Global Round : 23 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000191
| Global Round : 23 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.006998
| Global Round : 23 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000059
| Global Round : 23 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000109
| Global Round : 23 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000023
| Global Round : 23 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000047
| Global Round : 23 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000080
| Global Round : 23 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.001116
| Global Round : 23 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.047797
| Global Round : 23 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000313
| Global Round : 23 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000696
| Global Round : 23 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000013
| Global Round : 23 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000319
| Global Round : 23 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000177
| Global Round : 23 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000368
| Global Round : 23 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000025
| Global Round : 23 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000186
| Global Round : 23 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.001359
| Global Round : 23 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.003137
| Global Round : 23 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.001053
| Global Round : 23 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.001512
| Global Round : 23 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000137
| Global Round : 23 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.011390
| Global Round : 23 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000750
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 8, Epoch: 23--------
------------------------------------------
| Global Round : 23 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000031
| Global Round : 23 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000062
| Global Round : 23 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000082
| Global Round : 23 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000039
| Global Round : 23 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.003298
| Global Round : 23 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.003128
| Global Round : 23 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000014
| Global Round : 23 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000044
| Global Round : 23 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.002853
| Global Round : 23 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.028120
| Global Round : 23 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000996
| Global Round : 23 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000095
| Global Round : 23 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000046
| Global Round : 23 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000389
| Global Round : 23 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.005497
| Global Round : 23 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000220
| Global Round : 23 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000044
| Global Round : 23 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.003504
| Global Round : 23 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000011
| Global Round : 23 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000034
| Global Round : 23 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000823
| Global Round : 23 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000284
| Global Round : 23 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000008
| Global Round : 23 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000187
| Global Round : 23 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000029
| Global Round : 23 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.003444
| Global Round : 23 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.009238
| Global Round : 23 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000665
| Global Round : 23 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000161
| Global Round : 23 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000025
| Global Round : 23 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.001031
| Global Round : 23 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000734
| Global Round : 23 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000129
| Global Round : 23 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000271
| Global Round : 23 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000195
| Global Round : 23 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.002384
| Global Round : 23 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000045
| Global Round : 23 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.001772
| Global Round : 23 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000044
| Global Round : 23 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000715
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 3, Epoch: 23--------
------------------------------------------
| Global Round : 23 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000203
| Global Round : 23 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.001929
| Global Round : 23 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000087
| Global Round : 23 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000089
| Global Round : 23 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000062
| Global Round : 23 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000219
| Global Round : 23 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000010
| Global Round : 23 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000265
| Global Round : 23 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000451
| Global Round : 23 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000019
| Global Round : 23 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000238
| Global Round : 23 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000032
| Global Round : 23 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000247
| Global Round : 23 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.007139
| Global Round : 23 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000302
| Global Round : 23 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000159
| Global Round : 23 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000996
| Global Round : 23 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000077
| Global Round : 23 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.002931
| Global Round : 23 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000586
| Global Round : 23 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000753
| Global Round : 23 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000010
| Global Round : 23 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000051
| Global Round : 23 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000499
| Global Round : 23 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000555
| Global Round : 23 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000441
| Global Round : 23 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.002936
| Global Round : 23 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000596
| Global Round : 23 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000452
| Global Round : 23 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.003501
| Global Round : 23 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000242
| Global Round : 23 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000361
| Global Round : 23 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000077
| Global Round : 23 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000131
| Global Round : 23 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000625
| Global Round : 23 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000010
| Global Round : 23 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000655
| Global Round : 23 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000461
| Global Round : 23 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.001585
| Global Round : 23 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000652
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
 
Avg Training Stats after 24 global rounds:
Training Loss : nan
Train Accuracy: 9976.42% 

Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 50.06% | Val Loss: nan | Val Acc: 48.50%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 48.50 %
----Attack Model Testing----
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
 80%|  | 24/30 [1:29:20<22:22, 223.73s/it]Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

Selected users for the training: [0 2 5 1 9 4 8 6 7 3]
------------------------------------------
------User: 0, Epoch: 24--------
------------------------------------------
| Global Round : 24 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000475
| Global Round : 24 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000309
| Global Round : 24 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000067
| Global Round : 24 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000303
| Global Round : 24 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000651
| Global Round : 24 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000089
| Global Round : 24 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000338
| Global Round : 24 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.001000
| Global Round : 24 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000705
| Global Round : 24 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000012
| Global Round : 24 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000013
| Global Round : 24 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.002197
| Global Round : 24 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000488
| Global Round : 24 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000049
| Global Round : 24 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.002285
| Global Round : 24 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000313
| Global Round : 24 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.001599
| Global Round : 24 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000350
| Global Round : 24 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000006
| Global Round : 24 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000068
| Global Round : 24 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.001179
| Global Round : 24 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.001093
| Global Round : 24 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000139
| Global Round : 24 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.001922
| Global Round : 24 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.004270
| Global Round : 24 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.004336
| Global Round : 24 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.003992
| Global Round : 24 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000238
| Global Round : 24 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000047
| Global Round : 24 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000341
| Global Round : 24 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000055
| Global Round : 24 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000953
| Global Round : 24 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000137
| Global Round : 24 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000637
| Global Round : 24 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000969
| Global Round : 24 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000866
| Global Round : 24 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000591
| Global Round : 24 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000399
| Global Round : 24 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000216
| Global Round : 24 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000949
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 49.71% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 49.96% | Val Loss: nan | Val Acc: 50.90%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 50.90 %
----Attack Model Testing----
Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

------------------------------------------
------User: 2, Epoch: 24--------
------------------------------------------
| Global Round : 24 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000309
| Global Round : 24 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000029
| Global Round : 24 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000438
| Global Round : 24 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000033
| Global Round : 24 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000174
| Global Round : 24 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.012372
| Global Round : 24 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000762
| Global Round : 24 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000104
| Global Round : 24 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.001984
| Global Round : 24 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000079
| Global Round : 24 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.006280
| Global Round : 24 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000130
| Global Round : 24 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000334
| Global Round : 24 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.010937
| Global Round : 24 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.002708
| Global Round : 24 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.002171
| Global Round : 24 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000996
| Global Round : 24 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.001973
| Global Round : 24 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000178
| Global Round : 24 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000075
| Global Round : 24 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.002254
| Global Round : 24 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000898
| Global Round : 24 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000033
| Global Round : 24 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000074
| Global Round : 24 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000479
| Global Round : 24 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000511
| Global Round : 24 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000226
| Global Round : 24 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000061
| Global Round : 24 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000022
| Global Round : 24 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.001092
| Global Round : 24 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000161
| Global Round : 24 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000123
| Global Round : 24 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000108
| Global Round : 24 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.012859
| Global Round : 24 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000153
| Global Round : 24 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.001207
| Global Round : 24 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000167
| Global Round : 24 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.011917
| Global Round : 24 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000153
| Global Round : 24 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000118
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 5, Epoch: 24--------
------------------------------------------
| Global Round : 24 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000106
| Global Round : 24 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000015
| Global Round : 24 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000043
| Global Round : 24 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000066
| Global Round : 24 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000083
| Global Round : 24 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.001527
| Global Round : 24 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000073
| Global Round : 24 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000379
| Global Round : 24 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000013
| Global Round : 24 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000918
| Global Round : 24 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000020
| Global Round : 24 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000438
| Global Round : 24 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.001088
| Global Round : 24 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000681
| Global Round : 24 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.026357
| Global Round : 24 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000276
| Global Round : 24 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000014
| Global Round : 24 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000594
| Global Round : 24 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000489
| Global Round : 24 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000260
| Global Round : 24 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000070
| Global Round : 24 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000592
| Global Round : 24 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000081
| Global Round : 24 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000106
| Global Round : 24 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000269
| Global Round : 24 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000352
| Global Round : 24 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.003867
| Global Round : 24 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000011
| Global Round : 24 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000240
| Global Round : 24 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000043
| Global Round : 24 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.004045
| Global Round : 24 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000196
| Global Round : 24 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000871
| Global Round : 24 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.163142
| Global Round : 24 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000637
| Global Round : 24 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000628
| Global Round : 24 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000129
| Global Round : 24 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000020
| Global Round : 24 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.024054
| Global Round : 24 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.017610
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 1, Epoch: 24--------
------------------------------------------
| Global Round : 24 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000043
| Global Round : 24 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000012
| Global Round : 24 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.001073
| Global Round : 24 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000929
| Global Round : 24 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000217
| Global Round : 24 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000065
| Global Round : 24 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000129
| Global Round : 24 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.007007
| Global Round : 24 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000430
| Global Round : 24 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.004085
| Global Round : 24 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000100
| Global Round : 24 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000109
| Global Round : 24 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000107
| Global Round : 24 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000503
| Global Round : 24 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.011352
| Global Round : 24 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000558
| Global Round : 24 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.001250
| Global Round : 24 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000166
| Global Round : 24 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000024
| Global Round : 24 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000057
| Global Round : 24 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.002928
| Global Round : 24 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000122
| Global Round : 24 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000314
| Global Round : 24 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000104
| Global Round : 24 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000013
| Global Round : 24 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000229
| Global Round : 24 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.039139
| Global Round : 24 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000900
| Global Round : 24 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000247
| Global Round : 24 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.001076
| Global Round : 24 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.007072
| Global Round : 24 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000294
| Global Round : 24 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000111
| Global Round : 24 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000090
| Global Round : 24 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.001357
| Global Round : 24 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000095
| Global Round : 24 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000133
| Global Round : 24 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000017
| Global Round : 24 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.001095
| Global Round : 24 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.036928
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 9, Epoch: 24--------
------------------------------------------
| Global Round : 24 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000651
| Global Round : 24 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000069
| Global Round : 24 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000246
| Global Round : 24 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000351
| Global Round : 24 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000905
| Global Round : 24 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000023
| Global Round : 24 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000187
| Global Round : 24 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000014
| Global Round : 24 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.001060
| Global Round : 24 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.002373
| Global Round : 24 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000065
| Global Round : 24 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.007033
| Global Round : 24 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000051
| Global Round : 24 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000123
| Global Round : 24 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000087
| Global Round : 24 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000138
| Global Round : 24 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000896
| Global Round : 24 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000224
| Global Round : 24 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000198
| Global Round : 24 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000272
| Global Round : 24 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000033
| Global Round : 24 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000036
| Global Round : 24 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000150
| Global Round : 24 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000022
| Global Round : 24 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000469
| Global Round : 24 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000120
| Global Round : 24 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000248
| Global Round : 24 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000115
| Global Round : 24 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000783
| Global Round : 24 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.004271
| Global Round : 24 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.001247
| Global Round : 24 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000054
| Global Round : 24 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.001479
| Global Round : 24 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.002008
| Global Round : 24 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000044
| Global Round : 24 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000759
| Global Round : 24 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.004612
| Global Round : 24 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000551
| Global Round : 24 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000046
| Global Round : 24 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000410
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 4, Epoch: 24--------
------------------------------------------
| Global Round : 24 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000524
| Global Round : 24 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000128
| Global Round : 24 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000122
| Global Round : 24 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000519
| Global Round : 24 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000127
| Global Round : 24 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000028
| Global Round : 24 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000205
| Global Round : 24 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000135
| Global Round : 24 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000207
| Global Round : 24 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000715
| Global Round : 24 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.004044
| Global Round : 24 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000230
| Global Round : 24 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000016
| Global Round : 24 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000084
| Global Round : 24 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000180
| Global Round : 24 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000045
| Global Round : 24 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.002242
| Global Round : 24 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000228
| Global Round : 24 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.011921
| Global Round : 24 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000082
| Global Round : 24 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000520
| Global Round : 24 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000357
| Global Round : 24 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.014346
| Global Round : 24 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.001398
| Global Round : 24 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000277
| Global Round : 24 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000071
| Global Round : 24 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000080
| Global Round : 24 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000258
| Global Round : 24 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000098
| Global Round : 24 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000210
| Global Round : 24 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000566
| Global Round : 24 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000077
| Global Round : 24 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000124
| Global Round : 24 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000284
| Global Round : 24 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000375
| Global Round : 24 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000362
| Global Round : 24 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000465
| Global Round : 24 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.001371
| Global Round : 24 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000525
| Global Round : 24 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000115
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 8, Epoch: 24--------
------------------------------------------
| Global Round : 24 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000150
| Global Round : 24 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000532
| Global Round : 24 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000024
| Global Round : 24 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000071
| Global Round : 24 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000031
| Global Round : 24 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000239
| Global Round : 24 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.013869
| Global Round : 24 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000127
| Global Round : 24 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000151
| Global Round : 24 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.001009
| Global Round : 24 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000292
| Global Round : 24 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000660
| Global Round : 24 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000319
| Global Round : 24 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000007
| Global Round : 24 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000999
| Global Round : 24 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000255
| Global Round : 24 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.023449
| Global Round : 24 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000418
| Global Round : 24 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000006
| Global Round : 24 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000343
| Global Round : 24 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000023
| Global Round : 24 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000027
| Global Round : 24 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000128
| Global Round : 24 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000134
| Global Round : 24 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.001067
| Global Round : 24 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000226
| Global Round : 24 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000497
| Global Round : 24 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000191
| Global Round : 24 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000032
| Global Round : 24 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000204
| Global Round : 24 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000176
| Global Round : 24 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000012
| Global Round : 24 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000652
| Global Round : 24 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000405
| Global Round : 24 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.001418
| Global Round : 24 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000354
| Global Round : 24 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000068
| Global Round : 24 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000077
| Global Round : 24 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000175
| Global Round : 24 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000025
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 6, Epoch: 24--------
------------------------------------------
| Global Round : 24 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000391
| Global Round : 24 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.003811
| Global Round : 24 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000173
| Global Round : 24 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000051
| Global Round : 24 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000027
| Global Round : 24 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000188
| Global Round : 24 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.007109
| Global Round : 24 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000339
| Global Round : 24 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000130
| Global Round : 24 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000343
| Global Round : 24 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000022
| Global Round : 24 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000354
| Global Round : 24 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000396
| Global Round : 24 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000097
| Global Round : 24 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000519
| Global Round : 24 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000361
| Global Round : 24 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000915
| Global Round : 24 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000057
| Global Round : 24 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000041
| Global Round : 24 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000404
| Global Round : 24 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000103
| Global Round : 24 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000062
| Global Round : 24 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.001607
| Global Round : 24 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.001650
| Global Round : 24 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000352
| Global Round : 24 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.028292
| Global Round : 24 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.002585
| Global Round : 24 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000079
| Global Round : 24 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.007644
| Global Round : 24 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000013
| Global Round : 24 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000162
| Global Round : 24 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000339
| Global Round : 24 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000163
| Global Round : 24 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000779
| Global Round : 24 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000138
| Global Round : 24 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000094
| Global Round : 24 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000033
| Global Round : 24 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000097
| Global Round : 24 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000065
| Global Round : 24 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000155
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 7, Epoch: 24--------
------------------------------------------
| Global Round : 24 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000256
| Global Round : 24 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.030920
| Global Round : 24 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000155
| Global Round : 24 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000607
| Global Round : 24 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000180
| Global Round : 24 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000100
| Global Round : 24 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000046
| Global Round : 24 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.003554
| Global Round : 24 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000032
| Global Round : 24 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000197
| Global Round : 24 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.001176
| Global Round : 24 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.004073
| Global Round : 24 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000039
| Global Round : 24 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.002272
| Global Round : 24 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000251
| Global Round : 24 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000390
| Global Round : 24 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.001745
| Global Round : 24 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000859
| Global Round : 24 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000669
| Global Round : 24 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000158
| Global Round : 24 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000475
| Global Round : 24 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000051
| Global Round : 24 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000133
| Global Round : 24 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000069
| Global Round : 24 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000116
| Global Round : 24 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000013
| Global Round : 24 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000128
| Global Round : 24 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000079
| Global Round : 24 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000110
| Global Round : 24 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000976
| Global Round : 24 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000073
| Global Round : 24 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000451
| Global Round : 24 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000076
| Global Round : 24 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000768
| Global Round : 24 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000163
| Global Round : 24 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.002043
| Global Round : 24 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.010555
| Global Round : 24 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.003655
| Global Round : 24 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.001049
| Global Round : 24 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000976
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 3, Epoch: 24--------
------------------------------------------
| Global Round : 24 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000365
| Global Round : 24 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000246
| Global Round : 24 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000128
| Global Round : 24 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000763
| Global Round : 24 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000094
| Global Round : 24 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000817
| Global Round : 24 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.002579
| Global Round : 24 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.001601
| Global Round : 24 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000978
| Global Round : 24 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000088
| Global Round : 24 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000029
| Global Round : 24 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.006016
| Global Round : 24 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000389
| Global Round : 24 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000237
| Global Round : 24 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000036
| Global Round : 24 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000304
| Global Round : 24 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000350
| Global Round : 24 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.001375
| Global Round : 24 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000336
| Global Round : 24 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000066
| Global Round : 24 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000075
| Global Round : 24 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000070
| Global Round : 24 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000644
| Global Round : 24 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000207
| Global Round : 24 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000388
| Global Round : 24 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000241
| Global Round : 24 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000964
| Global Round : 24 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.001159
| Global Round : 24 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000003
| Global Round : 24 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000324
| Global Round : 24 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000073
| Global Round : 24 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000294
| Global Round : 24 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000183
| Global Round : 24 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.001735
| Global Round : 24 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000204
| Global Round : 24 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000368
| Global Round : 24 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.001421
| Global Round : 24 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.003691
| Global Round : 24 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.001250
| Global Round : 24 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000225
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
 
Avg Training Stats after 25 global rounds:
Training Loss : nan
Train Accuracy: 9977.36% 

Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 49.68% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.10%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 51.10 %
----Attack Model Testing----
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
 83%| | 25/30 [1:33:06<18:41, 224.38s/it]Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

Selected users for the training: [0 4 7 5 2 6 9 1 3 8]
------------------------------------------
------User: 0, Epoch: 25--------
------------------------------------------
| Global Round : 25 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000051
| Global Round : 25 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000697
| Global Round : 25 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000346
| Global Round : 25 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000918
| Global Round : 25 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000817
| Global Round : 25 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000313
| Global Round : 25 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000064
| Global Round : 25 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000802
| Global Round : 25 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000324
| Global Round : 25 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000806
| Global Round : 25 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000106
| Global Round : 25 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000108
| Global Round : 25 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000276
| Global Round : 25 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000093
| Global Round : 25 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000532
| Global Round : 25 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.001032
| Global Round : 25 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000244
| Global Round : 25 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000045
| Global Round : 25 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.001954
| Global Round : 25 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000567
| Global Round : 25 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.002480
| Global Round : 25 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000031
| Global Round : 25 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000084
| Global Round : 25 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.003989
| Global Round : 25 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.001500
| Global Round : 25 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.002219
| Global Round : 25 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000023
| Global Round : 25 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000964
| Global Round : 25 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000021
| Global Round : 25 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.001236
| Global Round : 25 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000010
| Global Round : 25 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.005322
| Global Round : 25 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000540
| Global Round : 25 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.002201
| Global Round : 25 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000089
| Global Round : 25 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.002452
| Global Round : 25 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000624
| Global Round : 25 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000811
| Global Round : 25 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000086
| Global Round : 25 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.002878
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 49.80% | Val Loss: nan | Val Acc: 49.30%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 50.03% | Val Loss: nan | Val Acc: 49.30%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 50.03% | Val Loss: nan | Val Acc: 49.30%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 50.03% | Val Loss: nan | Val Acc: 49.30%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 50.03% | Val Loss: nan | Val Acc: 49.30%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 50.03% | Val Loss: nan | Val Acc: 49.30%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 50.03% | Val Loss: nan | Val Acc: 49.30%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 50.03% | Val Loss: nan | Val Acc: 49.30%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 50.03% | Val Loss: nan | Val Acc: 49.30%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 50.03% | Val Loss: nan | Val Acc: 49.30%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 50.03% | Val Loss: nan | Val Acc: 49.30%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 50.03% | Val Loss: nan | Val Acc: 49.30%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 50.03% | Val Loss: nan | Val Acc: 49.30%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 50.03% | Val Loss: nan | Val Acc: 49.30%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 50.03% | Val Loss: nan | Val Acc: 49.30%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 50.03% | Val Loss: nan | Val Acc: 49.30%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 50.03% | Val Loss: nan | Val Acc: 49.30%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 50.03% | Val Loss: nan | Val Acc: 49.30%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 50.03% | Val Loss: nan | Val Acc: 49.30%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 50.03% | Val Loss: nan | Val Acc: 49.30%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 50.03% | Val Loss: nan | Val Acc: 49.30%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 50.03% | Val Loss: nan | Val Acc: 49.30%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 50.03% | Val Loss: nan | Val Acc: 49.30%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 50.03% | Val Loss: nan | Val Acc: 49.30%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 50.03% | Val Loss: nan | Val Acc: 49.30%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 50.03% | Val Loss: nan | Val Acc: 49.30%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 50.03% | Val Loss: nan | Val Acc: 49.30%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 50.03% | Val Loss: nan | Val Acc: 49.30%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 50.03% | Val Loss: nan | Val Acc: 49.30%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 50.03% | Val Loss: nan | Val Acc: 49.30%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 50.03% | Val Loss: nan | Val Acc: 49.30%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 50.03% | Val Loss: nan | Val Acc: 49.30%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 50.03% | Val Loss: nan | Val Acc: 49.30%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 50.03% | Val Loss: nan | Val Acc: 49.30%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 50.03% | Val Loss: nan | Val Acc: 49.30%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 50.03% | Val Loss: nan | Val Acc: 49.30%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 50.03% | Val Loss: nan | Val Acc: 49.30%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 50.03% | Val Loss: nan | Val Acc: 49.30%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 50.03% | Val Loss: nan | Val Acc: 49.30%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 50.03% | Val Loss: nan | Val Acc: 49.30%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 50.03% | Val Loss: nan | Val Acc: 49.30%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 50.03% | Val Loss: nan | Val Acc: 49.30%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 50.03% | Val Loss: nan | Val Acc: 49.30%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 50.03% | Val Loss: nan | Val Acc: 49.30%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 50.03% | Val Loss: nan | Val Acc: 49.30%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 50.03% | Val Loss: nan | Val Acc: 49.30%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 50.03% | Val Loss: nan | Val Acc: 49.30%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 50.03% | Val Loss: nan | Val Acc: 49.30%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 50.03% | Val Loss: nan | Val Acc: 49.30%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 50.03% | Val Loss: nan | Val Acc: 49.30%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 49.30 %
----Attack Model Testing----
Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

------------------------------------------
------User: 4, Epoch: 25--------
------------------------------------------
| Global Round : 25 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.003137
| Global Round : 25 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.009300
| Global Round : 25 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000204
| Global Round : 25 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000412
| Global Round : 25 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000017
| Global Round : 25 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000129
| Global Round : 25 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000049
| Global Round : 25 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000313
| Global Round : 25 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000060
| Global Round : 25 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000075
| Global Round : 25 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000204
| Global Round : 25 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000838
| Global Round : 25 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000019
| Global Round : 25 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000034
| Global Round : 25 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000812
| Global Round : 25 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.002764
| Global Round : 25 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.002223
| Global Round : 25 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000017
| Global Round : 25 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000195
| Global Round : 25 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.002954
| Global Round : 25 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000604
| Global Round : 25 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000116
| Global Round : 25 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.032269
| Global Round : 25 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000038
| Global Round : 25 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000032
| Global Round : 25 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000353
| Global Round : 25 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000336
| Global Round : 25 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.002915
| Global Round : 25 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000175
| Global Round : 25 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000125
| Global Round : 25 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000241
| Global Round : 25 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000007
| Global Round : 25 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000402
| Global Round : 25 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000036
| Global Round : 25 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.001553
| Global Round : 25 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000731
| Global Round : 25 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.001054
| Global Round : 25 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.003221
| Global Round : 25 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.001264
| Global Round : 25 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.001990
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 7, Epoch: 25--------
------------------------------------------
| Global Round : 25 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.003512
| Global Round : 25 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000051
| Global Round : 25 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000045
| Global Round : 25 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000081
| Global Round : 25 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.011913
| Global Round : 25 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000003
| Global Round : 25 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000612
| Global Round : 25 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.178724
| Global Round : 25 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.048579
| Global Round : 25 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000045
| Global Round : 25 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.001257
| Global Round : 25 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000128
| Global Round : 25 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000320
| Global Round : 25 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.001304
| Global Round : 25 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000067
| Global Round : 25 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000029
| Global Round : 25 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000225
| Global Round : 25 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000324
| Global Round : 25 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.001264
| Global Round : 25 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000049
| Global Round : 25 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.001466
| Global Round : 25 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.001050
| Global Round : 25 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.001126
| Global Round : 25 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000908
| Global Round : 25 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000219
| Global Round : 25 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000307
| Global Round : 25 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000021
| Global Round : 25 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000148
| Global Round : 25 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000169
| Global Round : 25 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000063
| Global Round : 25 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000009
| Global Round : 25 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.004439
| Global Round : 25 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000954
| Global Round : 25 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000270
| Global Round : 25 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000134
| Global Round : 25 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000024
| Global Round : 25 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.001101
| Global Round : 25 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000006
| Global Round : 25 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000023
| Global Round : 25 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000057
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 5, Epoch: 25--------
------------------------------------------
| Global Round : 25 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000251
| Global Round : 25 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000228
| Global Round : 25 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000417
| Global Round : 25 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000417
| Global Round : 25 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.003980
| Global Round : 25 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000079
| Global Round : 25 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000108
| Global Round : 25 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000732
| Global Round : 25 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000092
| Global Round : 25 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.068470
| Global Round : 25 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000738
| Global Round : 25 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.002161
| Global Round : 25 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000018
| Global Round : 25 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000269
| Global Round : 25 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.004125
| Global Round : 25 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000117
| Global Round : 25 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000031
| Global Round : 25 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000048
| Global Round : 25 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000178
| Global Round : 25 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.004962
| Global Round : 25 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000142
| Global Round : 25 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000071
| Global Round : 25 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000000
| Global Round : 25 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000054
| Global Round : 25 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000079
| Global Round : 25 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.001574
| Global Round : 25 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000058
| Global Round : 25 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000111
| Global Round : 25 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000027
| Global Round : 25 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000065
| Global Round : 25 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000109
| Global Round : 25 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.001214
| Global Round : 25 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000602
| Global Round : 25 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000413
| Global Round : 25 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000105
| Global Round : 25 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000017
| Global Round : 25 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000363
| Global Round : 25 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000768
| Global Round : 25 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000227
| Global Round : 25 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000236
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 2, Epoch: 25--------
------------------------------------------
| Global Round : 25 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.008381
| Global Round : 25 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.002111
| Global Round : 25 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000019
| Global Round : 25 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.001928
| Global Round : 25 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.003889
| Global Round : 25 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000036
| Global Round : 25 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000070
| Global Round : 25 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.005250
| Global Round : 25 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.002311
| Global Round : 25 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000078
| Global Round : 25 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000453
| Global Round : 25 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000655
| Global Round : 25 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000404
| Global Round : 25 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000545
| Global Round : 25 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.003745
| Global Round : 25 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000279
| Global Round : 25 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000121
| Global Round : 25 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000706
| Global Round : 25 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000334
| Global Round : 25 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000308
| Global Round : 25 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000773
| Global Round : 25 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000084
| Global Round : 25 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.001579
| Global Round : 25 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.001628
| Global Round : 25 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000088
| Global Round : 25 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000163
| Global Round : 25 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000149
| Global Round : 25 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000511
| Global Round : 25 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000275
| Global Round : 25 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.010096
| Global Round : 25 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.001096
| Global Round : 25 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000239
| Global Round : 25 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.001497
| Global Round : 25 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.004350
| Global Round : 25 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000881
| Global Round : 25 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000040
| Global Round : 25 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.001563
| Global Round : 25 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000077
| Global Round : 25 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000080
| Global Round : 25 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000048
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 6, Epoch: 25--------
------------------------------------------
| Global Round : 25 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000140
| Global Round : 25 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.001073
| Global Round : 25 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000832
| Global Round : 25 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000042
| Global Round : 25 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000121
| Global Round : 25 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000606
| Global Round : 25 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000090
| Global Round : 25 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.001512
| Global Round : 25 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000147
| Global Round : 25 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000697
| Global Round : 25 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000349
| Global Round : 25 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000002
| Global Round : 25 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000249
| Global Round : 25 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.002181
| Global Round : 25 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000204
| Global Round : 25 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000117
| Global Round : 25 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000080
| Global Round : 25 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.002932
| Global Round : 25 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.001545
| Global Round : 25 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000034
| Global Round : 25 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000072
| Global Round : 25 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000035
| Global Round : 25 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000546
| Global Round : 25 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000342
| Global Round : 25 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.001560
| Global Round : 25 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.002924
| Global Round : 25 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000013
| Global Round : 25 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000084
| Global Round : 25 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000072
| Global Round : 25 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000091
| Global Round : 25 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000083
| Global Round : 25 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.004409
| Global Round : 25 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000089
| Global Round : 25 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000033
| Global Round : 25 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000010
| Global Round : 25 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000067
| Global Round : 25 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.004083
| Global Round : 25 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000102
| Global Round : 25 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000395
| Global Round : 25 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000008
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 9, Epoch: 25--------
------------------------------------------
| Global Round : 25 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000006
| Global Round : 25 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000026
| Global Round : 25 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000707
| Global Round : 25 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000052
| Global Round : 25 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000036
| Global Round : 25 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.011999
| Global Round : 25 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000104
| Global Round : 25 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000446
| Global Round : 25 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.001762
| Global Round : 25 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000081
| Global Round : 25 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.001126
| Global Round : 25 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000220
| Global Round : 25 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000020
| Global Round : 25 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000036
| Global Round : 25 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000554
| Global Round : 25 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000130
| Global Round : 25 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000054
| Global Round : 25 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000021
| Global Round : 25 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000977
| Global Round : 25 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000071
| Global Round : 25 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000420
| Global Round : 25 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000004
| Global Round : 25 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000179
| Global Round : 25 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000020
| Global Round : 25 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000019
| Global Round : 25 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000053
| Global Round : 25 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.002019
| Global Round : 25 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000115
| Global Round : 25 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.003567
| Global Round : 25 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000381
| Global Round : 25 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000022
| Global Round : 25 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000562
| Global Round : 25 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000204
| Global Round : 25 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000115
| Global Round : 25 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000097
| Global Round : 25 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.001076
| Global Round : 25 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000268
| Global Round : 25 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000039
| Global Round : 25 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000686
| Global Round : 25 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000121
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 1, Epoch: 25--------
------------------------------------------
| Global Round : 25 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000002
| Global Round : 25 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.005155
| Global Round : 25 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000194
| Global Round : 25 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.001662
| Global Round : 25 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000121
| Global Round : 25 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000165
| Global Round : 25 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.001404
| Global Round : 25 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000410
| Global Round : 25 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000034
| Global Round : 25 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000280
| Global Round : 25 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.002354
| Global Round : 25 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000170
| Global Round : 25 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.001753
| Global Round : 25 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000036
| Global Round : 25 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.001241
| Global Round : 25 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000257
| Global Round : 25 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000045
| Global Round : 25 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000242
| Global Round : 25 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.001218
| Global Round : 25 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.008788
| Global Round : 25 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.005010
| Global Round : 25 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000678
| Global Round : 25 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.013101
| Global Round : 25 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000005
| Global Round : 25 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000513
| Global Round : 25 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.006294
| Global Round : 25 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000525
| Global Round : 25 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000281
| Global Round : 25 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000639
| Global Round : 25 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000107
| Global Round : 25 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000099
| Global Round : 25 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000085
| Global Round : 25 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000132
| Global Round : 25 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000266
| Global Round : 25 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000119
| Global Round : 25 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000045
| Global Round : 25 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000936
| Global Round : 25 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000161
| Global Round : 25 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000009
| Global Round : 25 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000242
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 3, Epoch: 25--------
------------------------------------------
| Global Round : 25 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000043
| Global Round : 25 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000113
| Global Round : 25 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000401
| Global Round : 25 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000214
| Global Round : 25 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000864
| Global Round : 25 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000244
| Global Round : 25 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000052
| Global Round : 25 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000012
| Global Round : 25 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000004
| Global Round : 25 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000531
| Global Round : 25 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000703
| Global Round : 25 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000522
| Global Round : 25 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000187
| Global Round : 25 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.032470
| Global Round : 25 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000075
| Global Round : 25 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000617
| Global Round : 25 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000213
| Global Round : 25 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.001458
| Global Round : 25 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000121
| Global Round : 25 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000496
| Global Round : 25 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000709
| Global Round : 25 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.003207
| Global Round : 25 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000038
| Global Round : 25 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000143
| Global Round : 25 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000094
| Global Round : 25 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000072
| Global Round : 25 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000118
| Global Round : 25 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000603
| Global Round : 25 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.001118
| Global Round : 25 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000301
| Global Round : 25 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000048
| Global Round : 25 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000320
| Global Round : 25 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000037
| Global Round : 25 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000493
| Global Round : 25 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.009005
| Global Round : 25 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000147
| Global Round : 25 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.001346
| Global Round : 25 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000099
| Global Round : 25 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000059
| Global Round : 25 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000034
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 8, Epoch: 25--------
------------------------------------------
| Global Round : 25 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000021
| Global Round : 25 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000008
| Global Round : 25 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000927
| Global Round : 25 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.008662
| Global Round : 25 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000730
| Global Round : 25 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000047
| Global Round : 25 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000130
| Global Round : 25 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000155
| Global Round : 25 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.032367
| Global Round : 25 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000315
| Global Round : 25 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000054
| Global Round : 25 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000125
| Global Round : 25 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.007691
| Global Round : 25 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.001787
| Global Round : 25 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000113
| Global Round : 25 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000077
| Global Round : 25 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000511
| Global Round : 25 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000077
| Global Round : 25 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000616
| Global Round : 25 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000097
| Global Round : 25 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000588
| Global Round : 25 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000380
| Global Round : 25 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.006314
| Global Round : 25 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000057
| Global Round : 25 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000297
| Global Round : 25 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000023
| Global Round : 25 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000111
| Global Round : 25 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000010
| Global Round : 25 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000598
| Global Round : 25 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000078
| Global Round : 25 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.001289
| Global Round : 25 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000025
| Global Round : 25 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000159
| Global Round : 25 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.009061
| Global Round : 25 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.001168
| Global Round : 25 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000055
| Global Round : 25 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.001064
| Global Round : 25 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.003307
| Global Round : 25 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000670
| Global Round : 25 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.002561
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 99.8], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
 
Avg Training Stats after 26 global rounds:
Training Loss : nan
Train Accuracy: 9978.15% 

Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 49.76% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 50.04% | Val Loss: nan | Val Acc: 49.00%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 49.00 %
----Attack Model Testing----
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
 87%| | 26/30 [1:36:44<14:50, 222.51s/it]Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

Selected users for the training: [9 0 7 8 6 2 5 3 1 4]
------------------------------------------
------User: 9, Epoch: 26--------
------------------------------------------
| Global Round : 26 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000172
| Global Round : 26 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.001865
| Global Round : 26 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000306
| Global Round : 26 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000171
| Global Round : 26 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000097
| Global Round : 26 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000142
| Global Round : 26 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000079
| Global Round : 26 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000072
| Global Round : 26 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000105
| Global Round : 26 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000512
| Global Round : 26 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000160
| Global Round : 26 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000890
| Global Round : 26 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000011
| Global Round : 26 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.001108
| Global Round : 26 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.002352
| Global Round : 26 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000824
| Global Round : 26 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.002211
| Global Round : 26 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.001072
| Global Round : 26 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.002043
| Global Round : 26 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.004607
| Global Round : 26 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000483
| Global Round : 26 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000062
| Global Round : 26 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000093
| Global Round : 26 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.001168
| Global Round : 26 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000579
| Global Round : 26 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000466
| Global Round : 26 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000104
| Global Round : 26 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000025
| Global Round : 26 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000368
| Global Round : 26 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.001147
| Global Round : 26 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000155
| Global Round : 26 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000131
| Global Round : 26 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000903
| Global Round : 26 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000209
| Global Round : 26 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.001186
| Global Round : 26 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000021
| Global Round : 26 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000025
| Global Round : 26 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.002211
| Global Round : 26 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000571
| Global Round : 26 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000369
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 99.8], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 0, Epoch: 26--------
------------------------------------------
| Global Round : 26 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.001253
| Global Round : 26 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000673
| Global Round : 26 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000534
| Global Round : 26 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000065
| Global Round : 26 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000182
| Global Round : 26 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000448
| Global Round : 26 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000596
| Global Round : 26 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000077
| Global Round : 26 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000431
| Global Round : 26 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000184
| Global Round : 26 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000245
| Global Round : 26 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.002359
| Global Round : 26 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000107
| Global Round : 26 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000065
| Global Round : 26 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000039
| Global Round : 26 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000310
| Global Round : 26 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000977
| Global Round : 26 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000014
| Global Round : 26 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000471
| Global Round : 26 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.001026
| Global Round : 26 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000978
| Global Round : 26 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000090
| Global Round : 26 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000525
| Global Round : 26 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000247
| Global Round : 26 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000019
| Global Round : 26 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000668
| Global Round : 26 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000057
| Global Round : 26 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000017
| Global Round : 26 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000993
| Global Round : 26 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000747
| Global Round : 26 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.002844
| Global Round : 26 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000713
| Global Round : 26 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.004210
| Global Round : 26 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000515
| Global Round : 26 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000539
| Global Round : 26 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000017
| Global Round : 26 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.011718
| Global Round : 26 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000030
| Global Round : 26 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.003049
| Global Round : 26 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000047
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 99.8], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.70%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.70%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.70%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.70%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.70%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.70%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.70%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.70%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.70%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.70%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.70%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.70%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.70%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.70%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.70%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.70%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.70%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.70%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.70%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.70%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.70%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.70%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.70%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.70%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.70%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.70%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.70%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.70%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.70%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.70%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.70%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.70%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.70%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.70%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.70%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.70%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.70%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.70%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.70%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.70%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.70%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.70%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.70%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.70%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.70%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.70%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.70%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.70%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.70%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.70%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 48.70 %
----Attack Model Testing----
Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

------------------------------------------
------User: 7, Epoch: 26--------
------------------------------------------
| Global Round : 26 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000402
| Global Round : 26 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000703
| Global Round : 26 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000246
| Global Round : 26 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000618
| Global Round : 26 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000918
| Global Round : 26 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000094
| Global Round : 26 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000145
| Global Round : 26 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000054
| Global Round : 26 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000356
| Global Round : 26 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000189
| Global Round : 26 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000080
| Global Round : 26 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.001246
| Global Round : 26 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.001011
| Global Round : 26 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000564
| Global Round : 26 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000199
| Global Round : 26 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000189
| Global Round : 26 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000391
| Global Round : 26 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000154
| Global Round : 26 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000119
| Global Round : 26 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000243
| Global Round : 26 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000444
| Global Round : 26 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.003168
| Global Round : 26 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000086
| Global Round : 26 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000323
| Global Round : 26 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000034
| Global Round : 26 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000898
| Global Round : 26 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000029
| Global Round : 26 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000055
| Global Round : 26 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000145
| Global Round : 26 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000245
| Global Round : 26 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000022
| Global Round : 26 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000660
| Global Round : 26 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000013
| Global Round : 26 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000065
| Global Round : 26 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000110
| Global Round : 26 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.001472
| Global Round : 26 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000235
| Global Round : 26 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.011026
| Global Round : 26 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000348
| Global Round : 26 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000336
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 99.8], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 8, Epoch: 26--------
------------------------------------------
| Global Round : 26 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.003232
| Global Round : 26 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.002012
| Global Round : 26 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000150
| Global Round : 26 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000286
| Global Round : 26 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000983
| Global Round : 26 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000236
| Global Round : 26 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000656
| Global Round : 26 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.002842
| Global Round : 26 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000058
| Global Round : 26 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.002365
| Global Round : 26 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000022
| Global Round : 26 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000130
| Global Round : 26 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000017
| Global Round : 26 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000178
| Global Round : 26 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000977
| Global Round : 26 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000579
| Global Round : 26 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000669
| Global Round : 26 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000263
| Global Round : 26 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000372
| Global Round : 26 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.006744
| Global Round : 26 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000202
| Global Round : 26 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.001245
| Global Round : 26 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000041
| Global Round : 26 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.001856
| Global Round : 26 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000095
| Global Round : 26 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000066
| Global Round : 26 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.003917
| Global Round : 26 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000146
| Global Round : 26 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000650
| Global Round : 26 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000039
| Global Round : 26 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000801
| Global Round : 26 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000794
| Global Round : 26 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000388
| Global Round : 26 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000283
| Global Round : 26 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000161
| Global Round : 26 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000017
| Global Round : 26 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000479
| Global Round : 26 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000033
| Global Round : 26 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.022132
| Global Round : 26 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000566
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 6, Epoch: 26--------
------------------------------------------
| Global Round : 26 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.002550
| Global Round : 26 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000194
| Global Round : 26 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000092
| Global Round : 26 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.002237
| Global Round : 26 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000186
| Global Round : 26 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000430
| Global Round : 26 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000702
| Global Round : 26 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000288
| Global Round : 26 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000221
| Global Round : 26 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000023
| Global Round : 26 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000598
| Global Round : 26 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000072
| Global Round : 26 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000965
| Global Round : 26 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000089
| Global Round : 26 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000559
| Global Round : 26 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.001241
| Global Round : 26 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.001718
| Global Round : 26 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000122
| Global Round : 26 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000807
| Global Round : 26 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000106
| Global Round : 26 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.001418
| Global Round : 26 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000117
| Global Round : 26 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000329
| Global Round : 26 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.001212
| Global Round : 26 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.010190
| Global Round : 26 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000383
| Global Round : 26 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000015
| Global Round : 26 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000045
| Global Round : 26 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000603
| Global Round : 26 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000128
| Global Round : 26 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.007989
| Global Round : 26 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.008900
| Global Round : 26 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000053
| Global Round : 26 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000017
| Global Round : 26 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000079
| Global Round : 26 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000536
| Global Round : 26 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000649
| Global Round : 26 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.002555
| Global Round : 26 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000099
| Global Round : 26 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.006030
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 2, Epoch: 26--------
------------------------------------------
| Global Round : 26 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000525
| Global Round : 26 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000708
| Global Round : 26 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000867
| Global Round : 26 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.010398
| Global Round : 26 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000436
| Global Round : 26 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.010268
| Global Round : 26 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000076
| Global Round : 26 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000026
| Global Round : 26 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000200
| Global Round : 26 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000067
| Global Round : 26 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000383
| Global Round : 26 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000090
| Global Round : 26 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000729
| Global Round : 26 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000303
| Global Round : 26 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000028
| Global Round : 26 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000560
| Global Round : 26 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000036
| Global Round : 26 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000753
| Global Round : 26 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.005845
| Global Round : 26 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000793
| Global Round : 26 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.001914
| Global Round : 26 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000306
| Global Round : 26 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000192
| Global Round : 26 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.019762
| Global Round : 26 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.001688
| Global Round : 26 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.003633
| Global Round : 26 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000239
| Global Round : 26 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000417
| Global Round : 26 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000649
| Global Round : 26 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000072
| Global Round : 26 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000016
| Global Round : 26 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000046
| Global Round : 26 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000095
| Global Round : 26 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000039
| Global Round : 26 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000004
| Global Round : 26 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000167
| Global Round : 26 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000137
| Global Round : 26 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000065
| Global Round : 26 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000104
| Global Round : 26 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000055
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 5, Epoch: 26--------
------------------------------------------
| Global Round : 26 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000036
| Global Round : 26 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000122
| Global Round : 26 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.002957
| Global Round : 26 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000073
| Global Round : 26 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.005622
| Global Round : 26 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000270
| Global Round : 26 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000135
| Global Round : 26 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.024693
| Global Round : 26 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000590
| Global Round : 26 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000001
| Global Round : 26 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.003186
| Global Round : 26 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000767
| Global Round : 26 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000068
| Global Round : 26 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000010
| Global Round : 26 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000250
| Global Round : 26 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000083
| Global Round : 26 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000013
| Global Round : 26 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000614
| Global Round : 26 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000049
| Global Round : 26 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000328
| Global Round : 26 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000371
| Global Round : 26 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.001248
| Global Round : 26 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000131
| Global Round : 26 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000204
| Global Round : 26 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000079
| Global Round : 26 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000133
| Global Round : 26 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000205
| Global Round : 26 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000214
| Global Round : 26 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000294
| Global Round : 26 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000149
| Global Round : 26 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.011717
| Global Round : 26 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000053
| Global Round : 26 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.001633
| Global Round : 26 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000082
| Global Round : 26 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000038
| Global Round : 26 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000061
| Global Round : 26 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.002886
| Global Round : 26 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000148
| Global Round : 26 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000199
| Global Round : 26 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000234
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 3, Epoch: 26--------
------------------------------------------
| Global Round : 26 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000389
| Global Round : 26 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000356
| Global Round : 26 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000010
| Global Round : 26 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000038
| Global Round : 26 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000022
| Global Round : 26 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000033
| Global Round : 26 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000423
| Global Round : 26 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000249
| Global Round : 26 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000732
| Global Round : 26 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000134
| Global Round : 26 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.001110
| Global Round : 26 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000070
| Global Round : 26 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000735
| Global Round : 26 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000375
| Global Round : 26 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000017
| Global Round : 26 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000384
| Global Round : 26 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.036415
| Global Round : 26 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000401
| Global Round : 26 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000606
| Global Round : 26 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000089
| Global Round : 26 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000051
| Global Round : 26 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000068
| Global Round : 26 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.001463
| Global Round : 26 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.027952
| Global Round : 26 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000011
| Global Round : 26 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.009797
| Global Round : 26 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.001432
| Global Round : 26 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000018
| Global Round : 26 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000162
| Global Round : 26 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000116
| Global Round : 26 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000079
| Global Round : 26 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.001018
| Global Round : 26 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000097
| Global Round : 26 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.001095
| Global Round : 26 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.004003
| Global Round : 26 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000121
| Global Round : 26 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000128
| Global Round : 26 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000334
| Global Round : 26 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.003199
| Global Round : 26 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000042
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 1, Epoch: 26--------
------------------------------------------
| Global Round : 26 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.005619
| Global Round : 26 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000273
| Global Round : 26 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000023
| Global Round : 26 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000007
| Global Round : 26 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000039
| Global Round : 26 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000046
| Global Round : 26 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000667
| Global Round : 26 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.001239
| Global Round : 26 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000039
| Global Round : 26 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000903
| Global Round : 26 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000027
| Global Round : 26 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000341
| Global Round : 26 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000267
| Global Round : 26 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000047
| Global Round : 26 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000226
| Global Round : 26 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000231
| Global Round : 26 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000203
| Global Round : 26 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000634
| Global Round : 26 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000275
| Global Round : 26 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000054
| Global Round : 26 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000086
| Global Round : 26 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000347
| Global Round : 26 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.004573
| Global Round : 26 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000997
| Global Round : 26 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000029
| Global Round : 26 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000071
| Global Round : 26 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000105
| Global Round : 26 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.002909
| Global Round : 26 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000233
| Global Round : 26 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000376
| Global Round : 26 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000316
| Global Round : 26 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000103
| Global Round : 26 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000076
| Global Round : 26 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000523
| Global Round : 26 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000160
| Global Round : 26 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000033
| Global Round : 26 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000092
| Global Round : 26 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.004507
| Global Round : 26 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000007
| Global Round : 26 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.002422
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 4, Epoch: 26--------
------------------------------------------
| Global Round : 26 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000233
| Global Round : 26 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000010
| Global Round : 26 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000139
| Global Round : 26 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000901
| Global Round : 26 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000032
| Global Round : 26 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000219
| Global Round : 26 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000087
| Global Round : 26 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.005653
| Global Round : 26 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000051
| Global Round : 26 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.002649
| Global Round : 26 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000504
| Global Round : 26 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000134
| Global Round : 26 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000064
| Global Round : 26 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000255
| Global Round : 26 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000199
| Global Round : 26 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000302
| Global Round : 26 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.001547
| Global Round : 26 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.001455
| Global Round : 26 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000473
| Global Round : 26 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000234
| Global Round : 26 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000044
| Global Round : 26 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.008933
| Global Round : 26 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000167
| Global Round : 26 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000233
| Global Round : 26 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000157
| Global Round : 26 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.025916
| Global Round : 26 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.004962
| Global Round : 26 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.003628
| Global Round : 26 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.001156
| Global Round : 26 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000201
| Global Round : 26 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.018947
| Global Round : 26 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000264
| Global Round : 26 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000285
| Global Round : 26 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.001316
| Global Round : 26 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000367
| Global Round : 26 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000386
| Global Round : 26 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000191
| Global Round : 26 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000257
| Global Round : 26 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000618
| Global Round : 26 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000403
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
 
Avg Training Stats after 27 global rounds:
Training Loss : nan
Train Accuracy: 9978.96% 

Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 49.40 %
----Attack Model Testing----
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
 90%| | 27/30 [1:40:31<11:12, 224.07s/it]Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

Selected users for the training: [8 5 9 2 3 0 6 7 4 1]
------------------------------------------
------User: 8, Epoch: 27--------
------------------------------------------
| Global Round : 27 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.001436
| Global Round : 27 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.001817
| Global Round : 27 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000008
| Global Round : 27 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000419
| Global Round : 27 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000030
| Global Round : 27 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.002689
| Global Round : 27 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000022
| Global Round : 27 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000586
| Global Round : 27 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000042
| Global Round : 27 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000014
| Global Round : 27 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000604
| Global Round : 27 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.001752
| Global Round : 27 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.004945
| Global Round : 27 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000014
| Global Round : 27 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000037
| Global Round : 27 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000276
| Global Round : 27 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000039
| Global Round : 27 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000031
| Global Round : 27 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000054
| Global Round : 27 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000068
| Global Round : 27 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000067
| Global Round : 27 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000387
| Global Round : 27 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000062
| Global Round : 27 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000066
| Global Round : 27 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000015
| Global Round : 27 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000737
| Global Round : 27 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000233
| Global Round : 27 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000017
| Global Round : 27 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000072
| Global Round : 27 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.004986
| Global Round : 27 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000504
| Global Round : 27 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000156
| Global Round : 27 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000080
| Global Round : 27 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000068
| Global Round : 27 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000265
| Global Round : 27 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000041
| Global Round : 27 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.024931
| Global Round : 27 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000548
| Global Round : 27 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000123
| Global Round : 27 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.002468
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 5, Epoch: 27--------
------------------------------------------
| Global Round : 27 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000037
| Global Round : 27 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000056
| Global Round : 27 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000060
| Global Round : 27 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.009929
| Global Round : 27 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000128
| Global Round : 27 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000444
| Global Round : 27 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000631
| Global Round : 27 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.001622
| Global Round : 27 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000121
| Global Round : 27 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.001889
| Global Round : 27 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000098
| Global Round : 27 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.004459
| Global Round : 27 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000037
| Global Round : 27 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000047
| Global Round : 27 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000539
| Global Round : 27 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000598
| Global Round : 27 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.001852
| Global Round : 27 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000041
| Global Round : 27 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000047
| Global Round : 27 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000464
| Global Round : 27 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000557
| Global Round : 27 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000148
| Global Round : 27 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000158
| Global Round : 27 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000362
| Global Round : 27 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000022
| Global Round : 27 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.002085
| Global Round : 27 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000105
| Global Round : 27 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000252
| Global Round : 27 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000820
| Global Round : 27 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000254
| Global Round : 27 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000067
| Global Round : 27 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000612
| Global Round : 27 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000304
| Global Round : 27 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000035
| Global Round : 27 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.010779
| Global Round : 27 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000212
| Global Round : 27 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000028
| Global Round : 27 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000003
| Global Round : 27 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.002269
| Global Round : 27 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.007852
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 9, Epoch: 27--------
------------------------------------------
| Global Round : 27 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000232
| Global Round : 27 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000369
| Global Round : 27 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000043
| Global Round : 27 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000068
| Global Round : 27 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000023
| Global Round : 27 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.001062
| Global Round : 27 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000288
| Global Round : 27 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.002902
| Global Round : 27 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000011
| Global Round : 27 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000062
| Global Round : 27 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000400
| Global Round : 27 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000066
| Global Round : 27 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.007207
| Global Round : 27 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.002264
| Global Round : 27 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000033
| Global Round : 27 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000053
| Global Round : 27 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000073
| Global Round : 27 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000326
| Global Round : 27 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000057
| Global Round : 27 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000485
| Global Round : 27 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000046
| Global Round : 27 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.001154
| Global Round : 27 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000052
| Global Round : 27 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000484
| Global Round : 27 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000028
| Global Round : 27 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000723
| Global Round : 27 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.001847
| Global Round : 27 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.004893
| Global Round : 27 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000339
| Global Round : 27 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000179
| Global Round : 27 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.001373
| Global Round : 27 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000060
| Global Round : 27 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000541
| Global Round : 27 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000782
| Global Round : 27 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000031
| Global Round : 27 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000366
| Global Round : 27 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.001467
| Global Round : 27 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000010
| Global Round : 27 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000016
| Global Round : 27 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000061
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 2, Epoch: 27--------
------------------------------------------
| Global Round : 27 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000020
| Global Round : 27 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.001426
| Global Round : 27 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000143
| Global Round : 27 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000203
| Global Round : 27 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000104
| Global Round : 27 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.005745
| Global Round : 27 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000446
| Global Round : 27 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.001353
| Global Round : 27 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000447
| Global Round : 27 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000220
| Global Round : 27 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000230
| Global Round : 27 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000014
| Global Round : 27 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000299
| Global Round : 27 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000100
| Global Round : 27 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000026
| Global Round : 27 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000419
| Global Round : 27 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000191
| Global Round : 27 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000144
| Global Round : 27 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000080
| Global Round : 27 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000009
| Global Round : 27 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000093
| Global Round : 27 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000455
| Global Round : 27 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000318
| Global Round : 27 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000805
| Global Round : 27 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000297
| Global Round : 27 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000424
| Global Round : 27 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000052
| Global Round : 27 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000270
| Global Round : 27 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000109
| Global Round : 27 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000127
| Global Round : 27 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000111
| Global Round : 27 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000012
| Global Round : 27 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000075
| Global Round : 27 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000254
| Global Round : 27 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000754
| Global Round : 27 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000516
| Global Round : 27 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000441
| Global Round : 27 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000299
| Global Round : 27 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000053
| Global Round : 27 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000442
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 3, Epoch: 27--------
------------------------------------------
| Global Round : 27 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000250
| Global Round : 27 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.002790
| Global Round : 27 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000032
| Global Round : 27 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.001022
| Global Round : 27 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000045
| Global Round : 27 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000156
| Global Round : 27 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000061
| Global Round : 27 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.010138
| Global Round : 27 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000584
| Global Round : 27 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000025
| Global Round : 27 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000058
| Global Round : 27 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000088
| Global Round : 27 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000156
| Global Round : 27 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000518
| Global Round : 27 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000189
| Global Round : 27 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.001276
| Global Round : 27 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000588
| Global Round : 27 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000776
| Global Round : 27 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000121
| Global Round : 27 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000103
| Global Round : 27 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000046
| Global Round : 27 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000082
| Global Round : 27 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000103
| Global Round : 27 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000031
| Global Round : 27 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000012
| Global Round : 27 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000030
| Global Round : 27 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000050
| Global Round : 27 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000125
| Global Round : 27 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000275
| Global Round : 27 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.004910
| Global Round : 27 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000068
| Global Round : 27 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000222
| Global Round : 27 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000015
| Global Round : 27 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.006771
| Global Round : 27 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.004907
| Global Round : 27 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.001174
| Global Round : 27 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000211
| Global Round : 27 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000077
| Global Round : 27 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000036
| Global Round : 27 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000093
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 0, Epoch: 27--------
------------------------------------------
| Global Round : 27 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.001024
| Global Round : 27 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000068
| Global Round : 27 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000006
| Global Round : 27 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000289
| Global Round : 27 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000379
| Global Round : 27 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.003913
| Global Round : 27 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000792
| Global Round : 27 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000527
| Global Round : 27 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000020
| Global Round : 27 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000292
| Global Round : 27 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000085
| Global Round : 27 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000018
| Global Round : 27 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000136
| Global Round : 27 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000247
| Global Round : 27 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000264
| Global Round : 27 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.010779
| Global Round : 27 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.005290
| Global Round : 27 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000518
| Global Round : 27 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000262
| Global Round : 27 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000186
| Global Round : 27 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000372
| Global Round : 27 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000550
| Global Round : 27 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000290
| Global Round : 27 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000065
| Global Round : 27 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000533
| Global Round : 27 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000398
| Global Round : 27 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000080
| Global Round : 27 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.003497
| Global Round : 27 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.006353
| Global Round : 27 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.002054
| Global Round : 27 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000447
| Global Round : 27 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.001150
| Global Round : 27 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000417
| Global Round : 27 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000343
| Global Round : 27 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.001787
| Global Round : 27 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000014
| Global Round : 27 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000034
| Global Round : 27 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000250
| Global Round : 27 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000092
| Global Round : 27 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000486
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 49.94% | Val Loss: nan | Val Acc: 51.30%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.30%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.30%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.30%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.30%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.30%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.30%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.30%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.30%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.30%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.30%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.30%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.30%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.30%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.30%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.30%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.30%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.30%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.30%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.30%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.30%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.30%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.30%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.30%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.30%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.30%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.30%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.30%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.30%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.30%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.30%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.30%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.30%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.30%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.30%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.30%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.30%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.30%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.30%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.30%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.30%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.30%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.30%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.30%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.30%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.30%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.30%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.30%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.30%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 49.95% | Val Loss: nan | Val Acc: 51.30%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 51.30 %
----Attack Model Testing----
Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

------------------------------------------
------User: 6, Epoch: 27--------
------------------------------------------
| Global Round : 27 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000063
| Global Round : 27 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.001016
| Global Round : 27 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000085
| Global Round : 27 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000889
| Global Round : 27 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.001931
| Global Round : 27 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.003174
| Global Round : 27 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000391
| Global Round : 27 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000127
| Global Round : 27 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000357
| Global Round : 27 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000919
| Global Round : 27 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000211
| Global Round : 27 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000025
| Global Round : 27 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.001011
| Global Round : 27 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000338
| Global Round : 27 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.001159
| Global Round : 27 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000225
| Global Round : 27 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000066
| Global Round : 27 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000073
| Global Round : 27 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000344
| Global Round : 27 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000106
| Global Round : 27 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000024
| Global Round : 27 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000245
| Global Round : 27 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000355
| Global Round : 27 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000143
| Global Round : 27 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000162
| Global Round : 27 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.001880
| Global Round : 27 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000285
| Global Round : 27 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000010
| Global Round : 27 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000923
| Global Round : 27 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.002717
| Global Round : 27 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.005339
| Global Round : 27 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000138
| Global Round : 27 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000207
| Global Round : 27 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.001851
| Global Round : 27 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000232
| Global Round : 27 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000322
| Global Round : 27 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000647
| Global Round : 27 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000330
| Global Round : 27 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.001101
| Global Round : 27 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000967
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 7, Epoch: 27--------
------------------------------------------
| Global Round : 27 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.001015
| Global Round : 27 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000379
| Global Round : 27 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000071
| Global Round : 27 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000048
| Global Round : 27 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000239
| Global Round : 27 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.003460
| Global Round : 27 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000015
| Global Round : 27 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.001029
| Global Round : 27 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000066
| Global Round : 27 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000358
| Global Round : 27 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000408
| Global Round : 27 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000818
| Global Round : 27 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000072
| Global Round : 27 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000158
| Global Round : 27 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000263
| Global Round : 27 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000309
| Global Round : 27 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000082
| Global Round : 27 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000013
| Global Round : 27 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000281
| Global Round : 27 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000032
| Global Round : 27 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.003831
| Global Round : 27 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.001569
| Global Round : 27 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000034
| Global Round : 27 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.001568
| Global Round : 27 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000122
| Global Round : 27 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000125
| Global Round : 27 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000764
| Global Round : 27 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000042
| Global Round : 27 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.031937
| Global Round : 27 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000102
| Global Round : 27 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000118
| Global Round : 27 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000456
| Global Round : 27 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.001288
| Global Round : 27 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.001284
| Global Round : 27 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000046
| Global Round : 27 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000027
| Global Round : 27 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000860
| Global Round : 27 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000343
| Global Round : 27 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000553
| Global Round : 27 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000012
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 4, Epoch: 27--------
------------------------------------------
| Global Round : 27 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000292
| Global Round : 27 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000224
| Global Round : 27 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000168
| Global Round : 27 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000682
| Global Round : 27 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000231
| Global Round : 27 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000142
| Global Round : 27 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000566
| Global Round : 27 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000438
| Global Round : 27 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000705
| Global Round : 27 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000038
| Global Round : 27 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.001565
| Global Round : 27 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000184
| Global Round : 27 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000082
| Global Round : 27 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000101
| Global Round : 27 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.014032
| Global Round : 27 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000039
| Global Round : 27 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000174
| Global Round : 27 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000029
| Global Round : 27 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000028
| Global Round : 27 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000243
| Global Round : 27 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.005016
| Global Round : 27 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000837
| Global Round : 27 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000018
| Global Round : 27 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000065
| Global Round : 27 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000163
| Global Round : 27 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000030
| Global Round : 27 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.544734
| Global Round : 27 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000100
| Global Round : 27 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000148
| Global Round : 27 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000143
| Global Round : 27 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000200
| Global Round : 27 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000141
| Global Round : 27 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000085
| Global Round : 27 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000460
| Global Round : 27 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.002028
| Global Round : 27 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000166
| Global Round : 27 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000048
| Global Round : 27 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.004562
| Global Round : 27 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000985
| Global Round : 27 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000059
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 1, Epoch: 27--------
------------------------------------------
| Global Round : 27 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.001219
| Global Round : 27 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000042
| Global Round : 27 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.001922
| Global Round : 27 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000023
| Global Round : 27 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000112
| Global Round : 27 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.001472
| Global Round : 27 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.016123
| Global Round : 27 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000174
| Global Round : 27 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000026
| Global Round : 27 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000006
| Global Round : 27 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000030
| Global Round : 27 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000208
| Global Round : 27 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000142
| Global Round : 27 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000032
| Global Round : 27 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000126
| Global Round : 27 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000797
| Global Round : 27 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000353
| Global Round : 27 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000064
| Global Round : 27 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000268
| Global Round : 27 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000224
| Global Round : 27 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.006273
| Global Round : 27 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000048
| Global Round : 27 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000085
| Global Round : 27 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000312
| Global Round : 27 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000029
| Global Round : 27 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000644
| Global Round : 27 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000485
| Global Round : 27 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000452
| Global Round : 27 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000074
| Global Round : 27 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000809
| Global Round : 27 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000702
| Global Round : 27 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.003473
| Global Round : 27 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000013
| Global Round : 27 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000041
| Global Round : 27 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000120
| Global Round : 27 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.001131
| Global Round : 27 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000027
| Global Round : 27 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000441
| Global Round : 27 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000454
| Global Round : 27 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000543
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
 
Avg Training Stats after 28 global rounds:
Training Loss : nan
Train Accuracy: 9979.71% 

Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 49.62% | Val Loss: nan | Val Acc: 52.80%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.80%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.80%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.80%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.80%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.80%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.80%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.80%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.80%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.80%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.80%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.80%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.80%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.80%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.80%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.80%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.80%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.80%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.80%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.80%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.80%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.80%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.80%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.80%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.80%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.80%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.80%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.80%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.80%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.80%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.80%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.80%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.80%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.80%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.80%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.80%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.80%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.80%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.80%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.80%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.80%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.80%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.80%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.80%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.80%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.80%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.80%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.80%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.80%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 49.88% | Val Loss: nan | Val Acc: 52.80%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 52.80 %
----Attack Model Testing----
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
 93%|| 28/30 [1:44:18<07:29, 224.78s/it]Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

Selected users for the training: [7 8 6 1 5 0 4 9 3 2]
------------------------------------------
------User: 7, Epoch: 28--------
------------------------------------------
| Global Round : 28 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000506
| Global Round : 28 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000008
| Global Round : 28 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.015069
| Global Round : 28 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000045
| Global Round : 28 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000046
| Global Round : 28 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000139
| Global Round : 28 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000267
| Global Round : 28 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000040
| Global Round : 28 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.001507
| Global Round : 28 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000118
| Global Round : 28 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.002143
| Global Round : 28 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000138
| Global Round : 28 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000070
| Global Round : 28 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.003673
| Global Round : 28 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.002914
| Global Round : 28 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000404
| Global Round : 28 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000052
| Global Round : 28 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000426
| Global Round : 28 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000058
| Global Round : 28 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.001213
| Global Round : 28 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000591
| Global Round : 28 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000043
| Global Round : 28 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.011226
| Global Round : 28 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000095
| Global Round : 28 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.002832
| Global Round : 28 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000845
| Global Round : 28 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000192
| Global Round : 28 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000831
| Global Round : 28 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.001204
| Global Round : 28 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000284
| Global Round : 28 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.002964
| Global Round : 28 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.001192
| Global Round : 28 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000252
| Global Round : 28 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000048
| Global Round : 28 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000114
| Global Round : 28 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000070
| Global Round : 28 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000916
| Global Round : 28 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.002526
| Global Round : 28 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000249
| Global Round : 28 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000848
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 8, Epoch: 28--------
------------------------------------------
| Global Round : 28 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000600
| Global Round : 28 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000013
| Global Round : 28 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000165
| Global Round : 28 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000053
| Global Round : 28 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000536
| Global Round : 28 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000130
| Global Round : 28 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000045
| Global Round : 28 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000159
| Global Round : 28 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000277
| Global Round : 28 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000027
| Global Round : 28 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000126
| Global Round : 28 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000047
| Global Round : 28 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000155
| Global Round : 28 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000536
| Global Round : 28 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000133
| Global Round : 28 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000065
| Global Round : 28 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000165
| Global Round : 28 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000343
| Global Round : 28 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.002061
| Global Round : 28 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000088
| Global Round : 28 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000142
| Global Round : 28 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000420
| Global Round : 28 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000371
| Global Round : 28 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000005
| Global Round : 28 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000663
| Global Round : 28 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000280
| Global Round : 28 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000176
| Global Round : 28 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.001676
| Global Round : 28 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000012
| Global Round : 28 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000171
| Global Round : 28 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.001566
| Global Round : 28 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.001920
| Global Round : 28 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000029
| Global Round : 28 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000031
| Global Round : 28 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000427
| Global Round : 28 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000012
| Global Round : 28 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.001147
| Global Round : 28 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000122
| Global Round : 28 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000776
| Global Round : 28 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000239
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 6, Epoch: 28--------
------------------------------------------
| Global Round : 28 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000846
| Global Round : 28 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.010779
| Global Round : 28 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000193
| Global Round : 28 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000183
| Global Round : 28 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000747
| Global Round : 28 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000289
| Global Round : 28 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000189
| Global Round : 28 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000976
| Global Round : 28 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000092
| Global Round : 28 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.003913
| Global Round : 28 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.001046
| Global Round : 28 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000062
| Global Round : 28 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000139
| Global Round : 28 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000017
| Global Round : 28 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.003701
| Global Round : 28 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000997
| Global Round : 28 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000155
| Global Round : 28 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.001481
| Global Round : 28 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000932
| Global Round : 28 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000047
| Global Round : 28 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000476
| Global Round : 28 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.010160
| Global Round : 28 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.001069
| Global Round : 28 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000040
| Global Round : 28 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000050
| Global Round : 28 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.001978
| Global Round : 28 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000115
| Global Round : 28 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000775
| Global Round : 28 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000145
| Global Round : 28 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000030
| Global Round : 28 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000659
| Global Round : 28 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000037
| Global Round : 28 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.005678
| Global Round : 28 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.001476
| Global Round : 28 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000092
| Global Round : 28 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000034
| Global Round : 28 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.009777
| Global Round : 28 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.004536
| Global Round : 28 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000087
| Global Round : 28 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000105
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 1, Epoch: 28--------
------------------------------------------
| Global Round : 28 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000102
| Global Round : 28 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000106
| Global Round : 28 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000269
| Global Round : 28 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000007
| Global Round : 28 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000131
| Global Round : 28 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.005080
| Global Round : 28 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.002638
| Global Round : 28 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000117
| Global Round : 28 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000139
| Global Round : 28 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000311
| Global Round : 28 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.012022
| Global Round : 28 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.001836
| Global Round : 28 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.002165
| Global Round : 28 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000093
| Global Round : 28 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000045
| Global Round : 28 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000139
| Global Round : 28 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000007
| Global Round : 28 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.001960
| Global Round : 28 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000049
| Global Round : 28 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.007748
| Global Round : 28 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.003829
| Global Round : 28 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000384
| Global Round : 28 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000069
| Global Round : 28 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.007342
| Global Round : 28 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000095
| Global Round : 28 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000390
| Global Round : 28 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.012153
| Global Round : 28 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000045
| Global Round : 28 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000112
| Global Round : 28 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.001004
| Global Round : 28 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000182
| Global Round : 28 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.005705
| Global Round : 28 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.004395
| Global Round : 28 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000020
| Global Round : 28 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000852
| Global Round : 28 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.003039
| Global Round : 28 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000049
| Global Round : 28 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000067
| Global Round : 28 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.003588
| Global Round : 28 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.002160
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 5, Epoch: 28--------
------------------------------------------
| Global Round : 28 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.001064
| Global Round : 28 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.002140
| Global Round : 28 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000189
| Global Round : 28 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000112
| Global Round : 28 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000877
| Global Round : 28 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000032
| Global Round : 28 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.002971
| Global Round : 28 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000088
| Global Round : 28 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000217
| Global Round : 28 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000623
| Global Round : 28 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.001441
| Global Round : 28 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.001980
| Global Round : 28 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000074
| Global Round : 28 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000176
| Global Round : 28 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000005
| Global Round : 28 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000615
| Global Round : 28 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.001174
| Global Round : 28 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000556
| Global Round : 28 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000219
| Global Round : 28 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000095
| Global Round : 28 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000649
| Global Round : 28 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000012
| Global Round : 28 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000086
| Global Round : 28 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000232
| Global Round : 28 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000950
| Global Round : 28 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000078
| Global Round : 28 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.001614
| Global Round : 28 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.006005
| Global Round : 28 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000753
| Global Round : 28 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000011
| Global Round : 28 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000288
| Global Round : 28 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000179
| Global Round : 28 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.001198
| Global Round : 28 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.002427
| Global Round : 28 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000049
| Global Round : 28 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000090
| Global Round : 28 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.006593
| Global Round : 28 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000243
| Global Round : 28 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000340
| Global Round : 28 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000509
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 0, Epoch: 28--------
------------------------------------------
| Global Round : 28 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000067
| Global Round : 28 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.002785
| Global Round : 28 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000732
| Global Round : 28 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000017
| Global Round : 28 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000280
| Global Round : 28 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000200
| Global Round : 28 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000133
| Global Round : 28 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000218
| Global Round : 28 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.001260
| Global Round : 28 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.001118
| Global Round : 28 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000144
| Global Round : 28 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000529
| Global Round : 28 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000065
| Global Round : 28 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.012087
| Global Round : 28 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000172
| Global Round : 28 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.004710
| Global Round : 28 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.001478
| Global Round : 28 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.003699
| Global Round : 28 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000049
| Global Round : 28 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000887
| Global Round : 28 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.011372
| Global Round : 28 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000377
| Global Round : 28 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.004116
| Global Round : 28 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000192
| Global Round : 28 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.002145
| Global Round : 28 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000127
| Global Round : 28 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000352
| Global Round : 28 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000089
| Global Round : 28 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000123
| Global Round : 28 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.012137
| Global Round : 28 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.001269
| Global Round : 28 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000066
| Global Round : 28 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000915
| Global Round : 28 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000278
| Global Round : 28 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000068
| Global Round : 28 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000484
| Global Round : 28 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000067
| Global Round : 28 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000020
| Global Round : 28 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000505
| Global Round : 28 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000333
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 49.97% | Val Loss: nan | Val Acc: 50.70%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 50.70 %
----Attack Model Testing----
Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

------------------------------------------
------User: 4, Epoch: 28--------
------------------------------------------
| Global Round : 28 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000120
| Global Round : 28 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000165
| Global Round : 28 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000020
| Global Round : 28 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.001713
| Global Round : 28 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000224
| Global Round : 28 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000102
| Global Round : 28 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000099
| Global Round : 28 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000038
| Global Round : 28 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000044
| Global Round : 28 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000496
| Global Round : 28 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000049
| Global Round : 28 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000643
| Global Round : 28 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000382
| Global Round : 28 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.022604
| Global Round : 28 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000036
| Global Round : 28 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000226
| Global Round : 28 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000246
| Global Round : 28 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000714
| Global Round : 28 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.003941
| Global Round : 28 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000102
| Global Round : 28 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000286
| Global Round : 28 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000089
| Global Round : 28 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000306
| Global Round : 28 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.001213
| Global Round : 28 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000026
| Global Round : 28 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.004057
| Global Round : 28 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000074
| Global Round : 28 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000131
| Global Round : 28 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000032
| Global Round : 28 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000007
| Global Round : 28 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000630
| Global Round : 28 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000029
| Global Round : 28 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000010
| Global Round : 28 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000016
| Global Round : 28 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.003648
| Global Round : 28 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000008
| Global Round : 28 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000467
| Global Round : 28 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.001388
| Global Round : 28 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000092
| Global Round : 28 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000050
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 9, Epoch: 28--------
------------------------------------------
| Global Round : 28 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000151
| Global Round : 28 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000063
| Global Round : 28 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.001142
| Global Round : 28 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000191
| Global Round : 28 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000486
| Global Round : 28 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000247
| Global Round : 28 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.002408
| Global Round : 28 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000113
| Global Round : 28 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000091
| Global Round : 28 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000993
| Global Round : 28 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.008074
| Global Round : 28 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000566
| Global Round : 28 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000093
| Global Round : 28 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000376
| Global Round : 28 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.001602
| Global Round : 28 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000015
| Global Round : 28 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.004615
| Global Round : 28 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000019
| Global Round : 28 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000357
| Global Round : 28 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000086
| Global Round : 28 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000406
| Global Round : 28 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000032
| Global Round : 28 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000006
| Global Round : 28 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000420
| Global Round : 28 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000616
| Global Round : 28 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000291
| Global Round : 28 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.001342
| Global Round : 28 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000307
| Global Round : 28 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000512
| Global Round : 28 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000041
| Global Round : 28 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000549
| Global Round : 28 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000096
| Global Round : 28 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000271
| Global Round : 28 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000136
| Global Round : 28 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.039440
| Global Round : 28 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.009715
| Global Round : 28 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.001333
| Global Round : 28 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000041
| Global Round : 28 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000207
| Global Round : 28 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.002781
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 3, Epoch: 28--------
------------------------------------------
| Global Round : 28 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000021
| Global Round : 28 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000448
| Global Round : 28 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000044
| Global Round : 28 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.001115
| Global Round : 28 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000274
| Global Round : 28 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000330
| Global Round : 28 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000067
| Global Round : 28 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000143
| Global Round : 28 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000013
| Global Round : 28 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000111
| Global Round : 28 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000104
| Global Round : 28 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000237
| Global Round : 28 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000083
| Global Round : 28 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000269
| Global Round : 28 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000823
| Global Round : 28 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000032
| Global Round : 28 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000242
| Global Round : 28 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000538
| Global Round : 28 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000406
| Global Round : 28 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000037
| Global Round : 28 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000009
| Global Round : 28 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.004157
| Global Round : 28 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000825
| Global Round : 28 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000065
| Global Round : 28 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000060
| Global Round : 28 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000050
| Global Round : 28 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000844
| Global Round : 28 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000044
| Global Round : 28 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000013
| Global Round : 28 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.003726
| Global Round : 28 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000408
| Global Round : 28 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000148
| Global Round : 28 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000257
| Global Round : 28 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000278
| Global Round : 28 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000636
| Global Round : 28 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000112
| Global Round : 28 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000062
| Global Round : 28 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000034
| Global Round : 28 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.011914
| Global Round : 28 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000310
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 2, Epoch: 28--------
------------------------------------------
| Global Round : 28 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000982
| Global Round : 28 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000207
| Global Round : 28 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000043
| Global Round : 28 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000140
| Global Round : 28 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000282
| Global Round : 28 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000610
| Global Round : 28 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000292
| Global Round : 28 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000468
| Global Round : 28 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000072
| Global Round : 28 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000062
| Global Round : 28 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.001666
| Global Round : 28 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000013
| Global Round : 28 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.001081
| Global Round : 28 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.002125
| Global Round : 28 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000066
| Global Round : 28 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.005562
| Global Round : 28 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000317
| Global Round : 28 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000344
| Global Round : 28 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000381
| Global Round : 28 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.001112
| Global Round : 28 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.001129
| Global Round : 28 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000107
| Global Round : 28 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000420
| Global Round : 28 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000356
| Global Round : 28 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000195
| Global Round : 28 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000251
| Global Round : 28 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.008042
| Global Round : 28 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.007154
| Global Round : 28 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000643
| Global Round : 28 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000021
| Global Round : 28 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000755
| Global Round : 28 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000137
| Global Round : 28 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000366
| Global Round : 28 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000401
| Global Round : 28 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.003095
| Global Round : 28 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000471
| Global Round : 28 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000719
| Global Round : 28 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000098
| Global Round : 28 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000067
| Global Round : 28 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000166
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
 
Avg Training Stats after 29 global rounds:
Training Loss : nan
Train Accuracy: 9980.41% 

Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 49.84% | Val Loss: nan | Val Acc: 48.20%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.20%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.20%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.20%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.20%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.20%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.20%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.20%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.20%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.20%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.20%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.20%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.20%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.20%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.20%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.20%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.20%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.20%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.20%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.20%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.20%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.20%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.20%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.20%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.20%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.20%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.20%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.20%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.20%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.20%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.20%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.20%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.20%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.20%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.20%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.20%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.20%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.20%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.20%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.20%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.20%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.20%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.20%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.20%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.20%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.20%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.20%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.20%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.20%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 50.08% | Val Loss: nan | Val Acc: 48.20%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 48.20 %
----Attack Model Testing----
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
 97%|| 29/30 [1:47:56<03:42, 222.77s/it]Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

Selected users for the training: [0 8 3 2 1 9 5 4 7 6]
------------------------------------------
------User: 0, Epoch: 29--------
------------------------------------------
| Global Round : 29 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000038
| Global Round : 29 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000434
| Global Round : 29 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000452
| Global Round : 29 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000462
| Global Round : 29 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.001107
| Global Round : 29 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000528
| Global Round : 29 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000061
| Global Round : 29 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000448
| Global Round : 29 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.001295
| Global Round : 29 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000385
| Global Round : 29 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000023
| Global Round : 29 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.016800
| Global Round : 29 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000201
| Global Round : 29 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000544
| Global Round : 29 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000093
| Global Round : 29 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000043
| Global Round : 29 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000071
| Global Round : 29 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000409
| Global Round : 29 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000157
| Global Round : 29 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000597
| Global Round : 29 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.004307
| Global Round : 29 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000171
| Global Round : 29 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000115
| Global Round : 29 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000298
| Global Round : 29 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.001155
| Global Round : 29 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.003156
| Global Round : 29 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.001694
| Global Round : 29 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000058
| Global Round : 29 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.001870
| Global Round : 29 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000049
| Global Round : 29 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.009424
| Global Round : 29 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.002707
| Global Round : 29 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000150
| Global Round : 29 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.002245
| Global Round : 29 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000593
| Global Round : 29 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.026293
| Global Round : 29 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.001150
| Global Round : 29 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.001178
| Global Round : 29 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000072
| Global Round : 29 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000070
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 49.79% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 50.05% | Val Loss: nan | Val Acc: 48.90%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 48.90 %
----Attack Model Testing----
Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

------------------------------------------
------User: 8, Epoch: 29--------
------------------------------------------
| Global Round : 29 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000303
| Global Round : 29 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000101
| Global Round : 29 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.001362
| Global Round : 29 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000988
| Global Round : 29 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.001097
| Global Round : 29 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000127
| Global Round : 29 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000100
| Global Round : 29 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000099
| Global Round : 29 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000055
| Global Round : 29 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000143
| Global Round : 29 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000559
| Global Round : 29 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000065
| Global Round : 29 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000025
| Global Round : 29 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000315
| Global Round : 29 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000221
| Global Round : 29 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000256
| Global Round : 29 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000033
| Global Round : 29 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000429
| Global Round : 29 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.001400
| Global Round : 29 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000771
| Global Round : 29 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000018
| Global Round : 29 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000082
| Global Round : 29 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000062
| Global Round : 29 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000078
| Global Round : 29 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000008
| Global Round : 29 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000108
| Global Round : 29 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000073
| Global Round : 29 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000321
| Global Round : 29 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000005
| Global Round : 29 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000042
| Global Round : 29 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000037
| Global Round : 29 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000054
| Global Round : 29 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000121
| Global Round : 29 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000049
| Global Round : 29 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.001238
| Global Round : 29 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.018496
| Global Round : 29 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000077
| Global Round : 29 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000482
| Global Round : 29 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000982
| Global Round : 29 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000371
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 3, Epoch: 29--------
------------------------------------------
| Global Round : 29 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000058
| Global Round : 29 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.001597
| Global Round : 29 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000033
| Global Round : 29 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000202
| Global Round : 29 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000116
| Global Round : 29 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000034
| Global Round : 29 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000868
| Global Round : 29 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000072
| Global Round : 29 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000009
| Global Round : 29 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000718
| Global Round : 29 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000148
| Global Round : 29 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000238
| Global Round : 29 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000017
| Global Round : 29 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000154
| Global Round : 29 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.004370
| Global Round : 29 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000461
| Global Round : 29 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000280
| Global Round : 29 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000005
| Global Round : 29 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000064
| Global Round : 29 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000862
| Global Round : 29 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.020644
| Global Round : 29 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.002836
| Global Round : 29 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000086
| Global Round : 29 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.001310
| Global Round : 29 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000008
| Global Round : 29 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.001555
| Global Round : 29 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000028
| Global Round : 29 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000050
| Global Round : 29 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.001018
| Global Round : 29 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000902
| Global Round : 29 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000046
| Global Round : 29 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000201
| Global Round : 29 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000110
| Global Round : 29 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000011
| Global Round : 29 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000513
| Global Round : 29 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000096
| Global Round : 29 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.001293
| Global Round : 29 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000022
| Global Round : 29 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000403
| Global Round : 29 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000009
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 2, Epoch: 29--------
------------------------------------------
| Global Round : 29 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000299
| Global Round : 29 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.002110
| Global Round : 29 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000435
| Global Round : 29 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000083
| Global Round : 29 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000046
| Global Round : 29 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.005388
| Global Round : 29 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000080
| Global Round : 29 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000061
| Global Round : 29 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000166
| Global Round : 29 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000295
| Global Round : 29 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000251
| Global Round : 29 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000355
| Global Round : 29 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000743
| Global Round : 29 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000226
| Global Round : 29 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000310
| Global Round : 29 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000271
| Global Round : 29 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000165
| Global Round : 29 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000641
| Global Round : 29 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000095
| Global Round : 29 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000241
| Global Round : 29 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.001517
| Global Round : 29 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.002014
| Global Round : 29 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000276
| Global Round : 29 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000141
| Global Round : 29 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.004972
| Global Round : 29 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.001390
| Global Round : 29 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.001015
| Global Round : 29 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000049
| Global Round : 29 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000075
| Global Round : 29 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.001243
| Global Round : 29 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000039
| Global Round : 29 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.002439
| Global Round : 29 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000137
| Global Round : 29 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000375
| Global Round : 29 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000015
| Global Round : 29 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000089
| Global Round : 29 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000998
| Global Round : 29 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000161
| Global Round : 29 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000473
| Global Round : 29 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000087
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 1, Epoch: 29--------
------------------------------------------
| Global Round : 29 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000107
| Global Round : 29 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000177
| Global Round : 29 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000622
| Global Round : 29 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000146
| Global Round : 29 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000166
| Global Round : 29 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000608
| Global Round : 29 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.005691
| Global Round : 29 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000075
| Global Round : 29 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000527
| Global Round : 29 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000274
| Global Round : 29 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000192
| Global Round : 29 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000055
| Global Round : 29 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000399
| Global Round : 29 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000017
| Global Round : 29 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000246
| Global Round : 29 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000061
| Global Round : 29 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000016
| Global Round : 29 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000204
| Global Round : 29 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000269
| Global Round : 29 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000368
| Global Round : 29 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000035
| Global Round : 29 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000455
| Global Round : 29 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.001158
| Global Round : 29 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.001464
| Global Round : 29 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.001018
| Global Round : 29 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000769
| Global Round : 29 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000165
| Global Round : 29 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000043
| Global Round : 29 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000116
| Global Round : 29 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000397
| Global Round : 29 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.058929
| Global Round : 29 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000253
| Global Round : 29 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.002758
| Global Round : 29 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000878
| Global Round : 29 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000084
| Global Round : 29 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000175
| Global Round : 29 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.002166
| Global Round : 29 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000237
| Global Round : 29 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000388
| Global Round : 29 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000006
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 9, Epoch: 29--------
------------------------------------------
| Global Round : 29 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000835
| Global Round : 29 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.001013
| Global Round : 29 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.002243
| Global Round : 29 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000104
| Global Round : 29 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000023
| Global Round : 29 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000016
| Global Round : 29 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000035
| Global Round : 29 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000071
| Global Round : 29 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000818
| Global Round : 29 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000309
| Global Round : 29 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000178
| Global Round : 29 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.012547
| Global Round : 29 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000319
| Global Round : 29 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000071
| Global Round : 29 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.001452
| Global Round : 29 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000824
| Global Round : 29 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000051
| Global Round : 29 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000008
| Global Round : 29 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000153
| Global Round : 29 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.003538
| Global Round : 29 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000405
| Global Round : 29 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000078
| Global Round : 29 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000011
| Global Round : 29 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000986
| Global Round : 29 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000111
| Global Round : 29 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000115
| Global Round : 29 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000762
| Global Round : 29 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000000
| Global Round : 29 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000015
| Global Round : 29 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.002875
| Global Round : 29 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000086
| Global Round : 29 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000229
| Global Round : 29 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000107
| Global Round : 29 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000037
| Global Round : 29 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.003708
| Global Round : 29 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000458
| Global Round : 29 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000484
| Global Round : 29 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.004035
| Global Round : 29 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000012
| Global Round : 29 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000088
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 5, Epoch: 29--------
------------------------------------------
| Global Round : 29 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000046
| Global Round : 29 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000017
| Global Round : 29 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000110
| Global Round : 29 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000023
| Global Round : 29 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000042
| Global Round : 29 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000344
| Global Round : 29 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.001438
| Global Round : 29 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000105
| Global Round : 29 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000693
| Global Round : 29 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000091
| Global Round : 29 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000017
| Global Round : 29 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000110
| Global Round : 29 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000659
| Global Round : 29 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.011856
| Global Round : 29 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.004177
| Global Round : 29 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000359
| Global Round : 29 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000692
| Global Round : 29 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000082
| Global Round : 29 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000102
| Global Round : 29 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.002400
| Global Round : 29 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000916
| Global Round : 29 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000402
| Global Round : 29 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000098
| Global Round : 29 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000020
| Global Round : 29 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000187
| Global Round : 29 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.007392
| Global Round : 29 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000055
| Global Round : 29 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000112
| Global Round : 29 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000252
| Global Round : 29 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000538
| Global Round : 29 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000027
| Global Round : 29 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.001365
| Global Round : 29 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000078
| Global Round : 29 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000028
| Global Round : 29 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000170
| Global Round : 29 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000003
| Global Round : 29 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000679
| Global Round : 29 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.004789
| Global Round : 29 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000986
| Global Round : 29 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000351
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 4, Epoch: 29--------
------------------------------------------
| Global Round : 29 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000958
| Global Round : 29 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000020
| Global Round : 29 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000535
| Global Round : 29 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.001017
| Global Round : 29 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000045
| Global Round : 29 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000148
| Global Round : 29 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000014
| Global Round : 29 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000417
| Global Round : 29 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000011
| Global Round : 29 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000259
| Global Round : 29 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000054
| Global Round : 29 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000939
| Global Round : 29 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000054
| Global Round : 29 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000208
| Global Round : 29 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000675
| Global Round : 29 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000063
| Global Round : 29 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.001448
| Global Round : 29 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000309
| Global Round : 29 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.003649
| Global Round : 29 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000060
| Global Round : 29 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000301
| Global Round : 29 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000524
| Global Round : 29 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000053
| Global Round : 29 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000802
| Global Round : 29 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000259
| Global Round : 29 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000499
| Global Round : 29 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000456
| Global Round : 29 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000110
| Global Round : 29 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000384
| Global Round : 29 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.002253
| Global Round : 29 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000056
| Global Round : 29 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000122
| Global Round : 29 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000850
| Global Round : 29 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000043
| Global Round : 29 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000092
| Global Round : 29 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000556
| Global Round : 29 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000066
| Global Round : 29 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000231
| Global Round : 29 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000066
| Global Round : 29 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000336
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 7, Epoch: 29--------
------------------------------------------
| Global Round : 29 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.000059
| Global Round : 29 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000305
| Global Round : 29 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000009
| Global Round : 29 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000046
| Global Round : 29 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000245
| Global Round : 29 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000502
| Global Round : 29 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000414
| Global Round : 29 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000137
| Global Round : 29 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000270
| Global Round : 29 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000086
| Global Round : 29 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000076
| Global Round : 29 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000851
| Global Round : 29 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.000215
| Global Round : 29 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.000018
| Global Round : 29 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000012
| Global Round : 29 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000580
| Global Round : 29 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000640
| Global Round : 29 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000032
| Global Round : 29 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000005
| Global Round : 29 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000120
| Global Round : 29 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000116
| Global Round : 29 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000229
| Global Round : 29 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.001965
| Global Round : 29 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000219
| Global Round : 29 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.006525
| Global Round : 29 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000278
| Global Round : 29 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.004296
| Global Round : 29 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000018
| Global Round : 29 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000050
| Global Round : 29 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000059
| Global Round : 29 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000192
| Global Round : 29 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000500
| Global Round : 29 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000047
| Global Round : 29 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.002315
| Global Round : 29 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000072
| Global Round : 29 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000003
| Global Round : 29 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000026
| Global Round : 29 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.008259
| Global Round : 29 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.001302
| Global Round : 29 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.000015
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
------------------------------------------
------User: 6, Epoch: 29--------
------------------------------------------
| Global Round : 29 | Local Epoch : 0 | [0/4000 (0%)]	Loss: 0.002078
| Global Round : 29 | Local Epoch : 0 | [100/4000 (2%)]	Loss: 0.000047
| Global Round : 29 | Local Epoch : 0 | [200/4000 (5%)]	Loss: 0.000018
| Global Round : 29 | Local Epoch : 0 | [300/4000 (8%)]	Loss: 0.000010
| Global Round : 29 | Local Epoch : 0 | [400/4000 (10%)]	Loss: 0.000072
| Global Round : 29 | Local Epoch : 0 | [500/4000 (12%)]	Loss: 0.000186
| Global Round : 29 | Local Epoch : 0 | [600/4000 (15%)]	Loss: 0.000109
| Global Round : 29 | Local Epoch : 0 | [700/4000 (18%)]	Loss: 0.000026
| Global Round : 29 | Local Epoch : 0 | [800/4000 (20%)]	Loss: 0.000527
| Global Round : 29 | Local Epoch : 0 | [900/4000 (22%)]	Loss: 0.000202
| Global Round : 29 | Local Epoch : 0 | [1000/4000 (25%)]	Loss: 0.000130
| Global Round : 29 | Local Epoch : 0 | [1100/4000 (28%)]	Loss: 0.000019
| Global Round : 29 | Local Epoch : 0 | [1200/4000 (30%)]	Loss: 0.001034
| Global Round : 29 | Local Epoch : 0 | [1300/4000 (32%)]	Loss: 0.003190
| Global Round : 29 | Local Epoch : 0 | [1400/4000 (35%)]	Loss: 0.000032
| Global Round : 29 | Local Epoch : 0 | [1500/4000 (38%)]	Loss: 0.000206
| Global Round : 29 | Local Epoch : 0 | [1600/4000 (40%)]	Loss: 0.000060
| Global Round : 29 | Local Epoch : 0 | [1700/4000 (42%)]	Loss: 0.000030
| Global Round : 29 | Local Epoch : 0 | [1800/4000 (45%)]	Loss: 0.000562
| Global Round : 29 | Local Epoch : 0 | [1900/4000 (48%)]	Loss: 0.000155
| Global Round : 29 | Local Epoch : 0 | [2000/4000 (50%)]	Loss: 0.000432
| Global Round : 29 | Local Epoch : 0 | [2100/4000 (52%)]	Loss: 0.000087
| Global Round : 29 | Local Epoch : 0 | [2200/4000 (55%)]	Loss: 0.000024
| Global Round : 29 | Local Epoch : 0 | [2300/4000 (58%)]	Loss: 0.000085
| Global Round : 29 | Local Epoch : 0 | [2400/4000 (60%)]	Loss: 0.000142
| Global Round : 29 | Local Epoch : 0 | [2500/4000 (62%)]	Loss: 0.000372
| Global Round : 29 | Local Epoch : 0 | [2600/4000 (65%)]	Loss: 0.000573
| Global Round : 29 | Local Epoch : 0 | [2700/4000 (68%)]	Loss: 0.000134
| Global Round : 29 | Local Epoch : 0 | [2800/4000 (70%)]	Loss: 0.000076
| Global Round : 29 | Local Epoch : 0 | [2900/4000 (72%)]	Loss: 0.000033
| Global Round : 29 | Local Epoch : 0 | [3000/4000 (75%)]	Loss: 0.000070
| Global Round : 29 | Local Epoch : 0 | [3100/4000 (78%)]	Loss: 0.000226
| Global Round : 29 | Local Epoch : 0 | [3200/4000 (80%)]	Loss: 0.000431
| Global Round : 29 | Local Epoch : 0 | [3300/4000 (82%)]	Loss: 0.000163
| Global Round : 29 | Local Epoch : 0 | [3400/4000 (85%)]	Loss: 0.000130
| Global Round : 29 | Local Epoch : 0 | [3500/4000 (88%)]	Loss: 0.000404
| Global Round : 29 | Local Epoch : 0 | [3600/4000 (90%)]	Loss: 0.000029
| Global Round : 29 | Local Epoch : 0 | [3700/4000 (92%)]	Loss: 0.000053
| Global Round : 29 | Local Epoch : 0 | [3800/4000 (95%)]	Loss: 0.000268
| Global Round : 29 | Local Epoch : 0 | [3900/4000 (98%)]	Loss: 0.001282
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
 
Avg Training Stats after 30 global rounds:
Training Loss : nan
Train Accuracy: 9981.07% 

Key layer1.0.bn1.weight altered
Key layer1.0.bn1.bias altered
Key layer1.0.bn1.running_mean altered
Key layer1.0.bn1.running_var altered
Key layer1.0.bn1.num_batches_tracked altered
Key layer1.0.bn2.weight altered
Key layer1.0.bn2.bias altered
Key layer1.0.bn2.running_mean altered
Key layer1.0.bn2.running_var altered
Key layer1.0.bn2.num_batches_tracked altered
Key layer1.1.bn1.weight altered
Key layer1.1.bn1.bias altered
Key layer1.1.bn1.running_mean altered
Key layer1.1.bn1.running_var altered
Key layer1.1.bn1.num_batches_tracked altered
Key layer1.1.bn2.weight altered
Key layer1.1.bn2.bias altered
Key layer1.1.bn2.running_mean altered
Key layer1.1.bn2.running_var altered
Key layer1.1.bn2.num_batches_tracked altered
Key layer2.0.bn1.weight altered
Key layer2.0.bn1.bias altered
Key layer2.0.bn1.running_mean altered
Key layer2.0.bn1.running_var altered
Key layer2.0.bn1.num_batches_tracked altered
Key layer2.0.bn2.weight altered
Key layer2.0.bn2.bias altered
Key layer2.0.bn2.running_mean altered
Key layer2.0.bn2.running_var altered
Key layer2.0.bn2.num_batches_tracked altered
Key layer2.1.bn1.weight altered
Key layer2.1.bn1.bias altered
Key layer2.1.bn1.running_mean altered
Key layer2.1.bn1.running_var altered
Key layer2.1.bn1.num_batches_tracked altered
Key layer2.1.bn2.weight altered
Key layer2.1.bn2.bias altered
Key layer2.1.bn2.running_mean altered
Key layer2.1.bn2.running_var altered
Key layer2.1.bn2.num_batches_tracked altered
Key layer3.0.bn1.weight altered
Key layer3.0.bn1.bias altered
Key layer3.0.bn1.running_mean altered
Key layer3.0.bn1.running_var altered
Key layer3.0.bn1.num_batches_tracked altered
Key layer3.0.bn2.weight altered
Key layer3.0.bn2.bias altered
Key layer3.0.bn2.running_mean altered
Key layer3.0.bn2.running_var altered
Key layer3.0.bn2.num_batches_tracked altered
Key layer3.1.bn1.weight altered
Key layer3.1.bn1.bias altered
Key layer3.1.bn1.running_mean altered
Key layer3.1.bn1.running_var altered
Key layer3.1.bn1.num_batches_tracked altered
Key layer3.1.bn2.weight altered
Key layer3.1.bn2.bias altered
Key layer3.1.bn2.running_mean altered
Key layer3.1.bn2.running_var altered
Key layer3.1.bn2.num_batches_tracked altered
Key layer4.0.bn1.weight altered
Key layer4.0.bn1.bias altered
Key layer4.0.bn1.running_mean altered
Key layer4.0.bn1.running_var altered
Key layer4.0.bn1.num_batches_tracked altered
Key layer4.0.bn2.weight altered
Key layer4.0.bn2.bias altered
Key layer4.0.bn2.running_mean altered
Key layer4.0.bn2.running_var altered
Key layer4.0.bn2.num_batches_tracked altered
Key layer4.1.bn1.weight altered
Key layer4.1.bn1.bias altered
Key layer4.1.bn1.running_mean altered
Key layer4.1.bn1.running_var altered
Key layer4.1.bn1.num_batches_tracked altered
Key layer4.1.bn2.weight altered
Key layer4.1.bn2.bias altered
Key layer4.1.bn2.running_mean altered
Key layer4.1.bn2.running_var altered
Key layer4.1.bn2.num_batches_tracked altered
Using device: cuda
Files already downloaded and verified
Total Test samples in cifar dataset : 10000
Total Train samples in cifar dataset : 50000
Number of Target train samples: 12500
Number of Target valid samples: 1000
Number of Target test samples: 12500
Number of Shadow train samples: 12500
Number of Shadow valid samples: 1000
Number of Shadow test samples: 12500
Use Target model at the path ====> [train_model_84.pt] 
---Peparing Attack Training data---
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home2/felhattab/mia/dataset.py:347: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(image), torch.tensor(label)
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home2/felhattab/.local/lib/python3.8/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
tensor([1, 1, 1,  ..., 1, 1, 1])
Using Shadow model at the path  ====> [./attack_model/best_shadow_model.ckpt] 
----Preparing Attack training data---
Input Feature dim for Attack Model : [10]
Shape of Attack Feature Data : torch.Size([25000, 10])
Shape of Attack Target Data : torch.Size([25000])
Length of Attack Model train dataset : [25000]
Epochs [50] and Batch size [128] for Attack Model training
----Attack Model Training------
Epoch [1/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [2/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [3/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [4/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [5/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [6/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [7/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [8/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [9/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [10/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [11/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [12/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [13/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [14/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [15/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [16/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [17/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [18/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [19/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [20/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [21/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [22/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [23/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [24/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [25/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [26/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [27/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [28/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [29/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [30/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [31/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [32/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [33/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [34/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [35/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [36/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [37/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [38/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [39/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [40/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [41/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [42/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [43/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [44/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [45/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [46/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [47/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [48/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [49/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Epoch [50/50], Train Loss: nan | Train Acc: 50.02% | Val Loss: nan | Val Acc: 49.40%
Saving model checkpoint
Validation Accuracy for the Best Attack Model is: 49.40 %
----Attack Model Testing----
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
100%|| 30/30 [1:51:33<00:00, 220.92s/it]100%|| 30/30 [1:51:33<00:00, 223.10s/it]
Attack Test Accuracy is  : 50.00%
---Detailed Results----
              precision    recall  f1-score   support

  Non-Member       0.50      1.00      0.67     12500
      Member       0.00      0.00      0.00     12500

    accuracy                           0.50     25000
   macro avg       0.25      0.50      0.33     25000
weighted avg       0.25      0.50      0.33     25000

 
 Results after 30 global rounds of training:
|---- Avg Train Accuracy: 9981.07%
|---- Test Accuracy: 85.27%
[50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0]
[0.8464, 0.8529, 0.8519, 0.8538, 0.8533, 0.8554, 0.8531, 0.8521, 0.8529, 0.8516, 0.853, 0.8535, 0.8529, 0.8526, 0.8525, 0.8534, 0.8531, 0.8524, 0.8526, 0.8532, 0.8542, 0.854, 0.8521, 0.8539, 0.8535, 0.8537, 0.8542, 0.8526, 0.8541, 0.8527]
defaultdict(<class 'list'>, {'client_accuracy_7': [94.0, 99.4, 99.0, 99.6, 100.0, 99.8, 99.8, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_0': [94.0, 99.6, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_5': [98.0, 99.2, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_6': [96.2, 99.6, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_9': [97.2, 99.4, 99.8, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_2': [96.0, 98.4, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_3': [96.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_1': [97.2, 99.8, 99.4, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_8': [96.8, 98.6, 99.6, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, 100.0, 99.8, 100.0, 100.0, 100.0, 100.0], 'client_accuracy_4': [96.8, 97.2, 99.6, 99.8, 99.6, 99.4, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]})
defaultdict(<class 'list'>, {'client_attack_0': [50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0]})
